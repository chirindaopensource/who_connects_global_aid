{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfwNDLVyrw8k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`README.md`**\n",
        "\n",
        "# Who Connects Global Aid? The Hidden Geometry of 10 Million Transactions\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2512.17243-b31b1b.svg)](https://arxiv.org/abs/2512.17243)\n",
        "[![Journal](https://img.shields.io/badge/Journal-ArXiv%20Preprint-003366)](https://arxiv.org/abs/2512.17243)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/who_connects_global_aid)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Network%20Science%20%7C%20Development%20Economics-00529B)](https://github.com/chirindaopensource/who_connects_global_aid)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-IATI%20Registry-lightgrey)](https://iatiregistry.org/)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-Web%20Hyperlink%20Graph-lightgrey)](https://commoncrawl.org/)\n",
        "[![Core Method](https://img.shields.io/badge/Method-Bipartite%20Projection-orange)](https://github.com/chirindaopensource/who_connects_global_aid)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Node2Vec%20Embedding-red)](https://github.com/chirindaopensource/who_connects_global_aid)\n",
        "[![Validation](https://img.shields.io/badge/Validation-PageRank%20Correlation-green)](https://github.com/chirindaopensource/who_connects_global_aid)\n",
        "[![Robustness](https://img.shields.io/badge/Robustness-Sensitivity%20Analysis-yellow)](https://github.com/chirindaopensource/who_connects_global_aid)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![SciPy](https://img.shields.io/badge/scipy-%230054A6.svg?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![NetworkX](https://img.shields.io/badge/networkx-%230054A6.svg?style=flat&logo=networkx&logoColor=white)](https://networkx.org/)\n",
        "[![UMAP](https://img.shields.io/badge/umap-%230054A6.svg?style=flat&logo=python&logoColor=white)](https://umap-learn.readthedocs.io/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/who_connects_global_aid`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Who Connects Global Aid? The Hidden Geometry of 10 Million Transactions\"** by:\n",
        "\n",
        "*   **Paul X. McCarthy** (League of Scholars, Sydney; UNSW)\n",
        "*   **Xian Gong** (University of Technology Sydney)\n",
        "*   **Marian-Andrei Rizoiu** (University of Technology Sydney)\n",
        "*   **Paolo Boldi** (Università degli Studi di Milano)\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from the ingestion and cleansing of massive IATI transaction logs to the rigorous construction of bipartite networks, high-dimensional structural embedding via Node2Vec, and the identification of critical \"knowledge brokers\" through centrality analysis.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `run_global_aid_pipeline`](#key-callable-run_global_aid_pipeline)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the analytical framework presented in McCarthy et al. (2025). The core of this repository is the iPython Notebook `who_connects_global_aid_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings. The pipeline is designed to map the topological structure of the global aid ecosystem, moving beyond aggregate volume flows to reveal the hidden geometry of influence.\n",
        "\n",
        "The paper addresses the structural complexity of the modern aid system, characterized by a \"triple revolution\" of new goals, instruments, and actors. This codebase operationalizes the paper's framework, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration via a single `config.yaml` file.\n",
        "-   Cleanse and normalize over 10 million transaction records from the International Aid Transparency Initiative (IATI).\n",
        "-   Construct a bipartite Provider-Receiver network and project it into a Provider-Provider co-investment graph.\n",
        "-   Learn high-dimensional structural embeddings using Node2Vec to capture functional roles.\n",
        "-   Reveal functional clusters (Humanitarian vs. Development) via UMAP dimensionality reduction.\n",
        "-   Identify the \"Solar System\" of central actors using Hub Scores (HITS) and Betweenness Centrality.\n",
        "-   Validate findings externally by correlating offline structural influence with online web authority (PageRank).\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods combine techniques from Network Science, Graph Theory, and Machine Learning.\n",
        "\n",
        "**1. Bipartite Network Construction:**\n",
        "The system is modeled as a bipartite graph $G = (U, V, E)$, where $U$ represents Provider organisations and $V$ represents Receiver organisations. An edge $e_{uv}$ exists if a financial transaction occurs, weighted by frequency.\n",
        "\n",
        "**2. One-Mode Projection:**\n",
        "To analyze donor relationships, the bipartite graph is projected into a Provider-Provider co-occurrence network $P = MM^T$, where $M$ is the incidence matrix of providers appearing in shared contexts (countries or sectors).\n",
        "\n",
        "**3. Structural Embedding (Node2Vec):**\n",
        "The topology is encoded into a low-dimensional vector space $f: V \\to \\mathbb{R}^d$ by optimizing a Skip-gram objective over biased random walks:\n",
        "$$ \\max_f \\sum_{u \\in V} \\log \\Pr(N_S(u) | f(u)) $$\n",
        "This captures structural equivalence, grouping actors with similar network roles regardless of direct connectivity.\n",
        "\n",
        "**4. Centrality and Brokerage:**\n",
        "Influence is quantified using HITS Hub Scores (for the \"Solar System\" ranking) and Betweenness Centrality (for identifying brokers):\n",
        "$$ C_B(v) = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}} $$\n",
        "This metric highlights actors like J-PAL and the Hewlett Foundation that bridge structural holes between disparate clusters.\n",
        "\n",
        "Below is a diagram which summarizes the proposed approach:\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`who_connects_global_aid_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The pipeline is decomposed into 29 distinct, modular tasks, each with its own orchestrator function.\n",
        "-   **Configuration-Driven Design:** All study parameters (ER thresholds, Node2Vec hyperparameters, UMAP settings) are managed in an external `config.yaml` file.\n",
        "-   **Rigorous Data Validation:** A multi-stage validation process checks schema integrity, type coercion feasibility, and referential integrity.\n",
        "-   **Deterministic Entity Resolution:** Implements a robust TF-IDF blocking and Cosine Similarity pipeline to resolve organisation names to canonical identities.\n",
        "-   **High-Performance Computing:** Utilizes Numba-accelerated random walk generation and sparse matrix algebra for scalability.\n",
        "-   **Reproducible Artifacts:** Generates structured dictionaries, serializable outputs, and cryptographic manifests for every intermediate result.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation & Cleansing (Tasks 1-8):** Ingests raw IATI data, validates schemas, enforces temporal scope (1967-2025), and normalizes multi-valued fields.\n",
        "2.  **Integration & ER (Tasks 9-10):** Joins transactions to activity contexts and performs entity resolution to construct a canonical organisation map.\n",
        "3.  **Descriptive Analysis (Tasks 11-12):** Computes geographic transaction density and longitudinal instrument evolution.\n",
        "4.  **Network Construction (Tasks 13-15):** Builds the bipartite graph and projects it into a provider co-occurrence network.\n",
        "5.  **Topology & Embeddings (Tasks 16-17):** Generates Node2Vec embeddings and projects them to 2D using UMAP to reveal functional clusters.\n",
        "6.  **Centrality & Ranking (Tasks 18-20):** Computes HITS and Betweenness centrality to construct the \"Solar System\" ranking.\n",
        "7.  **Analysis & Validation (Tasks 21-26):** Analyzes subgroups (Universities/Foundations), characterizes broker networks (Hewlett), and correlates findings with web PageRank.\n",
        "8.  **Orchestration & Provenance (Tasks 27-29):** Manages the end-to-end execution and packages reproducible outputs.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `who_connects_global_aid_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 29 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `run_global_aid_pipeline`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`run_global_aid_pipeline`:** This master orchestrator function runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project, managing data flow between cleansing, graph construction, and analysis modules.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `networkx`, `gensim`, `scikit-learn`, `umap-learn`, `numba`, `pyyaml`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/who_connects_global_aid.git\n",
        "    cd who_connects_global_aid\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scipy networkx gensim scikit-learn umap-learn numba pyyaml\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires four primary DataFrames:\n",
        "1.  **`df_transactions_raw`**: IATI transaction elements (Value, Date, Provider, Receiver).\n",
        "2.  **`df_activities_raw`**: Activity metadata (Sectors, Countries, Instruments).\n",
        "3.  **`df_organisations_raw`**: Master organisation list for entity resolution.\n",
        "4.  **`df_web_links_raw`**: Web crawl data for external validation.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `who_connects_global_aid_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell of the notebook, which demonstrates how to use the top-level `execute_master_workflow` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # 1. Load the master configuration from the YAML file.\n",
        "    import yaml\n",
        "    with open('config.yaml', 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        "    # 2. Load raw datasets (Example using synthetic generator provided in the notebook)\n",
        "    # In production, load from CSV/Parquet: pd.read_csv(...)\n",
        "    data_inputs = generate_synthetic_data()\n",
        "    \n",
        "    # 3. Execute the entire replication study.\n",
        "    results = execute_master_workflow(\n",
        "        df_transactions_raw=data_inputs[\"df_transactions_raw\"],\n",
        "        df_activities_raw=data_inputs[\"df_activities_raw\"],\n",
        "        df_organisations_raw=data_inputs[\"df_organisations_raw\"],\n",
        "        df_web_links_raw=data_inputs[\"df_web_links_raw\"],\n",
        "        config=config,\n",
        "        output_root=\"./global_aid_study_output\"\n",
        "    )\n",
        "    \n",
        "    # 4. Access results\n",
        "    if results.success:\n",
        "        print(f\"Validation Correlation: {results.baseline_results.artifacts['correlation'].pearson_r}\")\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline returns a `MasterWorkflowResults` object containing:\n",
        "-   **`baseline_results`**: A `PipelineResults` object with all artifacts from the primary run (Graph, Embeddings, Centrality Tables).\n",
        "-   **`robustness_results`**: A `RobustnessArtifact` containing sensitivity analysis metrics.\n",
        "-   **`provenance_artifact`**: A `ProvenanceArtifact` with cryptographic hashes and metadata summaries.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "who_connects_global_aid/\n",
        "│\n",
        "├── who_connects_global_aid_draft.ipynb  # Main implementation notebook\n",
        "├── config.yaml                          # Master configuration file\n",
        "├── requirements.txt                     # Python package dependencies\n",
        "│\n",
        "├── LICENSE                              # MIT Project License File\n",
        "└── README.md                            # This file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can modify study parameters such as:\n",
        "-   **Scope:** `start_year`, `end_year`.\n",
        "-   **Entity Resolution:** `fuzzy_matching` threshold.\n",
        "-   **Node2Vec:** `p`, `q`, `walk_length`, `num_walks`.\n",
        "-   **UMAP:** `n_neighbors`, `min_dist`.\n",
        "-   **Projection:** Context definitions (Country vs. Sector).\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Temporal Dynamics:** Analyzing the evolution of the network topology over sliding windows.\n",
        "-   **Multiplex Networks:** Modeling different financial instruments (Grants vs. Loans) as distinct layers.\n",
        "-   **Impact Analysis:** Correlating network centrality with aid effectiveness outcomes.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{mccarthy2025who,\n",
        "  title={Who Connects Global Aid? The Hidden Geometry of 10 Million Transactions},\n",
        "  author={McCarthy, Paul X. and Gong, Xian and Rizoiu, Marian-Andrei and Boldi, Paolo},\n",
        "  journal={arXiv preprint arXiv:2512.17243},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). Global Aid Network Analysis Pipeline: An Open Source Implementation.\n",
        "GitHub repository: https://github.com/chirindaopensource/who_connects_global_aid\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Paul X. McCarthy, Xian Gong, Marian-Andrei Rizoiu, and Paolo Boldi** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, SciPy, NetworkX, Gensim, and UMAP**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `who_connects_global_aid_draft.ipynb` notebook and follows best practices for research software documentation.*\n"
      ],
      "metadata": {
        "id": "iLvo9IXHTqW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Who Connects Global Aid? The Hidden Geometry of 10 Million Transactions*\"\n",
        "\n",
        "Authors: Paul X. McCarthy, Xian Gong, Marian-Andrei Rizoiu, Paolo Boldi\n",
        "\n",
        "E-Journal Submission Date: 19 December 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2512.17243\n",
        "\n",
        "Abstract:\n",
        "\n",
        "The global aid system functions as a complex and evolving ecosystem; yet widespread understanding of its structure remains largely limited to aggregate volume flows. Here we map the network topology of global aid using a dataset of unprecedented scale: over 10 million transaction records connecting 2,456 publishing organisations across 230 countries between 1967 and 2025. We apply bipartite projection and dimensionality reduction to reveal the geometry of the system and unveil hidden patterns. This exposes distinct functional clusters that are otherwise sparsely connected. We find that while governments and multilateral agencies provide the primary resources, a small set of knowledge brokers provide the critical connectivity. Universities and research foundations specifically act as essential bridges between disparate islands of implementers and funders. We identify a core solar system of 25 central actors who drive this connectivity including unanticipated brokers like J-PAL and the Hewlett Foundation. These findings demonstrate that influence in the aid ecosystem flows through structural connectivity as much as financial volume. Our results provide a new framework for donors to identify strategic partners that accelerate coordination and evidence diffusion across the global network."
      ],
      "metadata": {
        "id": "pk56KQDvsqYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "**Subject:** Network Topology and Systemic Risk in the Global Aid Ecosystem\n",
        "\n",
        "**Methodology:** Bipartite Graph Projection, `node2vec` Embeddings, UMAP Dimensionality Reduction\n",
        "\n",
        "**Data Source:** International Aid Transparency Initiative (IATI) Registry (1967–2025)\n",
        "\n",
        "### Executive Abstract\n",
        "This paper represents a paradigm shift in the econometric analysis of development assistance. Historically, we have modeled global aid as a linear function of aggregate volume—essentially, \"who spends the most.\" The authors argue this volume-based view is an artifact of outdated statistical aggregation. By treating the aid ecosystem as a complex adaptive system and analyzing over 10 million transaction records, they reveal a \"hidden geometry.\" The core finding is that **structural influence is orthogonal to financial volume.** While governments provide the capital, a small set of \"knowledge brokers\" (universities and foundations) provide the essential connectivity that prevents the system from fracturing into isolated islands.\n",
        "\n",
        "--\n",
        "\n",
        "### The Data and Computational Framework\n",
        "To understand the topology, the authors moved beyond standard regression models of donor-recipient dyads. They employed a graph-theoretic approach to handle a dataset of unprecedented scale.\n",
        "\n",
        "*   **The Corpus:** The study utilizes the IATI registry, comprising **10 million transaction records** involving **2,456 publishing organizations** across **230 countries** between 1967 and 2025.\n",
        "*   **Graph Construction:** The system was modeled as a **bipartite graph** (Provider nodes $\\leftrightarrow$ Receiver nodes). To analyze the relationships between organizations, they performed a bipartite projection to create a Provider-Provider co-investment graph. Edges were weighted by frequency of co-occurrence rather than raw dollar volume to emphasize relational strength.\n",
        "*   **Algorithmic Embedding:** To capture the latent structure, they applied **`node2vec`**, an algorithm that generates high-dimensional vector representations of nodes based on random walks. This captures the \"structural equivalence\" of actors—grouping them not by geography, but by their topological roles.\n",
        "\n",
        "### Dimensionality Reduction and The \"Hidden Geometry\"\n",
        "Using **Uniform Manifold Approximation and Projection (UMAP)**, the authors projected these high-dimensional embeddings into a 2D manifold. This revealed that the aid ecosystem is not a cohesive whole, but is defined by two primary axes of differentiation:\n",
        "\n",
        "1.  **The Horizontal Axis (Thematic):** A sharp divide between **Humanitarian** actors (crisis response, short-term) and **Development** institutions (long-term resilience).\n",
        "2.  **The Vertical Axis (Functional):** A stratification between **Funders** (upstream capital allocators) and **Implementers** (downstream NGOs and contractors).\n",
        "\n",
        "**The Econometric Implication:** This orthogonality creates distinct \"quadrants\" with sparse connectivity between them. There are significant \"structural holes\" where coordination fails, particularly in the transition from crisis relief to long-term development.\n",
        "\n",
        "### The \"Solar System\" of Centrality\n",
        "The authors introduce a \"Solar System\" visualization to rank organizations based on **Network Centrality (Hub Score)** rather than financial volume. This yields a counter-intuitive architecture:\n",
        "\n",
        "*   **The Inner Ring (Top 25):** While this core includes financial giants like the World Bank and USAID, it also includes actors with much smaller budgets but disproportionate influence.\n",
        "*   **The Paradox of Scale:** The analysis identifies a class of **\"Knowledge Brokers.\"** For example, **J-PAL (MIT)** and the **Hewlett Foundation** sit in the \"inner ring\" of centrality. Despite managing fewer deals and less capital than bilateral agencies, they occupy critical bridging positions (high betweenness centrality).\n",
        "\n",
        "### The Role of Knowledge Brokers\n",
        "The paper argues that the resilience of the global aid network depends on these brokers.\n",
        "\n",
        "*   **Universities and Foundations:** These entities act as the connective tissue. Academic institutions are 85% smaller by deal count than the average but occupy significantly more central positions.\n",
        "*   **Bridging the Divide:** The analysis shows that actors like the Hewlett Foundation and J-PAL sit on the shortest paths between the disconnected \"islands\" of the network (e.g., connecting a humanitarian NGO to a development bank).\n",
        "*   **Evidence Diffusion:** Without these brokers, information (such as evidence from randomized control trials) cannot traverse the structural holes between the development and humanitarian clusters.\n",
        "\n",
        "### Systemic Risk and Strategic Implications\n",
        "From a systems engineering perspective, the current architecture exhibits signs of fragility.\n",
        "\n",
        "*   **Fragility of the Core:** The system relies heavily on a \"Donor Core\" (USAID, EU, World Bank). In network theory, a hub-and-spoke model is resilient to random failure but highly vulnerable to targeted disruption (e.g., political shifts in a major donor nation).\n",
        "*   **Resilience via Mesh Topology:** The authors argue for strengthening the \"middle layer\" of knowledge brokers to create a distributed mesh topology. This allows the network to \"reroute\" around damaged nodes.\n",
        "*   **Optimization Strategy:** For donors, the optimal strategy is not merely to fund \"doers\" (high-volume implementers) but to invest in \"connectors.\" Funding a central broker provides access to the structural integrity of the entire network, effectively de-risking the portfolio.\n",
        "\n",
        "### Conclusion\n",
        "The paper concludes that we are transitioning from an era of **\"Aid as Charity\"** (unidirectional volume transfer) to **\"Aid as Network\"** (multi-directional connectivity). The metric for success in this new topology is not just capital deployment, but the optimization of brokerage to ensure evidence diffusion and coordination across a fragmented system."
      ],
      "metadata": {
        "id": "CPxuOIPAubsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "rpQZKKbkxRVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  Who Connects Global Aid? The Hidden Geometry of 10 Million Transactions\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Who Connects Global Aid? The Hidden Geometry\n",
        "#  of 10 Million Transactions\" by Paul X. McCarthy, Xian Gong, Marian-Andrei\n",
        "#  Rizoiu, and Paolo Boldi (2025). It delivers a computationally rigorous system\n",
        "#  for mapping the topological structure of the global aid ecosystem, enabling\n",
        "#  the identification of critical knowledge brokers, the quantification of\n",
        "#  structural influence versus financial volume, and the detection of functional\n",
        "#  clusters within the donor-implementer network.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Bipartite network construction from large-scale transaction records (IATI)\n",
        "#  • One-mode projection via co-occurrence frequency weighting\n",
        "#  • High-dimensional structural embedding using Node2Vec (biased random walks)\n",
        "#  • Manifold learning and dimensionality reduction via UMAP\n",
        "#  • Network centrality analysis: Hub Scores (HITS), Betweenness, and Degree\n",
        "#  • \"Solar System\" visualization ranking based on structural influence\n",
        "#  • External validation via web hyperlink graph PageRank correlation\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Scalable ETL pipeline for processing 10M+ transaction records\n",
        "#  • Deterministic entity resolution using TF-IDF blocking and Cosine Similarity\n",
        "#  • Numba-accelerated random walk generation for efficient graph sampling\n",
        "#  • Sparse matrix algebra for handling large-scale adjacency structures\n",
        "#  • Robust artifact management with cryptographic provenance tracking\n",
        "#  • Comprehensive sensitivity analysis for parameter robustness verification\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  McCarthy, P. X., Gong, X., Rizoiu, M.-A., & Boldi, P. (2025). Who Connects\n",
        "#  Global Aid? The Hidden Geometry of 10 Million Transactions. arXiv preprint\n",
        "#  arXiv:2512.17243. https://arxiv.org/abs/2512.17243\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Standard Library Imports\n",
        "# ------------------------------------------------------------------------------\n",
        "import datetime\n",
        "import hashlib\n",
        "import itertools\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "import traceback\n",
        "from dataclasses import asdict, dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Set, Tuple, Union\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Third-Party Scientific Computing & Data Analysis Imports\n",
        "# ------------------------------------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sparse\n",
        "import scipy.stats as stats\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Network Analysis & Graph Theory Imports\n",
        "# ------------------------------------------------------------------------------\n",
        "import networkx as nx\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Machine Learning & Natural Language Processing Imports\n",
        "# ------------------------------------------------------------------------------\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# High-Performance Computing & JIT Compilation Imports\n",
        "# ------------------------------------------------------------------------------\n",
        "import numba\n",
        "from numba import njit\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Manifold Learning & Dimensionality Reduction Imports\n",
        "# ------------------------------------------------------------------------------\n",
        "import umap\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Configure logging for this module\n",
        "# ------------------------------------------------------------------------------\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "if not logger.handlers:\n",
        "    handler = logging.StreamHandler()\n",
        "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "    handler.setFormatter(formatter)\n",
        "    logger.addHandler(handler)\n"
      ],
      "metadata": {
        "id": "rNPe30ub-NDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "5QAaJ72sxW9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Draft 1\n",
        "\n",
        "## **Discussion of the Inputs, Processes, Outputs, and Research Role of Key Callables**\n",
        "\n",
        "### **Task 1: `validate_study_configuration`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `config` (Dict): The master configuration dictionary containing study parameters (scope, filters, hyperparameters).\n",
        "    *   `input_data` (Dict): A dictionary of loaded raw DataFrames (`df_transactions_raw`, etc.).\n",
        "*   **Processes:**\n",
        "    *   **Scope Validation:** Asserts `start_year == 1967` and `end_year == 2025`.\n",
        "    *   **Methodological Purity Check:** Recursively scans the configuration for forbidden terms (e.g., \"LPPLS\", \"Lomb-Scargle\") to ensure strict adherence to the manuscript's methods.\n",
        "    *   **Dataset Presence Check:** Verifies that all required keys (e.g., `df_web_links_raw`) exist in `input_data`.\n",
        "    *   **Schema Validation:** Checks that each DataFrame contains the mandatory columns defined in the pre-code analysis.\n",
        "*   **Outputs:**\n",
        "    *   `bool`: Returns `True` if all validations pass; raises `ValueError` otherwise.\n",
        "*   **Research Role:**\n",
        "    *   This callable enforces the boundary conditions of the study as defined in the **Introduction** and **Methods** sections. It ensures the analysis is performed on the correct temporal window (\"1967 and 2025\") and utilizes the specific datasets (\"IATI registry... over 10 million transaction records\") required to reproduce the findings.\n",
        "\n",
        "### **Task 2: `validate_data_quality`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_transactions` (DataFrame): Raw transaction records.\n",
        "    *   `df_activities` (DataFrame): Raw activity metadata.\n",
        "    *   `df_organisations` (DataFrame): Master organisation list.\n",
        "*   **Processes:**\n",
        "    *   **Key Integrity:** Computes duplicate rates for `transaction_id` and `iati_identifier`. Calculates the intersection-over-union for the join keys between transactions and activities.\n",
        "    *   **Type Coercion Simulation:** Attempts to convert `transaction_value` to numeric and dates to datetime objects (using `errors='coerce'`) to quantify the rate of malformed data without mutating the input.\n",
        "    *   **Reference Coverage:** Calculates the percentage of provider and receiver references in the transaction table that map to entries in the organisation master list.\n",
        "*   **Outputs:**\n",
        "    *   `Dict[str, Any]`: A dictionary of quality metrics (e.g., `transaction_id_duplicate_rate`, `bad_numeric_values`, `provider_ref_match_rate`).\n",
        "*   **Research Role:**\n",
        "    *   This callable implements the data quality assessment implied by the manuscript's description of the \"complex and evolving ecosystem.\" By quantifying missing references and malformed dates, it establishes the reliability of the \"10 million transaction records\" before they are used to construct the network topology.\n",
        "\n",
        "### **Task 3: `establish_reproducibility_context`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `config` (Dict): Master configuration.\n",
        "    *   `input_data` (Dict): Raw DataFrames.\n",
        "*   **Processes:**\n",
        "    *   **Fingerprinting:** Computes row counts, null rates, and a structural hash for each input DataFrame to create an immutable record of the starting state.\n",
        "    *   **Seed Resolution:** Extracts or assigns fixed integer seeds for stochastic components (`node2vec`, `UMAP`) to ensure deterministic execution.\n",
        "    *   **Sort Policy Definition:** Defines the immutable column ordering rules (e.g., sort transactions by ID, Date, Value) used for deterministic deduplication.\n",
        "*   **Outputs:**\n",
        "    *   `ReproducibilityContext`: A dataclass containing `fingerprints`, `seeds`, and `sort_policies`.\n",
        "*   **Research Role:**\n",
        "    *   This callable establishes the computational provenance required for scientific reproducibility. It ensures that the \"hidden geometry\" revealed by the analysis is a stable property of the data and algorithms, not an artifact of random initialization or non-deterministic processing order.\n",
        "\n",
        "### **Task 4: `cleanse_transactions_temporal_value`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_transactions_raw` (DataFrame): Raw transactions.\n",
        "    *   `config` (Dict): Configuration containing scope parameters.\n",
        "*   **Processes:**\n",
        "    *   **Date Parsing:** Converts ISO-8601 date strings to datetime objects, quarantining failures.\n",
        "    *   **Value Filtering:** Coerces transaction values to numeric and applies the strict inequality filter $V(t) > 0$.\n",
        "    *   **Scope Filtering:** Filters transactions where the year falls within $[1967, 2025]$.\n",
        "*   **Outputs:**\n",
        "    *   `Tuple[DataFrame, TransactionExclusions]`: The cleansed DataFrame ($T^{++}$) and a ledger of excluded row IDs.\n",
        "*   **Research Role:**\n",
        "    *   This callable implements the specific data selection criteria described in the **Methods** section: \"filtered for nonzero financial transactions\" and \"covers the period from 1967 to 2025.\" It defines the set of valid edges $E$ for the network construction.\n",
        "\n",
        "### **Task 5: `cleanse_transactions_endpoints_dedup`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_transactions_filtered` (DataFrame): Temporally cleansed transactions.\n",
        "    *   `sort_policy` (SortPolicy): Deterministic sorting rules.\n",
        "*   **Processes:**\n",
        "    *   **Endpoint Validation:** Checks that both provider and receiver identifiers (ref or name) are present and non-empty.\n",
        "    *   **Deduplication:** Sorts the DataFrame according to the `sort_policy` and removes duplicate `transaction_id` entries, keeping the first occurrence.\n",
        "*   **Outputs:**\n",
        "    *   `Tuple[DataFrame, EndpointExclusions]`: The fully cleansed transaction table and an exclusion ledger.\n",
        "*   **Research Role:**\n",
        "    *   This callable ensures the topological validity of the network. By enforcing that every transaction has a valid source and target, it guarantees that the bipartite graph $G = (U, V, E)$ is well-defined, where $U$ are providers and $V$ are receivers.\n",
        "\n",
        "### **Task 6: `cleanse_activities_normalization`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_activities_raw` (DataFrame): Raw activity metadata.\n",
        "    *   `sort_policy` (SortPolicy): Sorting rules.\n",
        "*   **Processes:**\n",
        "    *   **Backbone Cleansing:** Deduplicates activities on `iati_identifier`.\n",
        "    *   **Normalization:** Explodes multi-valued `recipient_country_code` and `sector_code` fields into long-form DataFrames (one row per activity-context pair).\n",
        "*   **Outputs:**\n",
        "    *   `Tuple[DataFrame, DataFrame, DataFrame, ActivityExclusions]`: The activity backbone, exploded countries, exploded sectors, and exclusions.\n",
        "*   **Research Role:**\n",
        "    *   This callable prepares the metadata required for the \"Provider-Provider co-investment graph.\" By normalizing countries and sectors, it enables the calculation of the co-occurrence matrix $M$, where $M_{ix}$ represents the frequency of provider $i$ operating in context $x$.\n",
        "\n",
        "### **Task 7: `cleanse_activities_instruments_coverage`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_backbone` (DataFrame): Cleaned activity backbone.\n",
        "    *   `df_countries` (DataFrame): Normalized countries.\n",
        "    *   `df_sectors` (DataFrame): Normalized sectors.\n",
        "*   **Processes:**\n",
        "    *   **Instrument Normalization:** Maps `instrument_type` values to the controlled vocabulary $\\{ \\text{Grant, Loan, Equity} \\}$, setting invalid entries to `NaN`.\n",
        "    *   **Coverage Calculation:** Computes the percentage of activities that have valid instrument, country, and sector data.\n",
        "*   **Outputs:**\n",
        "    *   `Tuple[DataFrame, InstrumentExclusions, ActivityCoverageReport]`: The normalized backbone, exclusions, and coverage stats.\n",
        "*   **Research Role:**\n",
        "    *   This callable standardizes the financial instrument data used to generate **Extended Data Fig. 1** (\"Evolution of aid instruments\"). It ensures that the analysis of \"new instruments\" mentioned in the **Introduction** is based on a consistent taxonomy.\n",
        "\n",
        "### **Task 8: `cleanse_organisations_substrate`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_organisations_raw` (DataFrame): Raw organisation master list.\n",
        "*   **Processes:**\n",
        "    *   **Record Integrity:** Removes rows with missing `org_ref`.\n",
        "    *   **Normalization:** Standardizes `org_name` (NFKD, lowercase) and `website_domain` (strip protocol/path).\n",
        "    *   **Taxonomy Validation:** Enforces the controlled vocabulary for `org_type`.\n",
        "    *   **Alias Preparation:** Parses and normalizes `org_name_aliases`.\n",
        "*   **Outputs:**\n",
        "    *   `Tuple[DataFrame, OrganisationExclusions]`: The cleansed organisation table and exclusions.\n",
        "*   **Research Role:**\n",
        "    *   This callable prepares the \"substrate\" for entity resolution. By standardizing names and types, it facilitates the accurate mapping of the \"2,456 distinct publishing organisations\" described in the **Abstract**, ensuring that the nodes in the network represent unique real-world entities.\n",
        "\n",
        "### **Task 9: `join_transactions_to_contexts`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_transactions` (DataFrame): Cleansed transactions.\n",
        "    *   `df_activities_backbone` (DataFrame): Activity metadata.\n",
        "    *   `df_activities_countries` (DataFrame): Country contexts.\n",
        "    *   `df_activities_sectors` (DataFrame): Sector contexts.\n",
        "*   **Processes:**\n",
        "    *   **Context Joining:** Performs left joins to attach instruments (1:1) and inner joins to attach countries/sectors (1:M) to transactions.\n",
        "    *   **Unified Context Construction:** Concatenates country and sector contexts into a single long-form table with namespaced IDs (e.g., `COUNTRY:US`, `SECTOR:111`).\n",
        "*   **Outputs:**\n",
        "    *   `Tuple[DataFrame, DataFrame, ContextCoverageReport]`: Transactions with instruments, unified contexts, and coverage stats.\n",
        "*   **Research Role:**\n",
        "    *   This callable links the financial flows (transactions) to their operational contexts. It constructs the set of contexts $\\mathcal{X}$ used in the bipartite projection $P = MM^T$, where $M_{ix} = \\sum_{t} \\mathbf{1}[\\tilde{u}(t) = i] \\cdot \\mathbf{1}[\\kappa(t) = x]$.\n",
        "\n",
        "### **Task 10: `construct_canonical_mapping`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_organisations` (DataFrame): Cleansed organisation list.\n",
        "    *   `config_dict` (Dict): Configuration parameters.\n",
        "*   **Processes:**\n",
        "    *   **Precomputed Check:** Checks for an existing trusted mapping.\n",
        "    *   **Fuzzy Clustering:** If needed, uses TF-IDF vectorization and Cosine Similarity to cluster organisation names (blocking strategy).\n",
        "    *   **Canonicalization:** Maps raw `org_ref` to a unique `canonical_org_id` based on the clusters.\n",
        "*   **Outputs:**\n",
        "    *   `Tuple[DataFrame, CanonicalMappingArtifact]`: The mapped organisation table and the mapping artifact.\n",
        "*   **Research Role:**\n",
        "    *   This callable implements the entity resolution required to handle the \"diverse population of 2,456 distinct publishing organisations.\" It ensures that variations in naming (e.g., \"USAID\" vs. \"United States Agency for International Development\") are resolved to a single node in the network graph.\n",
        "\n",
        "### **Task 11: `compute_geographic_density`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_transactions` (DataFrame): Cleansed transactions.\n",
        "    *   `df_tx_countries` (DataFrame): Transaction-country mapping.\n",
        "*   **Processes:**\n",
        "    *   **Aggregation:** Counts unique transactions per recipient country: $D(c) = \\sum_{t \\in T^{++}} \\mathbf{1}[c(t) = c]$.\n",
        "    *   **Transformation:** Applies log transformation $D_{\\log}(c) = \\log(1 + D(c))$.\n",
        "*   **Outputs:**\n",
        "    *   `GeoDensityArtifact`: Contains the density table and coverage metadata.\n",
        "*   **Research Role:**\n",
        "    *   This callable generates the data for **Figure 1** (\"Global distribution of 10 million aid transactions\"). It quantifies the \"geographic footprint\" of the aid system, revealing the \"high-density belts\" and \"sparser connectivity\" described in the **Results**.\n",
        "\n",
        "### **Task 12: `compute_instrument_evolution`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_tx_instruments` (DataFrame): Transactions with instrument types.\n",
        "*   **Processes:**\n",
        "    *   **Aggregation:** Counts transactions by year and instrument type: $C(y, i) = \\sum_{t \\in T^{++}} \\mathbf{1}[\\mathrm{year}(d(t)) = y] \\cdot \\mathbf{1}[\\mathrm{instrument}(t) = i]$.\n",
        "    *   **Pivoting:** Reshapes the data into a wide-form time series table.\n",
        "*   **Outputs:**\n",
        "    *   `InstrumentEvolutionArtifact`: Time series data and validation report.\n",
        "*   **Research Role:**\n",
        "    *   This callable produces the data for **Extended Data Fig. 1**. It tracks the \"complex mix of financial instruments including standard grants, aid loans and equity investments\" over the 1967–2025 period, supporting the narrative of a \"triple revolution\" in aid architecture.\n",
        "\n",
        "### **Task 13: `construct_bipartite_graph`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_transactions` (DataFrame): Cleansed transactions.\n",
        "    *   `canonical_mapping` (DataFrame): Entity resolution map.\n",
        "*   **Processes:**\n",
        "    *   **Node Preparation:** Maps transaction endpoints to canonical IDs and appends role suffixes (`::PROVIDER`, `::RECEIVER`) to ensure disjoint sets $U$ and $V$.\n",
        "    *   **Matrix Construction:** Builds the sparse frequency-weighted incidence matrix $B$ where $B_{ij}$ is the count of transactions between provider $i$ and receiver $j$.\n",
        "*   **Outputs:**\n",
        "    *   `BipartiteGraphArtifact`: Contains the sparse matrix $B$, node tables, and index maps.\n",
        "*   **Research Role:**\n",
        "    *   This callable constructs the fundamental topological object of the study: the bipartite graph $G = (U, V, E)$. This graph serves as the basis for the \"Hub Score\" calculation and the subsequent one-mode projection.\n",
        "\n",
        "### **Task 14: `compute_node_sizes`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_tx_mapped` (DataFrame): Transactions with canonical IDs.\n",
        "    *   `df_organisations` (DataFrame): Master organisation list.\n",
        "*   **Processes:**\n",
        "    *   **Deal Counting:** Sums the number of transactions where an organisation appears as either a provider or a receiver: $\\text{deals}(o) = \\sum_{t} \\mathbf{1}[o \\in \\{u(t), v(t)\\}]$.\n",
        "    *   **Validation:** Computes summary statistics by organisation type.\n",
        "*   **Outputs:**\n",
        "    *   `NodeSizeArtifact`: Deal counts and validation stats.\n",
        "*   **Research Role:**\n",
        "    *   This callable computes the \"node size\" metric used in the **Solar System** visualization (Figure 3). It allows the study to contrast \"activity volume (size)\" with \"structural influence (position),\" revealing the paradox where smaller actors (like universities) can be highly central.\n",
        "\n",
        "### **Task 15: `construct_co_occurrence_projection`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_contexts` (DataFrame): Unified contexts.\n",
        "    *   `df_tx_mapped` (DataFrame): Mapped transactions.\n",
        "*   **Processes:**\n",
        "    *   **Incidence Construction:** Builds the provider-context matrix $M$.\n",
        "    *   **Projection:** Computes the one-mode projection $P = MM^T$ via sparse matrix multiplication and removes self-loops ($P_{ii} \\leftarrow 0$).\n",
        "*   **Outputs:**\n",
        "    *   `ProjectionArtifact`: The symmetric adjacency matrix $P$ and edge list.\n",
        "*   **Research Role:**\n",
        "    *   This callable implements the \"bipartite projection\" mentioned in the **Abstract**. It transforms the transaction data into a \"Provider-Provider co-investment graph,\" where edges represent shared funding or implementation contexts. This graph is the substrate for the centrality and community detection analyses.\n",
        "\n",
        "### **Task 16: `learn_node_embeddings`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `bipartite_artifact` (BipartiteGraphArtifact): The bipartite graph.\n",
        "    *   `config` (Dict): Hyperparameters.\n",
        "*   **Processes:**\n",
        "    *   **Graph Preparation:** Constructs the symmetric adjacency $A = \\begin{pmatrix} 0 & B \\\\ B^\\top & 0 \\end{pmatrix}$.\n",
        "    *   **Random Walks:** Generates biased random walks using Numba-accelerated sampling, implementing the 2nd-order transition probabilities $\\pi_{vx} \\propto \\alpha_{pq}(t, x) \\cdot w_{vx}$.\n",
        "    *   **Skip-gram Training:** Trains a Word2Vec model on the walks to learn 100-dimensional embeddings $f(u)$ that maximize $\\sum \\log \\Pr(N_S(u) | f(u))$.\n",
        "*   **Outputs:**\n",
        "    *   `EmbeddingArtifact`: The learned embeddings and metadata.\n",
        "*   **Research Role:**\n",
        "    *   This callable implements the `node2vec` algorithm to capture the \"structural equivalence\" of actors. These high-dimensional embeddings are the input for the dimensionality reduction step that reveals the \"hidden geometry\" of the system.\n",
        "\n",
        "### **Task 17: `project_embeddings_umap`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `embedding_artifact` (EmbeddingArtifact): Node embeddings.\n",
        "    *   `config` (Dict): UMAP parameters.\n",
        "*   **Processes:**\n",
        "    *   **Dimensionality Reduction:** Applies Uniform Manifold Approximation and Projection (UMAP) to project the 100D embeddings into 2D space ($z(u) \\in \\mathbb{R}^2$).\n",
        "*   **Outputs:**\n",
        "    *   `UMAPArtifact`: 2D coordinates and axis interpretation.\n",
        "*   **Research Role:**\n",
        "    *   This callable generates the coordinates for **Figure 2** (\"The hidden geometry of the aid ecosystem\"). It reveals the \"distinct functional clusters\" (Humanitarian vs. Development, Funder vs. Implementer) that characterize the system's topology.\n",
        "\n",
        "### **Task 18: `compute_hits_centrality`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `bipartite_artifact` (BipartiteGraphArtifact): The bipartite graph.\n",
        "*   **Processes:**\n",
        "    *   **HITS Algorithm:** Iteratively computes Hub ($h$) and Authority ($a$) scores using the power iteration method: $a^{(k)} = B^\\top h^{(k-1)}$, $h^{(k)} = B a^{(k)}$, with L2 normalization.\n",
        "*   **Outputs:**\n",
        "    *   `HITSArtifact`: Hub and Authority scores.\n",
        "*   **Research Role:**\n",
        "    *   This callable calculates the \"Hub Score\" used to rank actors in the **Solar System** visualization. It identifies the \"core solar system of 25 central actors\" by measuring their ability to connect to a wide range of recipients in the bipartite network.\n",
        "\n",
        "### **Task 19: `compute_network_centrality`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `projection_artifact` (ProjectionArtifact): The provider graph.\n",
        "    *   `hits_artifact` (HITSArtifact): Hub scores.\n",
        "*   **Processes:**\n",
        "    *   **Degree/Strength:** Computes unweighted degree ($C_D$) and weighted strength ($C_D^w$) on the projection graph.\n",
        "    *   **Betweenness:** Computes unweighted Betweenness Centrality ($C_B$) using Brandes' algorithm: $C_B(v) = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}$.\n",
        "    *   **Unification:** Merges all metrics into a master centrality table.\n",
        "*   **Outputs:**\n",
        "    *   `CentralityArtifact`: Unified centrality metrics.\n",
        "*   **Research Role:**\n",
        "    *   This callable quantifies \"brokerage\" and \"connectivity.\" The Betweenness Centrality metric specifically identifies the \"knowledge brokers\" (like universities) that \"sit on the shortest paths between otherwise poorly connected donor-implementer clusters.\"\n",
        "\n",
        "### **Task 20: `construct_solar_system`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `centrality_artifact` (CentralityArtifact): Centrality scores.\n",
        "    *   `node_size_artifact` (NodeSizeArtifact): Node sizes.\n",
        "*   **Processes:**\n",
        "    *   **Ranking:** Ranks organisations by Hub Score descending.\n",
        "    *   **Sizing:** Attaches transaction counts as node sizes.\n",
        "    *   **Ring Assignment:** Assigns \"Inner\", \"Middle\", and \"Outer\" rings based on rank thresholds (Top 25, Top 100).\n",
        "*   **Outputs:**\n",
        "    *   `SolarSystemArtifact`: Ranked and sized node table for visualization.\n",
        "*   **Research Role:**\n",
        "    *   This callable synthesizes the data for **Figure 3** (\"The solar system architecture of global aid\"). It operationalizes the core-periphery structure described in the **Results**, highlighting the \"critical paradox between scale and influence.\"\n",
        "\n",
        "### **Task 21: `compute_subgroup_statistics`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_organisations` (DataFrame): Master organisation list.\n",
        "    *   `solar_system_artifact` (SolarSystemArtifact): Ranked nodes.\n",
        "*   **Processes:**\n",
        "    *   **Partitioning:** Filters ranked nodes into \"Universities\" and \"Foundations\" subgroups.\n",
        "    *   **Statistical Comparison:** Computes mean/median deal counts and median ranks for each subgroup vs. the global population.\n",
        "*   **Outputs:**\n",
        "    *   `SubgroupStatsArtifact`: Summary statistics and broker lookup.\n",
        "*   **Research Role:**\n",
        "    *   This callable generates the quantitative evidence for the claim that \"Universities and research foundations specifically act as essential bridges.\" It validates the finding that universities occupy a \"significantly more central position\" despite having smaller deal volumes.\n",
        "\n",
        "### **Task 22: `compare_broker_betweenness`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `centrality_artifact` (CentralityArtifact): Centrality scores.\n",
        "    *   `subgroup_artifact` (SubgroupStatsArtifact): Broker info.\n",
        "    *   `solar_system_artifact` (SolarSystemArtifact): Rankings.\n",
        "*   **Processes:**\n",
        "    *   **Extraction:** Retrieves Betweenness scores for specific brokers (J-PAL, Hewlett).\n",
        "    *   **Baseline Computation:** Calculates the median Betweenness of the Top 25 actors.\n",
        "    *   **Comparison:** Computes the ratio of broker scores to the baseline median.\n",
        "*   **Outputs:**\n",
        "    *   `BrokerComparisonArtifact`: Comparison table.\n",
        "*   **Research Role:**\n",
        "    *   This callable produces the data for **Extended Data Fig. 3**. It demonstrates that knowledge brokers are \"positive outliers\" in terms of structural brokerage, validating their role as \"connectors rather than just funders.\"\n",
        "\n",
        "### **Task 23: `characterize_hewlett_network`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_transactions` (DataFrame): Cleansed transactions.\n",
        "    *   `subgroup_artifact` (SubgroupStatsArtifact): Broker info.\n",
        "    *   `canonical_mapping_artifact` (CanonicalMappingArtifact): Mapping table.\n",
        "    *   `solar_system_artifact` (SolarSystemArtifact): Rankings.\n",
        "*   **Processes:**\n",
        "    *   **Ego Extraction:** Filters transactions where Hewlett is the provider.\n",
        "    *   **Metric Computation:** Counts unique partners, total transactions, and overlap with the Top 100.\n",
        "*   **Outputs:**\n",
        "    *   `HewlettNetworkArtifact`: Ego network edges and metrics.\n",
        "*   **Research Role:**\n",
        "    *   This callable generates the data for **Extended Data Fig. 2**. It visualizes the \"highly diverse portfolio of downstream partners\" supported by the Hewlett Foundation, illustrating how a central broker \"diffuse[s] evidence across the humanitarian-development divide.\"\n",
        "\n",
        "### **Task 24: `construct_web_graph`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_web_links_raw` (DataFrame): Raw web links.\n",
        "    *   `df_organisations` (DataFrame): Master orgs.\n",
        "*   **Processes:**\n",
        "    *   **Graph Construction:** Builds the directed adjacency matrix of the web graph from source/target domains.\n",
        "    *   **Coverage Validation:** Computes the overlap between organisation websites and web graph nodes.\n",
        "*   **Outputs:**\n",
        "    *   `WebGraphArtifact`: Adjacency matrix and coverage stats.\n",
        "*   **Research Role:**\n",
        "    *   This callable constructs the external validation network. It allows the study to compare the \"offline\" aid network with the \"online\" web network, testing the hypothesis that structural influence in aid is reflected in digital authority.\n",
        "\n",
        "### **Task 25: `compute_website_pagerank`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `web_graph_artifact` (WebGraphArtifact): The web graph.\n",
        "    *   `config` (Dict): Parameters.\n",
        "*   **Processes:**\n",
        "    *   **Matrix Preparation:** Constructs the column-stochastic transition matrix $M$.\n",
        "    *   **Power Iteration:** Iteratively computes the PageRank vector $x$ using the update rule $x^{(k+1)} = \\alpha M x^{(k)} + (1-\\alpha)/N \\mathbf{1}$.\n",
        "*   **Outputs:**\n",
        "    *   `PageRankArtifact`: PageRank scores.\n",
        "*   **Research Role:**\n",
        "    *   This callable computes the \"online visibility metrics\" used for validation. PageRank serves as an independent measure of an organisation's prominence, used to validate the Hub Scores derived from transaction data.\n",
        "\n",
        "### **Task 26: `compute_validation_correlation`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `centrality_artifact` (CentralityArtifact): Offline scores.\n",
        "    *   `pagerank_artifact` (PageRankArtifact): Online scores.\n",
        "    *   `df_organisations` (DataFrame): Master table.\n",
        "*   **Processes:**\n",
        "    *   **Alignment:** Joins Hub Scores and PageRank scores via organisation domains.\n",
        "    *   **Correlation:** Computes the Pearson correlation coefficient $r$ between the two vectors.\n",
        "*   **Outputs:**\n",
        "    *   `CorrelationArtifact`: Correlation results ($r$, $p$, $n$).\n",
        "*   **Research Role:**\n",
        "    *   This callable produces the validation metric reported in the **Methods** section ($r=0.48$). It confirms that \"influence in the aid ecosystem flows through structural connectivity\" by showing a strong link between financial centrality and web authority.\n",
        "\n",
        "### **Task 27: `run_global_aid_pipeline`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   Raw DataFrames and Config.\n",
        "*   **Processes:**\n",
        "    *   **Orchestration:** Sequentially executes Tasks 1 through 26, managing data flow and artifact persistence.\n",
        "    *   **Error Handling:** Catches and logs exceptions.\n",
        "*   **Outputs:**\n",
        "    *   `PipelineResults`: A container of all generated artifacts.\n",
        "*   **Research Role:**\n",
        "    *   This is the execution engine for the baseline analysis. It ensures that the entire research pipeline is run in the correct order and that all dependencies are satisfied.\n",
        "\n",
        "### **Task 28: `conduct_robustness_analysis`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   Baseline results, config, pipeline function, raw data.\n",
        "*   **Processes:**\n",
        "    *   **Grid Generation:** Creates a set of perturbed configurations (varying $\\tau, p, q$).\n",
        "    *   **Batch Execution:** Runs the pipeline for each configuration.\n",
        "    *   **Synthesis:** Computes rank correlations (Spearman) and validation stability (Pearson) across runs.\n",
        "*   **Outputs:**\n",
        "    *   `RobustnessArtifact`: Sensitivity analysis results.\n",
        "*   **Research Role:**\n",
        "    *   This callable implements the sensitivity analysis required to demonstrate the robustness of the findings. It ensures that the identified \"Solar System\" structure is not an artifact of specific parameter choices.\n",
        "\n",
        "### **Task 29: `package_reproducible_outputs`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   Pipeline results, config, output root.\n",
        "*   **Processes:**\n",
        "    *   **Manifest Generation:** Computes SHA-256 hashes for all persisted files.\n",
        "    *   **Metadata Consolidation:** Aggregates metrics and parameters into a master JSON.\n",
        "    *   **Table Generation:** Exports specific CSVs for manuscript figures.\n",
        "*   **Outputs:**\n",
        "    *   `ProvenanceArtifact`: The final reproducibility package.\n",
        "*   **Research Role:**\n",
        "    *   This callable ensures the study adheres to open science principles. It packages the results in a way that allows for independent verification and auditing of the \"10 million transaction records\" analysis.\n",
        "\n",
        "### **Top-Level: `execute_master_workflow`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   Raw DataFrames and Config.\n",
        "*   **Processes:**\n",
        "    *   **Coordination:** Invokes the baseline pipeline, robustness analysis, and packaging steps in order.\n",
        "*   **Outputs:**\n",
        "    *   `MasterWorkflowResults`: The complete set of study outputs.\n",
        "*   **Research Role:**\n",
        "    *   This is the single entry point for the entire research project. It encapsulates the full scientific workflow, from raw data to final validated results and provenance tracking.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **Usage Example**\n",
        "\n",
        "Below is a script which uses synthetically-generated data to demonstate how to use the \"Who Connects Global Aid?\" research pipeline accurately:\n",
        "\n",
        "```python\n",
        "#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "# Global Aid Network Analysis: End-to-End Pipeline Usage Example\n",
        "# ==============================================================================\n",
        "#\n",
        "# This script demonstrates, with high fidelity, the usage of the\n",
        "# \"Who Connects Global Aid?\" research pipeline. It synthetically generates\n",
        "# realistic test data conforming to the IATI schema, loads the study configuration\n",
        "# from a YAML file, and executes the master orchestrator to produce all\n",
        "# topological artifacts and validation metrics.\n",
        "#\n",
        "# Key Steps:\n",
        "# 1. Synthetic Data Generation (Faker): Creates realistic DataFrames for\n",
        "#    Transactions, Activities, Organisations, and Web Links.\n",
        "# 2. Configuration Loading: Reads 'config.yaml' into a Python dictionary.\n",
        "# 3. Pipeline Execution: Invokes `execute_master_workflow` to run the full\n",
        "#    analysis lifecycle (Cleansing -> Network Construction -> Topology -> Validation).\n",
        "# 4. Result Inspection: Demonstrates how to access and verify the generated artifacts.\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "from faker import Faker\n",
        "from typing import Dict, List\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Import the pipeline orchestrator and necessary classes\n",
        "# (In a real package, these would be imported from the module)\n",
        "# from global_aid_pipeline import execute_master_workflow, MasterWorkflowResults\n",
        "\n",
        "# Configure logging for the example execution\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(\"GlobalAidExample\")\n",
        "\n",
        "# Initialize Faker for synthetic data generation\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 1: Synthetic Data Generation\n",
        "# ==============================================================================\n",
        "# We generate data that mimics the structure and complexity of the IATI registry.\n",
        "\n",
        "def generate_synthetic_data(\n",
        "    n_orgs: int = 50,\n",
        "    n_activities: int = 200,\n",
        "    n_transactions: int = 1000,\n",
        "    n_web_links: int = 500\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Generates synthetic DataFrames for the Global Aid Network analysis.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_orgs : int\n",
        "        Number of unique organisations to generate.\n",
        "    n_activities : int\n",
        "        Number of unique aid activities (projects).\n",
        "    n_transactions : int\n",
        "        Number of financial transactions.\n",
        "    n_web_links : int\n",
        "        Number of web hyperlinks for validation.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, pd.DataFrame]\n",
        "        Dictionary containing the four required raw DataFrames.\n",
        "    \"\"\"\n",
        "    logger.info(\"Generating synthetic IATI data...\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 1. Organisations (df_organisations_raw)\n",
        "    # -------------------------------------------------------------------------\n",
        "    org_types = [\n",
        "        \"Government\", \"Multilateral\", \"Foundation\", \"Academic/Research\",\n",
        "        \"International NGO\", \"National NGO\", \"Private Sector\", \"Other\"\n",
        "    ]\n",
        "    \n",
        "    orgs = []\n",
        "    for i in range(n_orgs):\n",
        "        name = fake.company()\n",
        "        # Create realistic variations for aliases\n",
        "        aliases = [name, name.upper(), name.split()[0] + \" Org\"]\n",
        "        \n",
        "        orgs.append({\n",
        "            \"org_ref\": f\"ORG-{i:04d}\",\n",
        "            \"org_name\": name,\n",
        "            \"org_name_aliases\": aliases, # List of strings\n",
        "            \"org_type\": np.random.choice(org_types, p=[0.1, 0.1, 0.1, 0.2, 0.2, 0.1, 0.1, 0.1]),\n",
        "            \"website_domain\": fake.url(),\n",
        "            \"canonical_org_id\": None # To be resolved by pipeline\n",
        "        })\n",
        "    \n",
        "    df_organisations_raw = pd.DataFrame(orgs)\n",
        "    \n",
        "    # Ensure we have specific brokers for the case study\n",
        "    # Hewlett Foundation\n",
        "    df_organisations_raw.loc[0, \"org_name\"] = \"William and Flora Hewlett Foundation\"\n",
        "    df_organisations_raw.loc[0, \"org_type\"] = \"Foundation\"\n",
        "    df_organisations_raw.loc[0, \"org_ref\"] = \"US-EIN-94-1655673\"\n",
        "    \n",
        "    # J-PAL (Academic)\n",
        "    df_organisations_raw.loc[1, \"org_name\"] = \"Abdul Latif Jameel Poverty Action Lab\"\n",
        "    df_organisations_raw.loc[1, \"org_type\"] = \"Academic/Research\"\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # 2. Activities (df_activities_raw)\n",
        "    # -------------------------------------------------------------------------\n",
        "    activities = []\n",
        "    instruments = [\"Grant\", \"Loan\", \"Equity\", None] # Include nulls\n",
        "    \n",
        "    for i in range(n_activities):\n",
        "        # Multi-valued countries and sectors (semicolon delimited or list)\n",
        "        # We'll use lists as per our pipeline's capability to handle them\n",
        "        n_countries = np.random.randint(1, 4)\n",
        "        countries = [fake.country_code() for _ in range(n_countries)]\n",
        "        \n",
        "        n_sectors = np.random.randint(1, 3)\n",
        "        sectors = [str(np.random.randint(100, 999)) for _ in range(n_sectors)]\n",
        "        \n",
        "        activities.append({\n",
        "            \"iati_identifier\": f\"ACT-{i:05d}\",\n",
        "            \"activity_start_date_iso_date\": fake.date_between(start_date='-10y', end_date='today').isoformat(),\n",
        "            \"recipient_country_code\": countries,\n",
        "            \"sector_code\": sectors,\n",
        "            \"instrument_type\": np.random.choice(instruments, p=[0.6, 0.3, 0.05, 0.05]),\n",
        "            \"activity_title\": fake.sentence(),\n",
        "            \"publisher_org_ref\": np.random.choice(df_organisations_raw[\"org_ref\"])\n",
        "        })\n",
        "        \n",
        "    df_activities_raw = pd.DataFrame(activities)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 3. Transactions (df_transactions_raw)\n",
        "    # -------------------------------------------------------------------------\n",
        "    transactions = []\n",
        "    for i in range(n_transactions):\n",
        "        # Pick random activity\n",
        "        act = np.random.choice(df_activities_raw[\"iati_identifier\"])\n",
        "        \n",
        "        # Pick provider and receiver from orgs\n",
        "        prov = df_organisations_raw.sample(1).iloc[0]\n",
        "        recv = df_organisations_raw.sample(1).iloc[0]\n",
        "        \n",
        "        # Ensure provider != receiver\n",
        "        while prov[\"org_ref\"] == recv[\"org_ref\"]:\n",
        "            recv = df_organisations_raw.sample(1).iloc[0]\n",
        "            \n",
        "        date = fake.date_between(start_date='-10y', end_date='today')\n",
        "        \n",
        "        transactions.append({\n",
        "            \"transaction_id\": f\"TX-{i:08d}\",\n",
        "            \"iati_identifier\": act,\n",
        "            \"reporting_org_ref\": prov[\"org_ref\"], # Usually provider reports\n",
        "            \"reporting_org_name\": prov[\"org_name\"],\n",
        "            \"transaction_transaction_date_iso_date\": date.isoformat(),\n",
        "            \"transaction_transaction_type_code\": np.random.choice([\"C\", \"D\", \"E\"]), # Commitment, Disbursement, Expenditure\n",
        "            \"transaction_value\": round(np.random.uniform(1000, 1000000), 2),\n",
        "            \"transaction_value_currency\": \"USD\",\n",
        "            \"transaction_provider_org_ref\": prov[\"org_ref\"],\n",
        "            \"transaction_provider_org_name\": prov[\"org_name\"],\n",
        "            \"transaction_receiver_org_ref\": recv[\"org_ref\"],\n",
        "            \"transaction_receiver_org_name\": recv[\"org_name\"]\n",
        "        })\n",
        "        \n",
        "    df_transactions_raw = pd.DataFrame(transactions)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 4. Web Links (df_web_links_raw)\n",
        "    # -------------------------------------------------------------------------\n",
        "    web_links = []\n",
        "    # Extract domains from orgs\n",
        "    domains = [d for d in df_organisations_raw[\"website_domain\"] if d]\n",
        "    \n",
        "    for _ in range(n_web_links):\n",
        "        if not domains: break\n",
        "        src = np.random.choice(domains)\n",
        "        tgt = np.random.choice(domains)\n",
        "        if src != tgt:\n",
        "            web_links.append({\n",
        "                \"source_domain\": src,\n",
        "                \"target_domain\": tgt,\n",
        "                \"crawl_date\": datetime.now().isoformat(),\n",
        "                \"link_count\": 1\n",
        "            })\n",
        "            \n",
        "    df_web_links_raw = pd.DataFrame(web_links)\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # 5. Exchange Rates (Optional - Empty for baseline)\n",
        "    # -------------------------------------------------------------------------\n",
        "    df_exchange_rates_raw = pd.DataFrame(columns=[\"currency_code\", \"date\", \"exchange_rate_to_usd\"])\n",
        "\n",
        "    logger.info(\"Synthetic data generation complete.\")\n",
        "    return {\n",
        "        \"df_transactions_raw\": df_transactions_raw,\n",
        "        \"df_activities_raw\": df_activities_raw,\n",
        "        \"df_organisations_raw\": df_organisations_raw,\n",
        "        \"df_web_links_raw\": df_web_links_raw,\n",
        "        \"df_exchange_rates_raw\": df_exchange_rates_raw\n",
        "    }\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 2: Configuration Loading\n",
        "# ==============================================================================\n",
        "\n",
        "def load_study_config(config_path: str = \"config.yaml\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Reads the YAML configuration file into a Python dictionary.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Loading configuration from {config_path}...\")\n",
        "    try:\n",
        "        with open(config_path, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "        logger.info(\"Configuration loaded successfully.\")\n",
        "        return config\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"Config file not found at {config_path}. Please ensure it exists.\")\n",
        "        raise\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 3: Pipeline Execution Example\n",
        "# ==============================================================================\n",
        "\n",
        "def main():\n",
        "    # 1. Generate Data\n",
        "    data_inputs = generate_synthetic_data()\n",
        "    \n",
        "    # 2. Load Config\n",
        "    # Note: Ensure 'config.yaml' exists in the working directory with the content provided previously.\n",
        "    # For this example, we assume it is present.\n",
        "    try:\n",
        "        config = load_study_config(\"config.yaml\")\n",
        "    except Exception as e:\n",
        "        logger.critical(\"Failed to load config. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # 3. Execute Master Workflow\n",
        "    # This invokes the top-level orchestrator which manages the entire lifecycle:\n",
        "    # Baseline -> Robustness -> Provenance\n",
        "    logger.info(\"Invoking Master Orchestrator...\")\n",
        "    \n",
        "    results = execute_master_workflow(\n",
        "        df_transactions_raw=data_inputs[\"df_transactions_raw\"],\n",
        "        df_activities_raw=data_inputs[\"df_activities_raw\"],\n",
        "        df_organisations_raw=data_inputs[\"df_organisations_raw\"],\n",
        "        df_web_links_raw=data_inputs[\"df_web_links_raw\"],\n",
        "        config=config,\n",
        "        output_root=\"./global_aid_study_output\"\n",
        "    )\n",
        "\n",
        "    # 4. Inspect Results\n",
        "    if results.success:\n",
        "        logger.info(f\"Workflow completed in {results.execution_time:.2f} seconds.\")\n",
        "        \n",
        "        # Access Baseline Artifacts\n",
        "        baseline = results.baseline_results\n",
        "        \n",
        "        # Example: Inspect Solar System Rankings\n",
        "        if baseline.artifacts.get(\"solar_system\"):\n",
        "            top_nodes = baseline.artifacts[\"solar_system\"].top_100_table\n",
        "            print(\"\\n--- Top 5 Central Actors (Solar System) ---\")\n",
        "            print(top_nodes[['rank', 'hub_score_hits', 'node_size', 'ring']].head(5))\n",
        "            \n",
        "        # Example: Inspect Broker Comparison\n",
        "        if baseline.artifacts.get(\"broker_comparison\"):\n",
        "            brokers = baseline.artifacts[\"broker_comparison\"].comparison_table\n",
        "            print(\"\\n--- Broker Betweenness Comparison ---\")\n",
        "            print(brokers[['org_name', 'betweenness', 'ratio_to_median']])\n",
        "            \n",
        "        # Example: Inspect Validation Correlation\n",
        "        if baseline.artifacts.get(\"correlation\"):\n",
        "            corr = baseline.artifacts[\"correlation\"]\n",
        "            print(\"\\n--- External Validation ---\")\n",
        "            print(f\"Pearson r: {corr.pearson_r:.4f} (p={corr.p_value:.4e})\")\n",
        "            \n",
        "        # Access Provenance\n",
        "        prov = results.provenance_artifact\n",
        "        print(f\"\\nProvenance Manifest: {len(prov.artifact_manifest)} files persisted.\")\n",
        "        \n",
        "    else:\n",
        "        logger.error(\"Workflow failed.\")\n",
        "        if results.baseline_results and results.baseline_results.error_message:\n",
        "            logger.error(f\"Baseline Error: {results.baseline_results.error_message}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "```\n",
        "<br>"
      ],
      "metadata": {
        "id": "ypvBvLD6xYux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Validate configuration fidelity to manuscript methods\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate configuration fidelity to manuscript methods\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 1: Assert alignment with paper-stated scope\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_scope_and_exclusions(config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates that the study configuration strictly adheres to the manuscript's\n",
        "    stated temporal scope and methodological exclusions (e.g., no LLMs).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The master study configuration dictionary.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If the start/end years do not match 1967-2025, if LLM usage is enabled,\n",
        "        or if forbidden methods (LPPLS, etc.) are referenced.\n",
        "    \"\"\"\n",
        "    # Extract scope parameters\n",
        "    # Check strict adherence to manuscript start year (1967)\n",
        "    start_year = config.get(\"SCOPE\", {}).get(\"start_year\")\n",
        "    # Check strict adherence to manuscript end year (2025)\n",
        "    end_year = config.get(\"SCOPE\", {}).get(\"end_year\")\n",
        "\n",
        "    # Validate temporal scope\n",
        "    if start_year != 1967 or end_year != 2025:\n",
        "        # Raise error if scope deviates from manuscript\n",
        "        raise ValueError(\n",
        "            f\"Configured scope ({start_year}-{end_year}) deviates from manuscript \"\n",
        "            \"requirement (1967-2025).\"\n",
        "        )\n",
        "\n",
        "    # Log confirmation of scope\n",
        "    logger.info(f\"Temporal scope validated: {start_year}-{end_year}\")\n",
        "\n",
        "    # Validate LLM exclusion\n",
        "    # Manuscript does not use LLMs; this must be explicitly disabled\n",
        "    llm_enabled = config.get(\"LLM_USAGE\", {}).get(\"enabled\")\n",
        "    if llm_enabled:\n",
        "        # Raise error if LLMs are enabled\n",
        "        raise ValueError(\"LLM usage is enabled but manuscript methods do not utilize LLMs.\")\n",
        "\n",
        "    # Log confirmation of LLM exclusion\n",
        "    logger.info(\"LLM usage explicitly disabled, consistent with manuscript.\")\n",
        "\n",
        "    # Scan for forbidden keys recursively\n",
        "    # These methods (LPPLS, Lomb-Scargle, etc.) are not in the paper\n",
        "    forbidden_terms = [\"LPPLS\", \"Lomb\", \"Scargle\", \"surrogate\", \"inception\"]\n",
        "\n",
        "    def recursive_scan(d: Dict[str, Any], path: str = \"\"):\n",
        "        \"\"\"Recursively scan dictionary keys for forbidden terms.\"\"\"\n",
        "        for k, v in d.items():\n",
        "            # Check current key against forbidden terms\n",
        "            for term in forbidden_terms:\n",
        "                if term.lower() in k.lower():\n",
        "                    raise ValueError(f\"Configuration contains forbidden non-manuscript term '{term}' at '{path}{k}'\")\n",
        "            # Recurse if value is a dictionary\n",
        "            if isinstance(v, dict):\n",
        "                recursive_scan(v, path + k + \".\")\n",
        "\n",
        "    # Execute recursive scan on the entire config\n",
        "    recursive_scan(config)\n",
        "    # Log confirmation of method purity\n",
        "    logger.info(\"No forbidden non-manuscript methods (LPPLS, etc.) detected in configuration.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Assert required dataset presence\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_dataset_presence(\n",
        "    config: Dict[str, Any],\n",
        "    input_data: Dict[str, pd.DataFrame]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates that all required datasets declared in the configuration are present\n",
        "    in the input data dictionary.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The master study configuration dictionary.\n",
        "    input_data : Dict[str, pd.DataFrame]\n",
        "        Dictionary mapping dataset names to loaded pandas DataFrames.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any dataset marked as 'required' in the config is missing from input_data.\n",
        "    \"\"\"\n",
        "    # Extract input dataset requirements from config\n",
        "    required_datasets_config = config.get(\"INPUT_DATASETS\", {})\n",
        "\n",
        "    # Identify missing datasets\n",
        "    missing_datasets = []\n",
        "\n",
        "    # Iterate over configured datasets\n",
        "    for dataset_name, requirements in required_datasets_config.items():\n",
        "        # Check if the dataset is marked as required\n",
        "        if requirements.get(\"required\", False):\n",
        "            # Check if the dataset exists in the input dictionary\n",
        "            if dataset_name not in input_data:\n",
        "                missing_datasets.append(dataset_name)\n",
        "            # Check if the value is actually a DataFrame (not None)\n",
        "            elif input_data[dataset_name] is None:\n",
        "                missing_datasets.append(dataset_name)\n",
        "\n",
        "    # Raise error if any required datasets are missing\n",
        "    if missing_datasets:\n",
        "        raise ValueError(\n",
        "            f\"The following required datasets are missing from input: {missing_datasets}. \"\n",
        "            \"Ensure df_web_links_raw is included for validation reproduction.\"\n",
        "        )\n",
        "\n",
        "    # Log confirmation of dataset presence\n",
        "    logger.info(\"All required datasets are present.\")\n",
        "\n",
        "    # Check for optional FX data\n",
        "    if \"df_exchange_rates_raw\" not in input_data:\n",
        "        logger.info(\"Optional 'df_exchange_rates_raw' not provided; proceeding with baseline count-based analysis.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Assert column-level schema validity\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_dataframe_schemas(input_data: Dict[str, pd.DataFrame]) -> None:\n",
        "    \"\"\"\n",
        "    Validates that the input DataFrames contain the specific columns required\n",
        "    for the manuscript's analysis pipeline.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : Dict[str, pd.DataFrame]\n",
        "        Dictionary mapping dataset names to loaded pandas DataFrames.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any required columns are missing from the DataFrames.\n",
        "    \"\"\"\n",
        "    # Define required schemas based on manuscript needs\n",
        "    # These columns are strictly required for the pipeline to function\n",
        "    required_schemas = {\n",
        "        \"df_transactions_raw\": {\n",
        "            \"iati_identifier\",\n",
        "            \"reporting_org_ref\",\n",
        "            \"reporting_org_name\",\n",
        "            \"transaction_transaction_date_iso_date\",\n",
        "            \"transaction_transaction_type_code\",\n",
        "            \"transaction_value\",\n",
        "            \"transaction_value_currency\",\n",
        "            \"transaction_provider_org_ref\",\n",
        "            \"transaction_provider_org_name\",\n",
        "            \"transaction_receiver_org_ref\",\n",
        "            \"transaction_receiver_org_name\",\n",
        "            \"transaction_id\"\n",
        "        },\n",
        "        \"df_activities_raw\": {\n",
        "            \"iati_identifier\",\n",
        "            \"recipient_country_code\",\n",
        "            \"sector_code\",\n",
        "            \"instrument_type\",\n",
        "            \"publisher_org_ref\"\n",
        "        },\n",
        "        \"df_organisations_raw\": {\n",
        "            \"org_ref\",\n",
        "            \"org_name\",\n",
        "            \"org_type\",\n",
        "            \"website_domain\"\n",
        "        },\n",
        "        \"df_web_links_raw\": {\n",
        "            \"source_domain\",\n",
        "            \"target_domain\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # List to collect all schema errors\n",
        "    schema_errors = []\n",
        "\n",
        "    # Iterate over each dataset to validate\n",
        "    for dataset_name, required_columns in required_schemas.items():\n",
        "        # Skip validation if optional dataset is missing (handled in Step 2)\n",
        "        if dataset_name not in input_data:\n",
        "            continue\n",
        "\n",
        "        df = input_data[dataset_name]\n",
        "        # Get actual columns from the DataFrame\n",
        "        actual_columns = set(df.columns)\n",
        "        # Calculate missing columns\n",
        "        missing_columns = required_columns - actual_columns\n",
        "\n",
        "        # If columns are missing, record the error\n",
        "        if missing_columns:\n",
        "            schema_errors.append(\n",
        "                f\"{dataset_name} is missing columns: {sorted(list(missing_columns))}\"\n",
        "            )\n",
        "\n",
        "    # Raise error if any schema violations were found\n",
        "    if schema_errors:\n",
        "        raise ValueError(\"Schema validation failed:\\n\" + \"\\n\".join(schema_errors))\n",
        "\n",
        "    # Log confirmation of schema validity\n",
        "    logger.info(\"Column-level schema validation passed for all datasets.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_study_configuration(\n",
        "    config: Dict[str, Any],\n",
        "    input_data: Dict[str, pd.DataFrame]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 1. Validates the study configuration and input data\n",
        "    against the strict requirements of the manuscript's methodology.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The master study configuration dictionary.\n",
        "    input_data : Dict[str, pd.DataFrame]\n",
        "        Dictionary mapping dataset names to loaded pandas DataFrames.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if all validations pass. Raises ValueError otherwise.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any validation step fails (scope, dataset presence, or schema).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 1: Configuration and Data Validation...\")\n",
        "\n",
        "    # Step 1: Validate scope and exclusions\n",
        "    # Ensures alignment with 1967-2025 scope and no LLM usage\n",
        "    validate_scope_and_exclusions(config)\n",
        "\n",
        "    # Step 2: Validate dataset presence\n",
        "    # Ensures all required DataFrames (including web links) are loaded\n",
        "    validate_dataset_presence(config, input_data)\n",
        "\n",
        "    # Step 3: Validate schemas\n",
        "    # Ensures all required columns exist in the DataFrames\n",
        "    validate_dataframe_schemas(input_data)\n",
        "\n",
        "    logger.info(\"Task 1 Completed Successfully: Configuration and Data are valid.\")\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "iKZj4-5VxbG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 — Validate data quality and integrity constraints\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate data quality and integrity constraints\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Validate key uniqueness and join compatibility\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_key_integrity(\n",
        "    df_transactions: pd.DataFrame,\n",
        "    df_activities: pd.DataFrame\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Validates the uniqueness of primary keys in transactions and activities,\n",
        "    and computes the join coverage between them.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions : pd.DataFrame\n",
        "        Raw transactions dataframe containing 'transaction_id' and 'iati_identifier'.\n",
        "    df_activities : pd.DataFrame\n",
        "        Raw activities dataframe containing 'iati_identifier'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, float]\n",
        "        A dictionary containing:\n",
        "        - 'transaction_id_duplicate_rate': Fraction of duplicate transaction IDs.\n",
        "        - 'activity_id_duplicate_rate': Fraction of duplicate activity IDs.\n",
        "        - 'tx_to_activity_coverage': Fraction of transactions with a valid activity ID.\n",
        "        - 'activity_to_tx_coverage': Fraction of activities associated with at least one transaction.\n",
        "    \"\"\"\n",
        "    # Validate transaction_id uniqueness\n",
        "    # Count total rows\n",
        "    n_tx = len(df_transactions)\n",
        "    if n_tx == 0:\n",
        "        tx_dup_rate = 0.0\n",
        "    else:\n",
        "        # Count duplicates in transaction_id\n",
        "        n_tx_dups = df_transactions.duplicated(subset=['transaction_id']).sum()\n",
        "        tx_dup_rate = n_tx_dups / n_tx\n",
        "\n",
        "    # Validate activity_id uniqueness\n",
        "    # Count total rows\n",
        "    n_act = len(df_activities)\n",
        "    if n_act == 0:\n",
        "        act_dup_rate = 0.0\n",
        "    else:\n",
        "        # Count duplicates in iati_identifier\n",
        "        n_act_dups = df_activities.duplicated(subset=['iati_identifier']).sum()\n",
        "        act_dup_rate = n_act_dups / n_act\n",
        "\n",
        "    # Compute Join Coverage (Transactions -> Activities)\n",
        "    # Get set of valid activity IDs\n",
        "    valid_activity_ids = set(df_activities['iati_identifier'].dropna().unique())\n",
        "\n",
        "    # Check which transactions have a valid activity ID\n",
        "    tx_has_activity = df_transactions['iati_identifier'].isin(valid_activity_ids)\n",
        "    tx_coverage = tx_has_activity.mean() if n_tx > 0 else 0.0\n",
        "\n",
        "    # Compute Join Coverage (Activities -> Transactions)\n",
        "    # Get set of activity IDs present in transactions\n",
        "    active_activity_ids = set(df_transactions['iati_identifier'].dropna().unique())\n",
        "\n",
        "    # Check which activities are referenced\n",
        "    act_is_referenced = df_activities['iati_identifier'].isin(active_activity_ids)\n",
        "    act_coverage = act_is_referenced.mean() if n_act > 0 else 0.0\n",
        "\n",
        "    logger.info(f\"Key Integrity: Tx Dups={tx_dup_rate:.2%}, Act Dups={act_dup_rate:.2%}, \"\n",
        "                f\"Tx Coverage={tx_coverage:.2%}\")\n",
        "\n",
        "    return {\n",
        "        \"transaction_id_duplicate_rate\": float(tx_dup_rate),\n",
        "        \"activity_id_duplicate_rate\": float(act_dup_rate),\n",
        "        \"tx_to_activity_coverage\": float(tx_coverage),\n",
        "        \"activity_to_tx_coverage\": float(act_coverage)\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Validate type coercion feasibility\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_type_coercion(\n",
        "    df_transactions: pd.DataFrame,\n",
        "    df_organisations: pd.DataFrame\n",
        ") -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Simulates type coercion for critical columns to identify potential data quality issues\n",
        "    without modifying the data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions : pd.DataFrame\n",
        "        Raw transactions dataframe.\n",
        "    df_organisations : pd.DataFrame\n",
        "        Raw organisations dataframe.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, int]\n",
        "        A dictionary containing counts of rows that would fail coercion:\n",
        "        - 'bad_numeric_values': Count of non-numeric transaction values.\n",
        "        - 'bad_dates': Count of unparseable transaction dates.\n",
        "        - 'bad_org_types': Count of organisation types outside the controlled vocabulary.\n",
        "    \"\"\"\n",
        "    # 1. Numeric Coercion Audit (transaction_value)\n",
        "    # Attempt to convert to numeric, turning errors into NaNs\n",
        "    numeric_series = pd.to_numeric(df_transactions['transaction_value'], errors='coerce')\n",
        "    # Count how many became NaN (excluding those that were already NaN)\n",
        "    # We assume original NaNs are \"missing\" not \"malformed\", but for strict typing,\n",
        "    # if the column is object, a NaN is a valid float.\n",
        "    # Here we specifically look for values that exist but fail conversion.\n",
        "    non_null_mask = df_transactions['transaction_value'].notna()\n",
        "    failed_numeric = non_null_mask & numeric_series.isna()\n",
        "    n_bad_numeric = failed_numeric.sum()\n",
        "\n",
        "    # 2. Date Parsing Audit (transaction_transaction_date_iso_date)\n",
        "    # Attempt to convert to datetime, turning errors into NaT\n",
        "    date_series = pd.to_datetime(df_transactions['transaction_transaction_date_iso_date'], errors='coerce')\n",
        "    non_null_date_mask = df_transactions['transaction_transaction_date_iso_date'].notna()\n",
        "    failed_dates = non_null_date_mask & date_series.isna()\n",
        "    n_bad_dates = failed_dates.sum()\n",
        "\n",
        "    # 3. Org Type Taxonomy Enforcement\n",
        "    # Define controlled vocabulary per manuscript/task requirements\n",
        "    allowed_org_types = {\n",
        "        \"Government\", \"Multilateral\", \"Foundation\", \"Academic/Research\",\n",
        "        \"International NGO\", \"National NGO\", \"Private Sector\", \"Other\"\n",
        "    }\n",
        "    # Check validity\n",
        "    # Normalize to string to handle potential mixed types, though input should be string\n",
        "    org_types = df_organisations['org_type'].astype(str)\n",
        "\n",
        "    # Identify types not in the allowed set\n",
        "    # Note: We treat NaN/None as \"bad\" here if strict taxonomy is required,\n",
        "    # or we can exclude them. The task says \"Quarantine ambiguous/missing\".\n",
        "    # So we check everything.\n",
        "    is_valid_type = org_types.isin(allowed_org_types)\n",
        "    n_bad_org_types = (~is_valid_type).sum()\n",
        "\n",
        "    logger.info(f\"Type Coercion: Bad Numeric={n_bad_numeric}, Bad Dates={n_bad_dates}, \"\n",
        "                f\"Bad Org Types={n_bad_org_types}\")\n",
        "\n",
        "    return {\n",
        "        \"bad_numeric_values\": int(n_bad_numeric),\n",
        "        \"bad_dates\": int(n_bad_dates),\n",
        "        \"bad_org_types\": int(n_bad_org_types)\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Validate organisation reference coverage\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_reference_coverage(\n",
        "    df_transactions: pd.DataFrame,\n",
        "    df_organisations: pd.DataFrame\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Computes the fraction of transaction provider and receiver references that\n",
        "    exist in the master organisation table.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions : pd.DataFrame\n",
        "        Raw transactions dataframe.\n",
        "    df_organisations : pd.DataFrame\n",
        "        Raw organisations dataframe.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, float]\n",
        "        A dictionary containing:\n",
        "        - 'provider_ref_match_rate': Fraction of non-null provider refs found in master.\n",
        "        - 'receiver_ref_match_rate': Fraction of non-null receiver refs found in master.\n",
        "    \"\"\"\n",
        "    # Extract master set of organisation references\n",
        "    # Drop nulls to ensure clean set\n",
        "    master_refs = set(df_organisations['org_ref'].dropna().unique())\n",
        "\n",
        "    # 1. Provider Reference Coverage\n",
        "    # Get non-null provider refs\n",
        "    provider_refs = df_transactions['transaction_provider_org_ref'].dropna()\n",
        "    if len(provider_refs) == 0:\n",
        "        prov_coverage = 0.0\n",
        "    else:\n",
        "        # Check membership\n",
        "        prov_matches = provider_refs.isin(master_refs)\n",
        "        prov_coverage = prov_matches.mean()\n",
        "\n",
        "    # 2. Receiver Reference Coverage\n",
        "    # Get non-null receiver refs\n",
        "    receiver_refs = df_transactions['transaction_receiver_org_ref'].dropna()\n",
        "    if len(receiver_refs) == 0:\n",
        "        recv_coverage = 0.0\n",
        "    else:\n",
        "        # Check membership\n",
        "        recv_matches = receiver_refs.isin(master_refs)\n",
        "        recv_coverage = recv_matches.mean()\n",
        "\n",
        "    logger.info(f\"Ref Coverage: Provider={prov_coverage:.2%}, Receiver={recv_coverage:.2%}\")\n",
        "\n",
        "    return {\n",
        "        \"provider_ref_match_rate\": float(prov_coverage),\n",
        "        \"receiver_ref_match_rate\": float(recv_coverage)\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_data_quality(\n",
        "    df_transactions: pd.DataFrame,\n",
        "    df_activities: pd.DataFrame,\n",
        "    df_organisations: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 2. Executes a suite of data quality checks to ensure\n",
        "    integrity, type safety, and referential consistency before processing.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions : pd.DataFrame\n",
        "        Raw transactions dataframe.\n",
        "    df_activities : pd.DataFrame\n",
        "        Raw activities dataframe.\n",
        "    df_organisations : pd.DataFrame\n",
        "        Raw organisations dataframe.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A consolidated dictionary of all quality metrics computed.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 2: Data Quality Validation...\")\n",
        "\n",
        "    # Step 1: Key Integrity\n",
        "    integrity_metrics = validate_key_integrity(df_transactions, df_activities)\n",
        "\n",
        "    # Step 2: Type Coercion\n",
        "    coercion_metrics = validate_type_coercion(df_transactions, df_organisations)\n",
        "\n",
        "    # Step 3: Reference Coverage\n",
        "    coverage_metrics = validate_reference_coverage(df_transactions, df_organisations)\n",
        "\n",
        "    # Consolidate results\n",
        "    quality_report = {\n",
        "        **integrity_metrics,\n",
        "        **coercion_metrics,\n",
        "        **coverage_metrics\n",
        "    }\n",
        "\n",
        "    logger.info(\"Task 2 Completed Successfully.\")\n",
        "    return quality_report\n"
      ],
      "metadata": {
        "id": "HxUCa2ax9FQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 — Freeze input snapshot and establish reproducibility metadata\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Freeze input snapshot and establish reproducibility metadata\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Record dataset fingerprints\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class DatasetFingerprint:\n",
        "    \"\"\"\n",
        "    Encapsulates the statistical fingerprint of a dataset for reproducibility.\n",
        "\n",
        "    This dataclass serves as an immutable record of the input data state at the\n",
        "    start of the pipeline. It captures key metrics (row counts, null rates,\n",
        "    temporal bounds, and content hashes) that allow for the verification of\n",
        "    data integrity and the detection of upstream data drift.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    row_count : int\n",
        "        The total number of rows in the DataFrame. Used to verify data volume.\n",
        "    column_null_rates : Dict[str, float]\n",
        "        A dictionary mapping column names to their fraction of null values.\n",
        "        Used to monitor data quality and completeness.\n",
        "    temporal_range : Optional[Tuple[pd.Timestamp, pd.Timestamp]], default=None\n",
        "        A tuple containing the minimum and maximum timestamps found in the\n",
        "        dataset (if applicable, e.g., for transactions). Used to verify\n",
        "        temporal coverage.\n",
        "    dataframe_hash : str, default=\"\"\n",
        "        A deterministic hash string derived from the DataFrame's content or\n",
        "        structure. Used for strict equality checks and provenance tracking.\n",
        "    \"\"\"\n",
        "    # The total number of rows in the dataset\n",
        "    row_count: int\n",
        "\n",
        "    # A dictionary mapping column names to the proportion of missing values (0.0 to 1.0)\n",
        "    column_null_rates: Dict[str, float]\n",
        "\n",
        "    # The minimum and maximum dates in the dataset, if a date column exists\n",
        "    temporal_range: Optional[Tuple[pd.Timestamp, pd.Timestamp]] = None\n",
        "\n",
        "    # A SHA-256 hash representing the dataset's content/structure\n",
        "    dataframe_hash: str = \"\"\n",
        "\n",
        "def compute_dataset_fingerprints(\n",
        "    input_data: Dict[str, pd.DataFrame]\n",
        ") -> Dict[str, DatasetFingerprint]:\n",
        "    \"\"\"\n",
        "    Computes statistical fingerprints (row counts, null rates, temporal ranges)\n",
        "    for all input DataFrames to establish a baseline snapshot.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : Dict[str, pd.DataFrame]\n",
        "        Dictionary of raw input DataFrames.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, DatasetFingerprint]\n",
        "        A dictionary mapping dataset names to their fingerprints.\n",
        "    \"\"\"\n",
        "    fingerprints = {}\n",
        "\n",
        "    for name, df in input_data.items():\n",
        "        # 1. Row Count\n",
        "        row_count = len(df)\n",
        "\n",
        "        # 2. Null Rates per Column\n",
        "        # Compute fraction of nulls for each column\n",
        "        null_rates = df.isnull().mean().to_dict()\n",
        "\n",
        "        # 3. Temporal Range (for transactions only)\n",
        "        temporal_range = None\n",
        "        if name == \"df_transactions_raw\" and \"transaction_transaction_date_iso_date\" in df.columns:\n",
        "            # Coerce dates to find min/max, ignoring errors for fingerprinting purposes\n",
        "            # We do not mutate the original DF here\n",
        "            dates = pd.to_datetime(df[\"transaction_transaction_date_iso_date\"], errors='coerce')\n",
        "            valid_dates = dates.dropna()\n",
        "            if not valid_dates.empty:\n",
        "                temporal_range = (valid_dates.min(), valid_dates.max())\n",
        "\n",
        "        # 4. Deterministic Hash (Simple structural hash for in-memory verification)\n",
        "        # We hash the column names and row count as a lightweight proxy\n",
        "        # A full content hash is expensive for 10M rows; this is a \"sanity check\" hash\n",
        "        header_str = \",\".join(sorted(df.columns))\n",
        "        content_summary = f\"{header_str}|{row_count}\"\n",
        "        df_hash = hashlib.sha256(content_summary.encode('utf-8')).hexdigest()\n",
        "\n",
        "        fingerprints[name] = DatasetFingerprint(\n",
        "            row_count=row_count,\n",
        "            column_null_rates=null_rates,\n",
        "            temporal_range=temporal_range,\n",
        "            dataframe_hash=df_hash\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Fingerprinted {name}: {row_count} rows, Hash={df_hash[:8]}...\")\n",
        "\n",
        "    return fingerprints\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Fix random seeds for stochastic components\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def fix_random_seeds(config: Dict[str, Any]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Ensures that random seeds for stochastic components (node2vec, UMAP) are\n",
        "    explicitly set in the configuration. If missing, assigns default fixed values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The master study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, int]\n",
        "        A dictionary of the resolved seeds.\n",
        "    \"\"\"\n",
        "    resolved_seeds = {}\n",
        "\n",
        "    # 1. node2vec Seed\n",
        "    # Check if seed exists in config, else set default\n",
        "    n2v_config = config.get(\"NODE2VEC\", {})\n",
        "    if \"random_seed\" not in n2v_config or n2v_config[\"random_seed\"] == \"MUST_BE_FIXED_AND_RECORDED\":\n",
        "        # Default fixed seed for reproducibility\n",
        "        resolved_seeds[\"node2vec_seed\"] = 42\n",
        "        logger.info(\"Assigned default node2vec_seed: 42\")\n",
        "    else:\n",
        "        resolved_seeds[\"node2vec_seed\"] = int(n2v_config[\"random_seed\"])\n",
        "        logger.info(f\"Using configured node2vec_seed: {resolved_seeds['node2vec_seed']}\")\n",
        "\n",
        "    # 2. UMAP Seed\n",
        "    # Check if seed exists in config, else set default\n",
        "    umap_config = config.get(\"UMAP\", {})\n",
        "    if \"random_state\" not in umap_config or umap_config[\"random_state\"] == \"MUST_BE_FIXED_AND_RECORDED\":\n",
        "        # Default fixed seed for reproducibility\n",
        "        resolved_seeds[\"umap_seed\"] = 42\n",
        "        logger.info(\"Assigned default umap_seed: 42\")\n",
        "    else:\n",
        "        resolved_seeds[\"umap_seed\"] = int(umap_config[\"random_state\"])\n",
        "        logger.info(f\"Using configured umap_seed: {resolved_seeds['umap_seed']}\")\n",
        "\n",
        "    return resolved_seeds\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Define deterministic ordering rules\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class SortPolicy:\n",
        "    \"\"\"\n",
        "    Defines the columns and order used for deterministic sorting of datasets.\n",
        "\n",
        "    This dataclass specifies the exact sorting rules required to ensure\n",
        "    deterministic behavior in downstream operations such as deduplication,\n",
        "    hashing, and rank assignment. By explicitly defining the sort keys and\n",
        "    order, we eliminate ambiguity caused by non-deterministic row ordering\n",
        "    in distributed or parallel processing environments.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    dataset_name : str\n",
        "        The name of the dataset to which this policy applies (e.g., \"df_transactions_raw\").\n",
        "    sort_columns : List[str]\n",
        "        A list of column names to use as sort keys. The order of columns in\n",
        "        this list determines the priority of sorting (primary key, secondary key, etc.).\n",
        "    ascending : List[bool]\n",
        "        A list of booleans corresponding to `sort_columns`, indicating whether\n",
        "        each column should be sorted in ascending (True) or descending (False) order.\n",
        "    \"\"\"\n",
        "    # The identifier of the dataset (e.g., 'df_transactions_raw')\n",
        "    dataset_name: str\n",
        "\n",
        "    # The list of column names to sort by, in order of priority\n",
        "    sort_columns: List[str]\n",
        "\n",
        "    # A list of booleans indicating sort direction for each column (True=Ascending)\n",
        "    ascending: List[bool]\n",
        "\n",
        "def define_sort_policies() -> Dict[str, SortPolicy]:\n",
        "    \"\"\"\n",
        "    Defines the immutable sorting rules for each dataset to ensure deterministic\n",
        "    processing order (e.g., for deduplication and hashing).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, SortPolicy]\n",
        "        A dictionary mapping dataset names to their sort policies.\n",
        "    \"\"\"\n",
        "    policies = {}\n",
        "\n",
        "    # 1. Transactions Policy\n",
        "    # Sort by ID, Date, Value, Provider, Receiver\n",
        "    policies[\"df_transactions_raw\"] = SortPolicy(\n",
        "        dataset_name=\"df_transactions_raw\",\n",
        "        sort_columns=[\n",
        "            \"iati_identifier\",\n",
        "            \"transaction_transaction_date_iso_date\",\n",
        "            \"transaction_value\",\n",
        "            \"transaction_provider_org_ref\",\n",
        "            \"transaction_receiver_org_ref\"\n",
        "        ],\n",
        "        # All ascending\n",
        "        ascending=[True, True, True, True, True]\n",
        "    )\n",
        "\n",
        "    # 2. Activities Policy\n",
        "    # Sort by IATI Identifier\n",
        "    policies[\"df_activities_raw\"] = SortPolicy(\n",
        "        dataset_name=\"df_activities_raw\",\n",
        "        sort_columns=[\"iati_identifier\"],\n",
        "        ascending=[True]\n",
        "    )\n",
        "\n",
        "    # 3. Organisations Policy\n",
        "    # Sort by Org Ref\n",
        "    policies[\"df_organisations_raw\"] = SortPolicy(\n",
        "        dataset_name=\"df_organisations_raw\",\n",
        "        sort_columns=[\"org_ref\"],\n",
        "        ascending=[True]\n",
        "    )\n",
        "\n",
        "    logger.info(\"Defined deterministic sort policies for Transactions, Activities, Organisations.\")\n",
        "    return policies\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class ReproducibilityContext:\n",
        "    \"\"\"\n",
        "    Container for all metadata required to reproduce the run.\n",
        "\n",
        "    This dataclass aggregates all the artifacts generated during the\n",
        "    \"Freeze input snapshot\" task. It serves as a single source of truth for\n",
        "    the run's initial conditions, including data fingerprints, random seeds,\n",
        "    and sorting policies. This context object is passed downstream to ensure\n",
        "    that all subsequent processing steps adhere to the established\n",
        "    reproducibility parameters.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    fingerprints : Dict[str, DatasetFingerprint]\n",
        "        A dictionary mapping dataset names to their computed `DatasetFingerprint` objects.\n",
        "    seeds : Dict[str, int]\n",
        "        A dictionary mapping stochastic component names (e.g., \"node2vec_seed\")\n",
        "        to their fixed integer seeds.\n",
        "    sort_policies : Dict[str, SortPolicy]\n",
        "        A dictionary mapping dataset names to their defined `SortPolicy` objects.\n",
        "    \"\"\"\n",
        "    # A dictionary of dataset fingerprints keyed by dataset name\n",
        "    fingerprints: Dict[str, DatasetFingerprint]\n",
        "\n",
        "    # A dictionary of fixed random seeds keyed by component name\n",
        "    seeds: Dict[str, int]\n",
        "\n",
        "    # A dictionary of sort policies keyed by dataset name\n",
        "    sort_policies: Dict[str, SortPolicy]\n",
        "\n",
        "def establish_reproducibility_context(\n",
        "    config: Dict[str, Any],\n",
        "    input_data: Dict[str, pd.DataFrame]\n",
        ") -> ReproducibilityContext:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 3. Freezes the input state by computing fingerprints,\n",
        "    resolving random seeds, and defining sorting policies.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The master study configuration dictionary.\n",
        "    input_data : Dict[str, pd.DataFrame]\n",
        "        Dictionary of raw input DataFrames.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ReproducibilityContext\n",
        "        The established context object containing fingerprints, seeds, and policies.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 3: Establishing Reproducibility Context...\")\n",
        "\n",
        "    # Step 1: Fingerprint Datasets\n",
        "    fingerprints = compute_dataset_fingerprints(input_data)\n",
        "\n",
        "    # Step 2: Fix Seeds\n",
        "    seeds = fix_random_seeds(config)\n",
        "\n",
        "    # Step 3: Define Sort Policies\n",
        "    sort_policies = define_sort_policies()\n",
        "\n",
        "    context = ReproducibilityContext(\n",
        "        fingerprints=fingerprints,\n",
        "        seeds=seeds,\n",
        "        sort_policies=sort_policies\n",
        "    )\n",
        "\n",
        "    logger.info(\"Task 3 Completed Successfully.\")\n",
        "    return context\n"
      ],
      "metadata": {
        "id": "uHznxtxY-d8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 — Cleanse df_transactions_raw (temporal and value filters)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Cleanse df_transactions_raw (temporal and value filters)\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class TransactionExclusions:\n",
        "    \"\"\"\n",
        "    Container for tracking rows excluded during transaction cleansing.\n",
        "\n",
        "    This dataclass serves as a structured ledger for all transactions dropped\n",
        "    during the temporal and value-based cleansing phase. By categorizing exclusions\n",
        "    into specific lists of transaction IDs, it enables precise auditing of data loss\n",
        "    and facilitates the debugging of upstream data quality issues.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    bad_date_ids : List[str]\n",
        "        A list of transaction IDs excluded because their date strings could not\n",
        "        be parsed into valid ISO-8601 datetime objects.\n",
        "    non_numeric_value_ids : List[str]\n",
        "        A list of transaction IDs excluded because their value fields could not\n",
        "        be coerced into valid floating-point numbers.\n",
        "    zero_or_negative_value_ids : List[str]\n",
        "        A list of transaction IDs excluded because their financial value was\n",
        "        less than or equal to zero (violating the V(t) > 0 constraint).\n",
        "    out_of_scope_ids : List[str]\n",
        "        A list of transaction IDs excluded because their transaction year fell\n",
        "        outside the study's defined temporal scope (1967-2025).\n",
        "    \"\"\"\n",
        "    # List of IDs with unparseable dates\n",
        "    bad_date_ids: List[str] = field(default_factory=list)\n",
        "\n",
        "    # List of IDs with non-numeric value fields\n",
        "    non_numeric_value_ids: List[str] = field(default_factory=list)\n",
        "\n",
        "    # List of IDs with zero or negative values\n",
        "    zero_or_negative_value_ids: List[str] = field(default_factory=list)\n",
        "\n",
        "    # List of IDs outside the temporal scope\n",
        "    out_of_scope_ids: List[str] = field(default_factory=list)\n",
        "\n",
        "    def total_excluded(self) -> int:\n",
        "        \"\"\"\n",
        "        Computes the total number of transactions excluded across all categories.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The sum of the lengths of all exclusion lists.\n",
        "        \"\"\"\n",
        "        # Sum the lengths of all exclusion lists to get the total count\n",
        "        return (len(self.bad_date_ids) +\n",
        "                len(self.non_numeric_value_ids) +\n",
        "                len(self.zero_or_negative_value_ids) +\n",
        "                len(self.out_of_scope_ids))\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Parse and validate transaction dates\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def parse_transaction_dates(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, List[str]]:\n",
        "    \"\"\"\n",
        "    Parses transaction dates to datetime objects and extracts the year.\n",
        "    Quarantines rows with unparseable dates.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The raw transactions DataFrame. Must contain 'transaction_transaction_date_iso_date'\n",
        "        and 'transaction_id'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, List[str]]\n",
        "        1. The DataFrame with a new 'transaction_year' column and valid dates.\n",
        "        2. A list of transaction_ids excluded due to invalid dates.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid mutating the input\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # Attempt strict ISO-8601 parsing\n",
        "    # errors='coerce' turns unparseable strings into NaT\n",
        "    df_out['parsed_date'] = pd.to_datetime(\n",
        "        df_out['transaction_transaction_date_iso_date'],\n",
        "        errors='coerce'\n",
        "    )\n",
        "\n",
        "    # Identify failures\n",
        "    # Rows where date is NaT are invalid\n",
        "    mask_invalid = df_out['parsed_date'].isna()\n",
        "    bad_date_ids = df_out.loc[mask_invalid, 'transaction_id'].astype(str).tolist()\n",
        "\n",
        "    # Filter to keep only valid rows\n",
        "    df_valid = df_out.loc[~mask_invalid].copy()\n",
        "\n",
        "    # Extract year as integer\n",
        "    # We use .dt.year which returns float (due to NaNs potentially), so we cast to int\n",
        "    df_valid['transaction_year'] = df_valid['parsed_date'].dt.year.astype(int)\n",
        "\n",
        "    logger.info(f\"Date Parsing: {len(bad_date_ids)} rows excluded due to invalid dates.\")\n",
        "\n",
        "    return df_valid, bad_date_ids\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Apply paper-stated value filter\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def apply_value_filter(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Applies the manuscript's value filter: V(t) > 0.\n",
        "    Handles non-numeric values and zero/negative values separately.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The transactions DataFrame with valid dates. Must contain 'transaction_value'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, List[str], List[str]]\n",
        "        1. The filtered DataFrame containing only positive financial transactions.\n",
        "        2. A list of transaction_ids excluded due to non-numeric values.\n",
        "        3. A list of transaction_ids excluded due to zero or negative values.\n",
        "    \"\"\"\n",
        "    # Coerce transaction_value to numeric\n",
        "    # errors='coerce' turns non-numeric strings into NaN\n",
        "    numeric_values = pd.to_numeric(df['transaction_value'], errors='coerce')\n",
        "\n",
        "    # Identify non-numeric failures\n",
        "    mask_non_numeric = numeric_values.isna()\n",
        "    non_numeric_ids = df.loc[mask_non_numeric, 'transaction_id'].astype(str).tolist()\n",
        "\n",
        "    # Create a working dataframe with the coerced numeric column\n",
        "    # We use .loc to ensure we are working with a clean subset\n",
        "    df_numeric = df.loc[~mask_non_numeric].copy()\n",
        "    df_numeric['transaction_value_numeric'] = numeric_values[~mask_non_numeric]\n",
        "\n",
        "    # Apply strict positive filter: V > 0\n",
        "    # Manuscript: \"filtered for nonzero financial transactions\" -> interpreted as strictly positive\n",
        "    # to exclude reversals/adjustments in baseline.\n",
        "    mask_positive = df_numeric['transaction_value_numeric'] > 0\n",
        "\n",
        "    # Identify zero/negative exclusions\n",
        "    zero_neg_ids = df_numeric.loc[~mask_positive, 'transaction_id'].astype(str).tolist()\n",
        "\n",
        "    # Filter to keep only positive rows\n",
        "    df_final = df_numeric.loc[mask_positive].copy()\n",
        "\n",
        "    # Replace the original string column with the numeric one for downstream use\n",
        "    # (or keep both, but usually we want the numeric one for analysis)\n",
        "    df_final['transaction_value'] = df_final['transaction_value_numeric']\n",
        "    df_final.drop(columns=['transaction_value_numeric'], inplace=True)\n",
        "\n",
        "    logger.info(f\"Value Filter: {len(non_numeric_ids)} non-numeric, {len(zero_neg_ids)} zero/negative excluded.\")\n",
        "\n",
        "    return df_final, non_numeric_ids, zero_neg_ids\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Apply temporal scope filter\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def apply_scope_filter(\n",
        "    df: pd.DataFrame,\n",
        "    start_year: int,\n",
        "    end_year: int\n",
        ") -> Tuple[pd.DataFrame, List[str]]:\n",
        "    \"\"\"\n",
        "    Applies the temporal scope filter: start_year <= year <= end_year.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The transactions DataFrame with 'transaction_year'.\n",
        "    start_year : int\n",
        "        The inclusive start year (e.g., 1967).\n",
        "    end_year : int\n",
        "        The inclusive end year (e.g., 2025).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, List[str]]\n",
        "        1. The filtered DataFrame within the temporal scope.\n",
        "        2. A list of transaction_ids excluded due to being out of scope.\n",
        "    \"\"\"\n",
        "    # Apply range filter\n",
        "    mask_in_scope = (df['transaction_year'] >= start_year) & (df['transaction_year'] <= end_year)\n",
        "\n",
        "    # Identify exclusions\n",
        "    out_of_scope_ids = df.loc[~mask_in_scope, 'transaction_id'].astype(str).tolist()\n",
        "\n",
        "    # Filter DataFrame\n",
        "    df_final = df.loc[mask_in_scope].copy()\n",
        "\n",
        "    logger.info(f\"Scope Filter: {len(out_of_scope_ids)} rows excluded (outside {start_year}-{end_year}).\")\n",
        "\n",
        "    return df_final, out_of_scope_ids\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_transactions_temporal_value(\n",
        "    df_transactions_raw: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, TransactionExclusions]:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 4. Executes the temporal and value-based cleansing pipeline\n",
        "    for transactions.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions_raw : pd.DataFrame\n",
        "        The raw transactions DataFrame.\n",
        "    config : Dict[str, Any]\n",
        "        The master study configuration dictionary containing scope parameters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, TransactionExclusions]\n",
        "        1. The cleansed DataFrame (T++), filtered for valid dates, positive values, and scope.\n",
        "        2. An exclusions ledger object tracking all dropped rows.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 4: Temporal and Value Cleansing...\")\n",
        "\n",
        "    # Initialize exclusions tracker\n",
        "    exclusions = TransactionExclusions()\n",
        "\n",
        "    # Step 1: Parse Dates\n",
        "    df_dated, bad_date_ids = parse_transaction_dates(df_transactions_raw)\n",
        "    exclusions.bad_date_ids = bad_date_ids\n",
        "\n",
        "    # Step 2: Apply Value Filter\n",
        "    df_valued, non_numeric_ids, zero_neg_ids = apply_value_filter(df_dated)\n",
        "    exclusions.non_numeric_value_ids = non_numeric_ids\n",
        "    exclusions.zero_or_negative_value_ids = zero_neg_ids\n",
        "\n",
        "    # Step 3: Apply Scope Filter\n",
        "    start_year = config[\"SCOPE\"][\"start_year\"]\n",
        "    end_year = config[\"SCOPE\"][\"end_year\"]\n",
        "    df_final, out_of_scope_ids = apply_scope_filter(df_valued, start_year, end_year)\n",
        "    exclusions.out_of_scope_ids = out_of_scope_ids\n",
        "\n",
        "    logger.info(f\"Task 4 Completed. Total rows retained: {len(df_final)}. \"\n",
        "                f\"Total excluded: {exclusions.total_excluded()}.\")\n",
        "\n",
        "    return df_final, exclusions\n"
      ],
      "metadata": {
        "id": "EMU560N-C9P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5 — Cleanse df_transactions_raw (endpoint validity and deduplication)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Cleanse df_transactions_raw (endpoint validity and deduplication)\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class EndpointExclusions:\n",
        "    \"\"\"\n",
        "    Container for tracking rows excluded during endpoint validation and deduplication.\n",
        "\n",
        "    This dataclass serves as a structured ledger for all transactions dropped\n",
        "    during the endpoint validation and deduplication phase. It categorizes exclusions\n",
        "    based on the specific data quality failure (missing provider, missing receiver,\n",
        "    or duplicate record), enabling precise auditing of the graph construction process.\n",
        "    Ensuring that every edge in the network has valid source and target nodes is\n",
        "    critical for the topological integrity of the subsequent bipartite projection.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    missing_provider_ids : List[str]\n",
        "        A list of transaction IDs excluded because they lacked a valid provider\n",
        "        identifier (both `transaction_provider_org_ref` and `transaction_provider_org_name`\n",
        "        were null or empty).\n",
        "    missing_receiver_ids : List[str]\n",
        "        A list of transaction IDs excluded because they lacked a valid receiver\n",
        "        identifier (both `transaction_receiver_org_ref` and `transaction_receiver_org_name`\n",
        "        were null or empty), despite having a valid provider.\n",
        "    duplicate_transaction_ids : List[str]\n",
        "        A list of transaction IDs excluded because they were identified as duplicates\n",
        "        of an existing record based on the `transaction_id` field, following a\n",
        "        deterministic sort.\n",
        "    \"\"\"\n",
        "    # List of IDs excluded due to missing provider information\n",
        "    missing_provider_ids: List[str] = field(default_factory=list)\n",
        "\n",
        "    # List of IDs excluded due to missing receiver information\n",
        "    missing_receiver_ids: List[str] = field(default_factory=list)\n",
        "\n",
        "    # List of IDs excluded due to duplication\n",
        "    duplicate_transaction_ids: List[str] = field(default_factory=list)\n",
        "\n",
        "    def total_excluded(self) -> int:\n",
        "        \"\"\"\n",
        "        Computes the total number of transactions excluded across all categories.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The sum of the lengths of all exclusion lists.\n",
        "        \"\"\"\n",
        "        # Sum the lengths of all exclusion lists to get the total count\n",
        "        return (len(self.missing_provider_ids) +\n",
        "                len(self.missing_receiver_ids) +\n",
        "                len(self.duplicate_transaction_ids))\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Enforce endpoint presence policy\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def enforce_endpoint_validity(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Enforces that every transaction has at least one valid provider identifier\n",
        "    (ref or name) and at least one valid receiver identifier (ref or name).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The filtered transactions DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, List[str], List[str]]\n",
        "        1. The DataFrame with valid endpoints.\n",
        "        2. List of transaction_ids excluded due to missing provider info.\n",
        "        3. List of transaction_ids excluded due to missing receiver info.\n",
        "    \"\"\"\n",
        "    # Work on a copy to avoid side effects\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # Normalize endpoint columns: trim whitespace, convert empty strings to NaN\n",
        "    endpoint_cols = [\n",
        "        'transaction_provider_org_ref', 'transaction_provider_org_name',\n",
        "        'transaction_receiver_org_ref', 'transaction_receiver_org_name'\n",
        "    ]\n",
        "\n",
        "    for col in endpoint_cols:\n",
        "        if col in df_out.columns:\n",
        "            # Ensure string type, strip whitespace\n",
        "            df_out[col] = df_out[col].astype(str).str.strip()\n",
        "            # Replace empty strings and 'nan' strings with actual NaN\n",
        "            df_out[col] = df_out[col].replace({'': np.nan, 'nan': np.nan, 'None': np.nan})\n",
        "\n",
        "    # Define validity conditions\n",
        "    # Provider valid if Ref is present OR Name is present\n",
        "    has_provider = df_out['transaction_provider_org_ref'].notna() | df_out['transaction_provider_org_name'].notna()\n",
        "\n",
        "    # Receiver valid if Ref is present OR Name is present\n",
        "    has_receiver = df_out['transaction_receiver_org_ref'].notna() | df_out['transaction_receiver_org_name'].notna()\n",
        "\n",
        "    # Identify exclusions\n",
        "    missing_provider_ids = df_out.loc[~has_provider, 'transaction_id'].astype(str).tolist()\n",
        "\n",
        "    # For receiver exclusions, we only count those that HAVE a provider but lack a receiver\n",
        "    # (rows missing both are already caught by missing_provider check logic if we filter sequentially,\n",
        "    # but here we calculate masks independently. To avoid double counting in lists, we can be specific).\n",
        "    # However, standard practice is to report the primary reason.\n",
        "    # Let's define:\n",
        "    # - Missing Provider: dropped immediately.\n",
        "    # - Missing Receiver: dropped if Provider exists but Receiver doesn't.\n",
        "    # Rows to keep must have BOTH\n",
        "    keep_mask = has_provider & has_receiver\n",
        "\n",
        "    # Calculate specific exclusion lists based on the keep_mask logic\n",
        "    # Rows failing provider check\n",
        "    missing_provider_ids = df_out.loc[~has_provider, 'transaction_id'].astype(str).tolist()\n",
        "\n",
        "    # Rows passing provider check but failing receiver check\n",
        "    missing_receiver_ids = df_out.loc[has_provider & ~has_receiver, 'transaction_id'].astype(str).tolist()\n",
        "\n",
        "    # Filter DataFrame\n",
        "    df_valid = df_out.loc[keep_mask].copy()\n",
        "\n",
        "    logger.info(f\"Endpoint Validity: {len(missing_provider_ids)} missing provider, \"\n",
        "                f\"{len(missing_receiver_ids)} missing receiver.\")\n",
        "\n",
        "    return df_valid, missing_provider_ids, missing_receiver_ids\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 2: Deduplicate on transaction identity\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def deduplicate_transactions(\n",
        "    df: pd.DataFrame,\n",
        "    sort_policy: Any  # Typed as Any to avoid circular import of SortPolicy class, but effectively SortPolicy\n",
        ") -> Tuple[pd.DataFrame, List[str]]:\n",
        "    \"\"\"\n",
        "    Deduplicates transactions based on 'transaction_id', keeping the first occurrence\n",
        "    after applying a deterministic sort order.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The transactions DataFrame with valid endpoints.\n",
        "    sort_policy : SortPolicy\n",
        "        The sorting policy object defining columns and order for deterministic sorting.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, List[str]]\n",
        "        1. The deduplicated DataFrame.\n",
        "        2. List of transaction_ids that were removed as duplicates.\n",
        "    \"\"\"\n",
        "    # Apply deterministic sort\n",
        "    # We assume sort_policy has .sort_columns and .ascending attributes\n",
        "    try:\n",
        "        df_sorted = df.sort_values(\n",
        "            by=sort_policy.sort_columns,\n",
        "            ascending=sort_policy.ascending\n",
        "        )\n",
        "    except KeyError as e:\n",
        "        # Fallback if sort columns are missing (e.g. during testing with partial data)\n",
        "        logger.warning(f\"Sort columns missing: {e}. Falling back to 'transaction_id'.\")\n",
        "        df_sorted = df.sort_values(by=['transaction_id'])\n",
        "\n",
        "    # Identify duplicates\n",
        "    # keep='first' retains the first row in the sorted order\n",
        "    is_duplicate = df_sorted.duplicated(subset=['transaction_id'], keep='first')\n",
        "\n",
        "    # Extract IDs of removed rows\n",
        "    # Note: This list might contain the same ID multiple times if a transaction ID\n",
        "    # appears 3+ times. This is correct behavior for an exclusion ledger (one entry per dropped row).\n",
        "    duplicate_ids = df_sorted.loc[is_duplicate, 'transaction_id'].astype(str).tolist()\n",
        "\n",
        "    # Filter\n",
        "    df_deduped = df_sorted.loc[~is_duplicate].copy()\n",
        "\n",
        "    logger.info(f\"Deduplication: {len(duplicate_ids)} duplicate rows removed.\")\n",
        "\n",
        "    return df_deduped, duplicate_ids\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 3: Retain but do not filter on transaction type code\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def verify_transaction_type_retention(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Verifies that the 'transaction_transaction_type_code' column is present\n",
        "    and logs that no filtering has been applied to it, consistent with baseline requirements.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The deduplicated transactions DataFrame.\n",
        "    \"\"\"\n",
        "    if 'transaction_transaction_type_code' in df.columns:\n",
        "        unique_types = df['transaction_transaction_type_code'].unique()\n",
        "        logger.info(f\"Transaction Type Retention: Column present. \"\n",
        "                    f\"Contains {len(unique_types)} unique types. No filtering applied.\")\n",
        "    else:\n",
        "        logger.warning(\"Transaction Type Retention: 'transaction_transaction_type_code' column missing.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_transactions_endpoints_dedup(\n",
        "    df_transactions_filtered: pd.DataFrame,\n",
        "    sort_policy: Any\n",
        ") -> Tuple[pd.DataFrame, EndpointExclusions]:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 5. Executes endpoint validation and deterministic deduplication.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions_filtered : pd.DataFrame\n",
        "        The transactions DataFrame resulting from Task 4 (temporal/value cleansed).\n",
        "    sort_policy : SortPolicy\n",
        "        The sorting policy for transactions to ensure deterministic deduplication.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, EndpointExclusions]\n",
        "        1. The fully cleansed transactions DataFrame ready for joining.\n",
        "        2. An exclusions ledger object tracking dropped rows.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 5: Endpoint Validity and Deduplication...\")\n",
        "\n",
        "    exclusions = EndpointExclusions()\n",
        "\n",
        "    # Step 1: Enforce Endpoint Validity\n",
        "    df_valid, missing_prov, missing_recv = enforce_endpoint_validity(df_transactions_filtered)\n",
        "    exclusions.missing_provider_ids = missing_prov\n",
        "    exclusions.missing_receiver_ids = missing_recv\n",
        "\n",
        "    # Step 2: Deduplicate\n",
        "    df_deduped, dup_ids = deduplicate_transactions(df_valid, sort_policy)\n",
        "    exclusions.duplicate_transaction_ids = dup_ids\n",
        "\n",
        "    # Step 3: Verify Type Retention\n",
        "    verify_transaction_type_retention(df_deduped)\n",
        "\n",
        "    logger.info(f\"Task 5 Completed. Rows remaining: {len(df_deduped)}. \"\n",
        "                f\"Total excluded: {exclusions.total_excluded()}.\")\n",
        "\n",
        "    return df_deduped, exclusions\n"
      ],
      "metadata": {
        "id": "BGrhYQhMEm4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6 — Cleanse df_activities_raw (join backbone and multi-value normalization)\n",
        "\n",
        "# ===============================================================================\n",
        "# Task 6: Cleanse df_activities_raw (join backbone and multi-value normalization)\n",
        "# ===============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ActivityExclusions:\n",
        "    \"\"\"\n",
        "    Container for tracking rows excluded during activity cleansing.\n",
        "\n",
        "    This dataclass serves as a structured ledger for all activity records dropped\n",
        "    during the cleansing and normalization phase. It specifically tracks exclusions\n",
        "    due to missing primary keys (`iati_identifier`) and duplicate records.\n",
        "    Maintaining accurate counts of these exclusions is essential for verifying\n",
        "    the integrity of the activity backbone, which serves as the join target for\n",
        "    transactional data.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    missing_identifier_count : int, default=0\n",
        "        The number of rows excluded because the `iati_identifier` field was null\n",
        "        or missing. These rows cannot be joined to transactions and are thus\n",
        "        invalid.\n",
        "    duplicate_identifier_count : int, default=0\n",
        "        The number of rows excluded because they shared an `iati_identifier` with\n",
        "        another row that was retained (based on deterministic sorting). Duplicate\n",
        "        activities can cause Cartesian explosions during joins if not removed.\n",
        "    \"\"\"\n",
        "    # Count of rows with missing IATI identifiers\n",
        "    missing_identifier_count: int = 0\n",
        "\n",
        "    # Count of duplicate rows removed\n",
        "    duplicate_identifier_count: int = 0\n",
        "\n",
        "    def total_excluded(self) -> int:\n",
        "        \"\"\"\n",
        "        Computes the total number of activity rows excluded.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The sum of missing identifier counts and duplicate identifier counts.\n",
        "        \"\"\"\n",
        "        # Sum the counts to get the total number of excluded rows\n",
        "        return self.missing_identifier_count + self.duplicate_identifier_count\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 1: Validate activity identifier integrity\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_activity_backbone(\n",
        "    df: pd.DataFrame,\n",
        "    sort_policy: Any\n",
        ") -> Tuple[pd.DataFrame, ActivityExclusions]:\n",
        "    \"\"\"\n",
        "    Validates activity identifiers, removes nulls, and deduplicates deterministically.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The raw activities DataFrame.\n",
        "    sort_policy : SortPolicy\n",
        "        The sorting policy for activities to ensure deterministic deduplication.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, ActivityExclusions]\n",
        "        1. The cleaned activity backbone DataFrame.\n",
        "        2. An exclusions ledger object.\n",
        "    \"\"\"\n",
        "    exclusions = ActivityExclusions()\n",
        "\n",
        "    # 1. Remove rows with missing iati_identifier\n",
        "    initial_count = len(df)\n",
        "    df_clean = df.dropna(subset=['iati_identifier']).copy()\n",
        "    exclusions.missing_identifier_count = initial_count - len(df_clean)\n",
        "\n",
        "    # 2. Deduplicate deterministically\n",
        "    # Apply sort\n",
        "    try:\n",
        "        df_sorted = df_clean.sort_values(\n",
        "            by=sort_policy.sort_columns,\n",
        "            ascending=sort_policy.ascending\n",
        "        )\n",
        "    except KeyError as e:\n",
        "        logger.warning(f\"Sort columns missing: {e}. Falling back to 'iati_identifier'.\")\n",
        "        df_sorted = df_clean.sort_values(by=['iati_identifier'])\n",
        "\n",
        "    # Drop duplicates, keeping first\n",
        "    # We check for duplicates in iati_identifier\n",
        "    is_dup = df_sorted.duplicated(subset=['iati_identifier'], keep='first')\n",
        "    exclusions.duplicate_identifier_count = is_dup.sum()\n",
        "\n",
        "    df_final = df_sorted.loc[~is_dup].copy()\n",
        "\n",
        "    logger.info(f\"Activity Backbone: {exclusions.missing_identifier_count} missing IDs, \"\n",
        "                f\"{exclusions.duplicate_identifier_count} duplicates removed.\")\n",
        "\n",
        "    return df_final, exclusions\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 2: Normalize multi-valued recipient country field\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_activity_countries(\n",
        "    df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalizes the 'recipient_country_code' field by exploding multi-valued entries\n",
        "    into a long-form DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The cleaned activity backbone DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        A DataFrame with columns ['iati_identifier', 'recipient_country_code'],\n",
        "        where each row represents a unique (activity, country) pair.\n",
        "    \"\"\"\n",
        "    # Select relevant columns\n",
        "    # We use .copy() to ensure we don't affect the backbone\n",
        "    df_countries = df[['iati_identifier', 'recipient_country_code']].copy()\n",
        "\n",
        "    # Drop nulls immediately to avoid exploding NaNs\n",
        "    df_countries = df_countries.dropna(subset=['recipient_country_code'])\n",
        "\n",
        "    # Helper to normalize input to list\n",
        "    def to_list(val: Union[str, List, Any]) -> List[str]:\n",
        "        if isinstance(val, list):\n",
        "            return [str(x).strip() for x in val if x]\n",
        "        if isinstance(val, str):\n",
        "            # Assume semicolon delimiter for strings, common in IATI\n",
        "            return [x.strip() for x in val.split(';') if x.strip()]\n",
        "        return [str(val).strip()]\n",
        "\n",
        "    # Apply normalization\n",
        "    df_countries['recipient_country_code'] = df_countries['recipient_country_code'].apply(to_list)\n",
        "\n",
        "    # Explode\n",
        "    df_exploded = df_countries.explode('recipient_country_code')\n",
        "\n",
        "    # Ensure clean strings\n",
        "    df_exploded['recipient_country_code'] = df_exploded['recipient_country_code'].astype(str)\n",
        "\n",
        "    # Remove duplicates if an activity listed the same country twice\n",
        "    df_final = df_exploded.drop_duplicates()\n",
        "\n",
        "    logger.info(f\"Normalized Countries: {len(df_final)} rows generated from {len(df)} activities.\")\n",
        "\n",
        "    return df_final\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 3: Normalize multi-valued sector field\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_activity_sectors(\n",
        "    df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalizes the 'sector_code' field by exploding multi-valued entries\n",
        "    into a long-form DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The cleaned activity backbone DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        A DataFrame with columns ['iati_identifier', 'sector_code'],\n",
        "        where each row represents a unique (activity, sector) pair.\n",
        "    \"\"\"\n",
        "    # Select relevant columns\n",
        "    df_sectors = df[['iati_identifier', 'sector_code']].copy()\n",
        "\n",
        "    # Drop nulls\n",
        "    df_sectors = df_sectors.dropna(subset=['sector_code'])\n",
        "\n",
        "    # Helper to normalize input to list (same logic as countries)\n",
        "    def to_list(val: Union[str, List, Any]) -> List[str]:\n",
        "        if isinstance(val, list):\n",
        "            return [str(x).strip() for x in val if x]\n",
        "        if isinstance(val, str):\n",
        "            return [x.strip() for x in val.split(';') if x.strip()]\n",
        "        return [str(val).strip()]\n",
        "\n",
        "    # Apply normalization\n",
        "    df_sectors['sector_code'] = df_sectors['sector_code'].apply(to_list)\n",
        "\n",
        "    # Explode\n",
        "    df_exploded = df_sectors.explode('sector_code')\n",
        "\n",
        "    # Ensure clean strings\n",
        "    df_exploded['sector_code'] = df_exploded['sector_code'].astype(str)\n",
        "\n",
        "    # Remove duplicates\n",
        "    df_final = df_exploded.drop_duplicates()\n",
        "\n",
        "    logger.info(f\"Normalized Sectors: {len(df_final)} rows generated from {len(df)} activities.\")\n",
        "\n",
        "    return df_final\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_activities_normalization(\n",
        "    df_activities_raw: pd.DataFrame,\n",
        "    sort_policy: Any\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, ActivityExclusions]:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 6. Cleanses the activity backbone and normalizes\n",
        "    multi-valued context fields (countries, sectors).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_activities_raw : pd.DataFrame\n",
        "        The raw activities DataFrame.\n",
        "    sort_policy : SortPolicy\n",
        "        The sorting policy for activities.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, ActivityExclusions]\n",
        "        1. The cleaned activity backbone DataFrame (unique iati_identifier).\n",
        "        2. The normalized countries DataFrame (long form).\n",
        "        3. The normalized sectors DataFrame (long form).\n",
        "        4. An exclusions ledger object.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 6: Activity Cleansing and Normalization...\")\n",
        "\n",
        "    # Step 1: Cleanse Backbone\n",
        "    df_backbone, exclusions = cleanse_activity_backbone(df_activities_raw, sort_policy)\n",
        "\n",
        "    # Step 2: Normalize Countries\n",
        "    df_countries = normalize_activity_countries(df_backbone)\n",
        "\n",
        "    # Step 3: Normalize Sectors\n",
        "    df_sectors = normalize_activity_sectors(df_backbone)\n",
        "\n",
        "    logger.info(\"Task 6 Completed Successfully.\")\n",
        "\n",
        "    return df_backbone, df_countries, df_sectors, exclusions\n"
      ],
      "metadata": {
        "id": "LsElX5OMGytC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7 — Cleanse df_activities_raw (instrument type normalization)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Cleanse df_activities_raw (instrument type normalization)\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class InstrumentExclusions:\n",
        "    \"\"\"\n",
        "    Container for tracking rows excluded during instrument normalization.\n",
        "\n",
        "    This dataclass serves as a structured ledger for tracking the number of\n",
        "    activity records that contained invalid or unmappable instrument types.\n",
        "    By quantifying these exclusions, we can assess the quality of the\n",
        "    financial instrument metadata and ensure that downstream analyses (such as\n",
        "    longitudinal instrument evolution) are based on a clean, standardized\n",
        "    taxonomy {Grant, Loan, Equity}.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    bad_instrument_type_count : int, default=0\n",
        "        The count of activity rows where the `instrument_type` field contained\n",
        "        a value that could not be mapped to the controlled vocabulary\n",
        "        {Grant, Loan, Equity} and was consequently set to NaN.\n",
        "    \"\"\"\n",
        "    # Count of invalid instrument types found\n",
        "    bad_instrument_type_count: int = 0\n",
        "\n",
        "    def total_excluded(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the total count of instrument exclusions.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The number of rows with invalid instrument types.\n",
        "        \"\"\"\n",
        "        # Return the tracked count\n",
        "        return self.bad_instrument_type_count\n",
        "\n",
        "@dataclass\n",
        "class ActivityCoverageReport:\n",
        "    \"\"\"\n",
        "    Container for activity-level coverage statistics.\n",
        "\n",
        "    This dataclass encapsulates key data quality metrics regarding the completeness\n",
        "    of activity metadata. It tracks the proportion of activities that have valid\n",
        "    instrument types, recipient countries, and sector codes. These metrics are\n",
        "    critical for understanding the potential bias in downstream projections (e.g.,\n",
        "    co-occurrence networks) where missing context data leads to exclusion from\n",
        "    specific analyses.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    total_activities : int\n",
        "        The total number of unique activities in the cleaned backbone.\n",
        "    instrument_null_rate : float\n",
        "        The fraction of activities (0.0 to 1.0) that lack a valid instrument type.\n",
        "    country_coverage_rate : float\n",
        "        The fraction of activities (0.0 to 1.0) that are associated with at least\n",
        "        one valid recipient country.\n",
        "    sector_coverage_rate : float\n",
        "        The fraction of activities (0.0 to 1.0) that are associated with at least\n",
        "        one valid sector code.\n",
        "    \"\"\"\n",
        "    # Total count of unique activities\n",
        "    total_activities: int\n",
        "\n",
        "    # Fraction of activities missing instrument type\n",
        "    instrument_null_rate: float\n",
        "\n",
        "    # Fraction of activities with at least one country\n",
        "    country_coverage_rate: float\n",
        "\n",
        "    # Fraction of activities with at least one sector\n",
        "    sector_coverage_rate: float\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string representation of the coverage report.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the coverage statistics.\n",
        "        \"\"\"\n",
        "        # Format metrics as percentages for readability\n",
        "        return (f\"Activity Coverage: N={self.total_activities}, \"\n",
        "                f\"Null Inst={self.instrument_null_rate:.2%}, \"\n",
        "                f\"Country Cov={self.country_coverage_rate:.2%}, \"\n",
        "                f\"Sector Cov={self.sector_coverage_rate:.2%}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 1: Enforce instrument taxonomy consistent with Extended Data Fig. S1 categories\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_instrument_types(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, InstrumentExclusions]:\n",
        "    \"\"\"\n",
        "    Normalizes the 'instrument_type' column to a controlled vocabulary:\n",
        "    {Grant, Loan, Equity}. Quarantines unmappable values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The cleaned activity backbone DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, InstrumentExclusions]\n",
        "        1. The DataFrame with normalized instrument types (invalid set to NaN).\n",
        "        2. An exclusions ledger object tracking unmappable counts.\n",
        "    \"\"\"\n",
        "    # Work on a copy\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # Allowed taxonomy\n",
        "    allowed_types = {'Grant', 'Loan', 'Equity'}\n",
        "\n",
        "    # Normalize: Title Case, strip whitespace\n",
        "    # Handle non-string types gracefully\n",
        "    if 'instrument_type' in df_out.columns:\n",
        "        # Convert to string, strip, title case\n",
        "        # We treat 'nan', 'None' as actual NaN first to avoid stringifying them\n",
        "        df_out['instrument_type'] = df_out['instrument_type'].replace({np.nan: None})\n",
        "\n",
        "        # Apply normalization only to non-nulls\n",
        "        mask_notna = df_out['instrument_type'].notna()\n",
        "        df_out.loc[mask_notna, 'instrument_type'] = (\n",
        "            df_out.loc[mask_notna, 'instrument_type']\n",
        "            .astype(str)\n",
        "            .str.strip()\n",
        "            .str.title()\n",
        "        )\n",
        "\n",
        "        # Check validity\n",
        "        # Valid if in allowed set OR is null (we allow nulls, just track them as missing later)\n",
        "        # The task says \"Quarantine unmappable rows with reason BAD_INSTRUMENT_TYPE\".\n",
        "        # This implies we should set them to NaN and count them as \"bad\" (distinct from originally missing).\n",
        "\n",
        "        # Identify values that are NOT null and NOT in allowed set\n",
        "        current_values = df_out['instrument_type']\n",
        "        is_valid = current_values.isin(allowed_types) | current_values.isna()\n",
        "\n",
        "        bad_count = (~is_valid).sum()\n",
        "\n",
        "        # Set invalid to NaN\n",
        "        df_out.loc[~is_valid, 'instrument_type'] = np.nan\n",
        "\n",
        "        exclusions = InstrumentExclusions(bad_instrument_type_count=bad_count)\n",
        "\n",
        "        logger.info(f\"Instrument Normalization: {bad_count} invalid types set to NaN.\")\n",
        "\n",
        "    else:\n",
        "        logger.warning(\"Instrument Normalization: 'instrument_type' column missing.\")\n",
        "        exclusions = InstrumentExclusions(bad_instrument_type_count=0)\n",
        "        df_out['instrument_type'] = np.nan\n",
        "\n",
        "    return df_out, exclusions\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 2 & 3: Compute activity-level coverage statistics\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_activity_coverage(\n",
        "    df_backbone: pd.DataFrame,\n",
        "    df_countries: pd.DataFrame,\n",
        "    df_sectors: pd.DataFrame\n",
        ") -> ActivityCoverageReport:\n",
        "    \"\"\"\n",
        "    Computes coverage statistics for instruments, countries, and sectors across\n",
        "    the unique activity set.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_backbone : pd.DataFrame\n",
        "        The cleaned activity backbone DataFrame (unique iati_identifier).\n",
        "    df_countries : pd.DataFrame\n",
        "        The normalized countries DataFrame (long form).\n",
        "    df_sectors : pd.DataFrame\n",
        "        The normalized sectors DataFrame (long form).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ActivityCoverageReport\n",
        "        A structured report of coverage rates.\n",
        "    \"\"\"\n",
        "    total_activities = len(df_backbone)\n",
        "\n",
        "    if total_activities == 0:\n",
        "        return ActivityCoverageReport(0, 0.0, 0.0, 0.0)\n",
        "\n",
        "    # 1. Instrument Null Rate\n",
        "    # Count nulls in backbone\n",
        "    n_null_inst = df_backbone['instrument_type'].isna().sum()\n",
        "    inst_null_rate = n_null_inst / total_activities\n",
        "\n",
        "    # 2. Country Coverage Rate\n",
        "    # Count unique IDs in df_countries that exist in backbone\n",
        "    # (They should all exist if derived from backbone, but we check intersection for rigor)\n",
        "    ids_with_country = set(df_countries['iati_identifier'].unique())\n",
        "    ids_in_backbone = set(df_backbone['iati_identifier'].unique())\n",
        "\n",
        "    n_with_country = len(ids_with_country.intersection(ids_in_backbone))\n",
        "    country_cov_rate = n_with_country / total_activities\n",
        "\n",
        "    # 3. Sector Coverage Rate\n",
        "    ids_with_sector = set(df_sectors['iati_identifier'].unique())\n",
        "    n_with_sector = len(ids_with_sector.intersection(ids_in_backbone))\n",
        "    sector_cov_rate = n_with_sector / total_activities\n",
        "\n",
        "    report = ActivityCoverageReport(\n",
        "        total_activities=total_activities,\n",
        "        instrument_null_rate=inst_null_rate,\n",
        "        country_coverage_rate=country_cov_rate,\n",
        "        sector_coverage_rate=sector_cov_rate\n",
        "    )\n",
        "\n",
        "    logger.info(str(report))\n",
        "\n",
        "    return report\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_activities_instruments_coverage(\n",
        "    df_backbone: pd.DataFrame,\n",
        "    df_countries: pd.DataFrame,\n",
        "    df_sectors: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, InstrumentExclusions, ActivityCoverageReport]:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 7. Normalizes instruments and computes final activity coverage metrics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_backbone : pd.DataFrame\n",
        "        The cleaned activity backbone from Task 6.\n",
        "    df_countries : pd.DataFrame\n",
        "        The normalized countries from Task 6.\n",
        "    df_sectors : pd.DataFrame\n",
        "        The normalized sectors from Task 6.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, InstrumentExclusions, ActivityCoverageReport]\n",
        "        1. The backbone DataFrame with normalized instruments.\n",
        "        2. An exclusions ledger for instruments.\n",
        "        3. A coverage report.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 7: Instrument Normalization and Coverage Analysis...\")\n",
        "\n",
        "    # Step 1: Normalize Instruments\n",
        "    df_final, exclusions = normalize_instrument_types(df_backbone)\n",
        "\n",
        "    # Step 2 & 3: Compute Coverage\n",
        "    coverage_report = compute_activity_coverage(df_final, df_countries, df_sectors)\n",
        "\n",
        "    logger.info(\"Task 7 Completed Successfully.\")\n",
        "\n",
        "    return df_final, exclusions, coverage_report\n"
      ],
      "metadata": {
        "id": "gs5G39rAIZuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8 — Cleanse df_organisations_raw (entity resolution substrate)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Cleanse df_organisations_raw (entity resolution substrate)\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class OrganisationExclusions:\n",
        "    \"\"\"\n",
        "    Container for tracking rows excluded or flagged during organisation cleansing.\n",
        "\n",
        "    This dataclass serves as a structured ledger for data quality issues identified\n",
        "    within the organisation master table. It distinguishes between fatal errors\n",
        "    that require row exclusion (missing primary keys) and non-fatal data quality\n",
        "    issues (invalid type labels) that result in field nullification but retention\n",
        "    of the record. This distinction preserves the maximum amount of usable\n",
        "    entity data for the resolution process.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    missing_ref_count : int, default=0\n",
        "        The number of rows excluded because the `org_ref` field was null or missing.\n",
        "        Since `org_ref` is the primary key for joining and entity resolution,\n",
        "        records without it are unusable.\n",
        "    bad_org_type_count : int, default=0\n",
        "        The number of rows where the `org_type` field contained a value outside\n",
        "        the controlled vocabulary. These fields are set to NaN, but the organisation\n",
        "        record itself is retained.\n",
        "    \"\"\"\n",
        "    # Count of rows dropped due to missing reference ID\n",
        "    missing_ref_count: int = 0\n",
        "\n",
        "    # Count of rows with invalid organisation type labels\n",
        "    bad_org_type_count: int = 0\n",
        "\n",
        "    def total_excluded(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the total count of rows strictly excluded from the dataset.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The count of rows missing a reference ID. Invalid types do not\n",
        "            cause row exclusion, so they are not summed here.\n",
        "        \"\"\"\n",
        "        # Only missing refs cause row exclusion; bad types just nullify the field\n",
        "        return self.missing_ref_count\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 1: Enforce organisation record integrity\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_domains(domain_series: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Normalizes website domains by stripping protocols, paths, and lowercasing.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    domain_series : pd.Series\n",
        "        Series containing raw URL/domain strings.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Normalized domain strings.\n",
        "    \"\"\"\n",
        "    # Convert to string, lowercase, strip whitespace\n",
        "    s = domain_series.astype(str).str.lower().str.strip()\n",
        "\n",
        "    # Remove protocol (http://, https://)\n",
        "    s = s.str.replace(r'^https?://', '', regex=True)\n",
        "\n",
        "    # Remove 'www.' prefix (optional, but good for canonicalization)\n",
        "    # The task says \"define subdomain policy\". We will keep subdomains generally\n",
        "    # but stripping 'www.' is standard practice for entity resolution.\n",
        "    s = s.str.replace(r'^www\\.', '', regex=True)\n",
        "\n",
        "    # Remove trailing paths and slashes\n",
        "    s = s.str.split('/').str[0]\n",
        "\n",
        "    # Handle 'nan' strings resulting from astype(str) on nulls\n",
        "    s = s.replace({'nan': np.nan, 'none': np.nan, '': np.nan})\n",
        "\n",
        "    return s\n",
        "\n",
        "def cleanse_organisation_records(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, OrganisationExclusions]:\n",
        "    \"\"\"\n",
        "    Validates organisation records, normalizes names and domains, and removes\n",
        "    rows without a reference ID.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The raw organisations DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, OrganisationExclusions]\n",
        "        1. The cleaned organisations DataFrame.\n",
        "        2. An exclusions ledger.\n",
        "    \"\"\"\n",
        "    exclusions = OrganisationExclusions()\n",
        "\n",
        "    # 1. Enforce org_ref presence\n",
        "    initial_count = len(df)\n",
        "    df_clean = df.dropna(subset=['org_ref']).copy()\n",
        "    exclusions.missing_ref_count = initial_count - len(df_clean)\n",
        "\n",
        "    # 2. Normalize Names\n",
        "    if 'org_name' in df_clean.columns:\n",
        "        # Unicode normalize (NFKD), lowercase, strip\n",
        "        df_clean['org_name_normalized'] = (\n",
        "            df_clean['org_name']\n",
        "            .astype(str)\n",
        "            .str.normalize('NFKD')\n",
        "            .str.lower()\n",
        "            .str.strip()\n",
        "            .replace({'nan': np.nan, 'none': np.nan, '': np.nan})\n",
        "        )\n",
        "    else:\n",
        "        logger.warning(\"Organisation Cleansing: 'org_name' column missing.\")\n",
        "        df_clean['org_name_normalized'] = np.nan\n",
        "\n",
        "    # 3. Normalize Domains\n",
        "    if 'website_domain' in df_clean.columns:\n",
        "        df_clean['website_domain_normalized'] = normalize_domains(df_clean['website_domain'])\n",
        "    else:\n",
        "        logger.warning(\"Organisation Cleansing: 'website_domain' column missing.\")\n",
        "        df_clean['website_domain_normalized'] = np.nan\n",
        "\n",
        "    logger.info(f\"Organisation Records: {exclusions.missing_ref_count} missing refs removed.\")\n",
        "\n",
        "    return df_clean, exclusions\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 2: Validate organisation type taxonomy\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_org_taxonomy(\n",
        "    df: pd.DataFrame,\n",
        "    exclusions: OrganisationExclusions\n",
        ") -> Tuple[pd.DataFrame, OrganisationExclusions]:\n",
        "    \"\"\"\n",
        "    Enforces the controlled vocabulary for organisation types.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The cleaned organisations DataFrame.\n",
        "    exclusions : OrganisationExclusions\n",
        "        The existing exclusions ledger.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, OrganisationExclusions]\n",
        "        1. The DataFrame with validated 'org_type' (invalid set to NaN).\n",
        "        2. The updated exclusions ledger.\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # Controlled vocabulary\n",
        "    allowed_types = {\n",
        "        \"Government\", \"Multilateral\", \"Foundation\", \"Academic/Research\",\n",
        "        \"International NGO\", \"National NGO\", \"Private Sector\", \"Other\"\n",
        "    }\n",
        "\n",
        "    if 'org_type' in df_out.columns:\n",
        "        # Normalize input to match vocabulary (Title Case)\n",
        "        # We assume input is close to valid, just needs casing fix\n",
        "        # If input is raw IATI codes (10, 15, etc.), a mapping step would be needed upstream.\n",
        "        # Here we assume labels are present but potentially messy.\n",
        "        # Convert to string, strip\n",
        "        s = df_out['org_type'].astype(str).str.strip()\n",
        "\n",
        "        # Check validity\n",
        "        is_valid = s.isin(allowed_types)\n",
        "\n",
        "        # Count invalid (excluding actual NaNs which become 'nan' string)\n",
        "        # We treat 'nan' as missing, not invalid type per se, but for the count\n",
        "        # we want to know how many *labels* were rejected.\n",
        "        # If original was NaN, s is 'nan'. 'nan' is not in allowed_types.\n",
        "        # So we should check if original was not null.\n",
        "        was_not_null = df_out['org_type'].notna()\n",
        "        bad_labels = was_not_null & (~is_valid)\n",
        "\n",
        "        bad_count = bad_labels.sum()\n",
        "        exclusions.bad_org_type_count = bad_count\n",
        "\n",
        "        # Set invalid to NaN (keep valid ones)\n",
        "        # We rely on the original column values if they matched, or we could standardize.\n",
        "        # Let's standardize to the allowed set values (which they matched).\n",
        "        # Since we only checked exact match (isin), the values are already standard.\n",
        "        df_out.loc[~is_valid, 'org_type'] = np.nan\n",
        "\n",
        "        logger.info(f\"Org Taxonomy: {bad_count} invalid types set to NaN.\")\n",
        "\n",
        "    else:\n",
        "        logger.warning(\"Org Taxonomy: 'org_type' column missing.\")\n",
        "        df_out['org_type'] = np.nan\n",
        "\n",
        "    return df_out, exclusions\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 3: Prepare alias structure\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_aliases(\n",
        "    df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Standardizes organisation aliases into a list of normalized strings.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The validated organisations DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        The DataFrame with a new 'aliases_normalized' column (List[str]).\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    if 'org_name_aliases' not in df_out.columns:\n",
        "        logger.info(\"Alias Preparation: 'org_name_aliases' missing. Creating empty lists.\")\n",
        "        df_out['aliases_normalized'] = [[] for _ in range(len(df_out))]\n",
        "        return df_out\n",
        "\n",
        "    def normalize_alias_list(val: Union[str, List, Any]) -> List[str]:\n",
        "        raw_list = []\n",
        "        if isinstance(val, list):\n",
        "            raw_list = val\n",
        "        elif isinstance(val, str):\n",
        "            # Assume semicolon delimiter\n",
        "            raw_list = val.split(';')\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "        # Normalize each alias\n",
        "        normalized = []\n",
        "        for alias in raw_list:\n",
        "            s = str(alias).strip()\n",
        "            if s:\n",
        "                # Apply same normalization as org_name\n",
        "                norm_s = (\n",
        "                    pd.Series([s])\n",
        "                    .str.normalize('NFKD')\n",
        "                    .str.lower()\n",
        "                    .str.strip()\n",
        "                    .iloc[0]\n",
        "                )\n",
        "                normalized.append(norm_s)\n",
        "        return list(set(normalized)) # Deduplicate\n",
        "\n",
        "    df_out['aliases_normalized'] = df_out['org_name_aliases'].apply(normalize_alias_list)\n",
        "\n",
        "    logger.info(\"Alias Preparation: Aliases normalized and deduplicated.\")\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_organisations_substrate(\n",
        "    df_organisations_raw: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, OrganisationExclusions]:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 8. Prepares the organisation master table for entity resolution.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_organisations_raw : pd.DataFrame\n",
        "        The raw organisations DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, OrganisationExclusions]\n",
        "        1. The fully cleansed organisations DataFrame.\n",
        "        2. An exclusions ledger.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 8: Organisation Cleansing...\")\n",
        "\n",
        "    # Step 1: Record Integrity & Normalization\n",
        "    df_clean, exclusions = cleanse_organisation_records(df_organisations_raw)\n",
        "\n",
        "    # Step 2: Taxonomy Validation\n",
        "    df_typed, exclusions = validate_org_taxonomy(df_clean, exclusions)\n",
        "\n",
        "    # Step 3: Alias Preparation\n",
        "    df_final = prepare_aliases(df_typed)\n",
        "\n",
        "    logger.info(\"Task 8 Completed Successfully.\")\n",
        "\n",
        "    return df_final, exclusions\n"
      ],
      "metadata": {
        "id": "9W2zPcFIJ67Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9 — Join transactions to activity contexts (countries, sectors, instruments)\n",
        "\n",
        "# ================================================================================\n",
        "# Task 9: Join transactions to activity contexts (countries, sectors, instruments)\n",
        "# ================================================================================\n",
        "\n",
        "@dataclass\n",
        "class ContextCoverageReport:\n",
        "    \"\"\"\n",
        "    Container for transaction-level context coverage statistics.\n",
        "\n",
        "    This dataclass encapsulates the success rates of joining transaction records\n",
        "    to their associated activity-level metadata (contexts). It tracks the total\n",
        "    number of transactions and the subset that successfully matched to at least\n",
        "    one recipient country, sector code, or instrument type. These metrics are\n",
        "    vital for assessing the \"projection eligibility\" of the transaction corpus:\n",
        "    transactions without contexts cannot contribute to the co-occurrence network\n",
        "    projection, representing a potential source of topological bias.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    total_transactions : int\n",
        "        The total number of transactions in the cleansed dataset (T++).\n",
        "    transactions_with_country : int\n",
        "        The number of transactions that were successfully joined to at least one\n",
        "        recipient country context.\n",
        "    transactions_with_sector : int\n",
        "        The number of transactions that were successfully joined to at least one\n",
        "        sector code context.\n",
        "    transactions_with_instrument : int\n",
        "        The number of transactions that were successfully joined to a valid\n",
        "        instrument type.\n",
        "    \"\"\"\n",
        "    # Total count of transactions processed\n",
        "    total_transactions: int\n",
        "\n",
        "    # Count of transactions with valid country context\n",
        "    transactions_with_country: int\n",
        "\n",
        "    # Count of transactions with valid sector context\n",
        "    transactions_with_sector: int\n",
        "\n",
        "    # Count of transactions with valid instrument type\n",
        "    transactions_with_instrument: int\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string representation of the coverage report.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the context coverage percentages.\n",
        "        \"\"\"\n",
        "        # Calculate percentages for readability\n",
        "        return (f\"Context Coverage (N={self.total_transactions}): \"\n",
        "                f\"Country={self.transactions_with_country/self.total_transactions:.2%}, \"\n",
        "                f\"Sector={self.transactions_with_sector/self.total_transactions:.2%}, \"\n",
        "                f\"Instrument={self.transactions_with_instrument/self.total_transactions:.2%}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 1: Left-join transactions to activity metadata\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def join_activity_contexts(\n",
        "    df_transactions: pd.DataFrame,\n",
        "    df_activities_backbone: pd.DataFrame,\n",
        "    df_activities_countries: pd.DataFrame,\n",
        "    df_activities_sectors: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Joins transactions to activity contexts, producing separate tables to avoid\n",
        "    Cartesian explosion between countries and sectors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions : pd.DataFrame\n",
        "        The cleansed transactions DataFrame.\n",
        "    df_activities_backbone : pd.DataFrame\n",
        "        Activity backbone with 'instrument_type'.\n",
        "    df_activities_countries : pd.DataFrame\n",
        "        Long-form activity-country mapping.\n",
        "    df_activities_sectors : pd.DataFrame\n",
        "        Long-form activity-sector mapping.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n",
        "        1. df_tx_instruments: Transactions enriched with instrument_type (1:1).\n",
        "        2. df_tx_countries: Transactions exploded by country (1:M).\n",
        "        3. df_tx_sectors: Transactions exploded by sector (1:M).\n",
        "    \"\"\"\n",
        "    # 1. Join Instruments (1:1 expected)\n",
        "    # We only need iati_identifier and instrument_type from backbone\n",
        "    df_tx_instruments = pd.merge(\n",
        "        df_transactions,\n",
        "        df_activities_backbone[['iati_identifier', 'instrument_type']],\n",
        "        on='iati_identifier',\n",
        "        how='left',\n",
        "        validate='many_to_one' # Enforce that backbone is unique on join key\n",
        "    )\n",
        "\n",
        "    # 2. Join Countries (1:M)\n",
        "    # We keep transaction_id to link back\n",
        "    df_tx_countries = pd.merge(\n",
        "        df_transactions[['transaction_id', 'iati_identifier']],\n",
        "        df_activities_countries,\n",
        "        on='iati_identifier',\n",
        "        how='inner' # Inner join to keep only valid contexts; missing ones are just absent\n",
        "    )\n",
        "\n",
        "    # 3. Join Sectors (1:M)\n",
        "    df_tx_sectors = pd.merge(\n",
        "        df_transactions[['transaction_id', 'iati_identifier']],\n",
        "        df_activities_sectors,\n",
        "        on='iati_identifier',\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Joins: Instruments N={len(df_tx_instruments)}, \"\n",
        "                f\"Countries N={len(df_tx_countries)}, Sectors N={len(df_tx_sectors)}\")\n",
        "\n",
        "    return df_tx_instruments, df_tx_countries, df_tx_sectors\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 2: Quantify join coverage and enforce exclusion policies\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_context_coverage(\n",
        "    df_transactions: pd.DataFrame,\n",
        "    df_tx_instruments: pd.DataFrame,\n",
        "    df_tx_countries: pd.DataFrame,\n",
        "    df_tx_sectors: pd.DataFrame\n",
        ") -> ContextCoverageReport:\n",
        "    \"\"\"\n",
        "    Computes the fraction of transactions that successfully matched to contexts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions : pd.DataFrame\n",
        "        Base transactions.\n",
        "    df_tx_instruments : pd.DataFrame\n",
        "        Transactions with instruments.\n",
        "    df_tx_countries : pd.DataFrame\n",
        "        Transactions exploded by country.\n",
        "    df_tx_sectors : pd.DataFrame\n",
        "        Transactions exploded by sector.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ContextCoverageReport\n",
        "        Coverage statistics.\n",
        "    \"\"\"\n",
        "    total_tx = len(df_transactions)\n",
        "\n",
        "    # Instruments: check non-null in the joined table\n",
        "    # Note: df_tx_instruments has same row count as df_transactions (left join)\n",
        "    n_with_inst = df_tx_instruments['instrument_type'].notna().sum()\n",
        "\n",
        "    # Countries: count unique transaction_ids in the inner-joined table\n",
        "    n_with_country = df_tx_countries['transaction_id'].nunique()\n",
        "\n",
        "    # Sectors: count unique transaction_ids in the inner-joined table\n",
        "    n_with_sector = df_tx_sectors['transaction_id'].nunique()\n",
        "\n",
        "    report = ContextCoverageReport(\n",
        "        total_transactions=total_tx,\n",
        "        transactions_with_country=n_with_country,\n",
        "        transactions_with_sector=n_with_sector,\n",
        "        transactions_with_instrument=n_with_inst\n",
        "    )\n",
        "\n",
        "    logger.info(str(report))\n",
        "\n",
        "    return report\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 3: Define the projection context function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_unified_projection_contexts(\n",
        "    df_tx_countries: pd.DataFrame,\n",
        "    df_tx_sectors: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs the unified context table for projection, applying namespacing\n",
        "    to distinguish countries from sectors.\n",
        "\n",
        "    Context Function: kappa(t) in {COUNTRY:c, SECTOR:s}\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_tx_countries : pd.DataFrame\n",
        "        Transactions exploded by country.\n",
        "    df_tx_sectors : pd.DataFrame\n",
        "        Transactions exploded by sector.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        A long-form DataFrame with columns ['transaction_id', 'context_id'].\n",
        "    \"\"\"\n",
        "    # Prepare Country Contexts\n",
        "    # Namespace: \"COUNTRY:<code >\"\n",
        "    countries = df_tx_countries[['transaction_id', 'recipient_country_code']].copy()\n",
        "    countries['context_id'] = 'COUNTRY:' + countries['recipient_country_code'].astype(str)\n",
        "    countries = countries[['transaction_id', 'context_id']]\n",
        "\n",
        "    # Prepare Sector Contexts\n",
        "    # Namespace: \"SECTOR:<code>\"\n",
        "    sectors = df_tx_sectors[['transaction_id', 'sector_code']].copy()\n",
        "    sectors['context_id'] = 'SECTOR:' + sectors['sector_code'].astype(str)\n",
        "    sectors = sectors[['transaction_id', 'context_id']]\n",
        "\n",
        "    # Concatenate\n",
        "    df_contexts = pd.concat([countries, sectors], ignore_index=True)\n",
        "\n",
        "    # Deduplicate (just in case an activity listed same country/sector twice)\n",
        "    df_contexts = df_contexts.drop_duplicates()\n",
        "\n",
        "    logger.info(f\"Unified Contexts: {len(df_contexts)} context rows generated.\")\n",
        "\n",
        "    return df_contexts\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def join_transactions_to_contexts(\n",
        "    df_transactions: pd.DataFrame,\n",
        "    df_activities_backbone: pd.DataFrame,\n",
        "    df_activities_countries: pd.DataFrame,\n",
        "    df_activities_sectors: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, ContextCoverageReport]:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 9. Joins transactions to activity metadata and builds\n",
        "    the unified context table for projection.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions : pd.DataFrame\n",
        "        Cleansed transactions.\n",
        "    df_activities_backbone : pd.DataFrame\n",
        "        Activity backbone.\n",
        "    df_activities_countries : pd.DataFrame\n",
        "        Activity countries.\n",
        "    df_activities_sectors : pd.DataFrame\n",
        "        Activity sectors.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, pd.DataFrame, ContextCoverageReport]\n",
        "        1. df_tx_instruments: Transactions enriched with instrument info (for time series).\n",
        "        2. df_contexts: Unified long-form contexts (for projection).\n",
        "        3. Coverage report.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 9: Context Joins...\")\n",
        "\n",
        "    # Step 1: Joins\n",
        "    df_tx_inst, df_tx_ctry, df_tx_sect = join_activity_contexts(\n",
        "        df_transactions,\n",
        "        df_activities_backbone,\n",
        "        df_activities_countries,\n",
        "        df_activities_sectors\n",
        "    )\n",
        "\n",
        "    # Step 2: Coverage\n",
        "    report = compute_context_coverage(\n",
        "        df_transactions,\n",
        "        df_tx_inst,\n",
        "        df_tx_ctry,\n",
        "        df_tx_sect\n",
        "    )\n",
        "\n",
        "    # Step 3: Unified Contexts\n",
        "    df_contexts = build_unified_projection_contexts(df_tx_ctry, df_tx_sect)\n",
        "\n",
        "    logger.info(\"Task 9 Completed Successfully.\")\n",
        "\n",
        "    return df_tx_inst, df_contexts, report\n"
      ],
      "metadata": {
        "id": "lOh9Mk2HMPbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10 — Entity resolution: construct canonical organisation mapping\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Entity resolution: construct canonical organisation mapping\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class EntityResolutionConfig:\n",
        "    \"\"\"\n",
        "    Configuration for the entity resolution process.\n",
        "\n",
        "    This dataclass encapsulates the hyperparameters used to control the fuzzy\n",
        "    matching algorithm. By explicitly defining the similarity threshold and\n",
        "    n-gram range, we ensure that the entity resolution process is deterministic\n",
        "    and reproducible. These parameters govern the sensitivity of the matching\n",
        "    logic, balancing the risk of false positives (merging distinct entities)\n",
        "    against false negatives (failing to merge variants).\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    similarity_threshold : float, default=0.90\n",
        "        The minimum cosine similarity score (0.0 to 1.0) required to consider\n",
        "        two organisation names as a match. A higher threshold yields stricter\n",
        "        matching.\n",
        "    ngram_range : Tuple[int, int], default=(3, 3)\n",
        "        The range of n-gram sizes to use for TF-IDF vectorization. (3, 3)\n",
        "        indicates using character trigrams, which is robust against minor\n",
        "        spelling variations.\n",
        "    use_precomputed : bool, default=False\n",
        "        A flag indicating whether to bypass computation and use an existing\n",
        "        `canonical_org_id` column if present and trusted.\n",
        "    \"\"\"\n",
        "    # Minimum cosine similarity for a match\n",
        "    similarity_threshold: float = 0.90\n",
        "\n",
        "    # Range for character n-grams (min, max)\n",
        "    ngram_range: Tuple[int, int] = (3, 3)\n",
        "\n",
        "    # Flag to use existing mapping if available\n",
        "    use_precomputed: bool = False\n",
        "\n",
        "@dataclass\n",
        "class CanonicalMappingArtifact:\n",
        "    \"\"\"\n",
        "    Container for the final canonical mapping table and metadata.\n",
        "\n",
        "    This dataclass serves as the primary output artifact of the entity resolution\n",
        "    task. It contains the resolved mapping table that links raw organisation\n",
        "    references to their canonical identities, along with metadata describing\n",
        "    the resolution process. This artifact is essential for auditing the\n",
        "    consolidation of the network nodes and ensuring traceability from raw data\n",
        "    to the final graph topology.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    mapping_table : pd.DataFrame\n",
        "        A DataFrame containing the mapping between `org_ref` (raw) and\n",
        "        `canonical_org_id` (resolved).\n",
        "    resolution_method : str\n",
        "        A description of the method used to generate the mapping (e.g.,\n",
        "        \"Precomputed\" or \"TF-IDF/Cosine Clustering\").\n",
        "    cluster_count : int\n",
        "        The total number of unique canonical entities identified after resolution.\n",
        "    \"\"\"\n",
        "    # The mapping DataFrame\n",
        "    mapping_table: pd.DataFrame\n",
        "\n",
        "    # Description of the resolution method\n",
        "    resolution_method: str\n",
        "\n",
        "    # Count of unique canonical entities\n",
        "    cluster_count: int\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the mapping artifact.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary including entry count, unique entities,\n",
        "            and the method used.\n",
        "        \"\"\"\n",
        "        # Format summary string\n",
        "        return (f\"Canonical Mapping: {len(self.mapping_table)} entries, \"\n",
        "                f\"{self.cluster_count} unique entities. Method: {self.resolution_method}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 1: Determine if canonical mapping is pre-provided\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def check_precomputed_mapping(\n",
        "    df_organisations: pd.DataFrame\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Checks if a trusted canonical mapping already exists in the input data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_organisations : pd.DataFrame\n",
        "        The cleansed organisations DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if 'canonical_org_id' exists and is fully populated.\n",
        "    \"\"\"\n",
        "    if 'canonical_org_id' in df_organisations.columns:\n",
        "        null_rate = df_organisations['canonical_org_id'].isna().mean()\n",
        "        if null_rate < 0.01: # Allow <1% missing, treat as fully populated\n",
        "            logger.info(\"Entity Resolution: Precomputed 'canonical_org_id' found and trusted.\")\n",
        "            return True\n",
        "\n",
        "    logger.info(\"Entity Resolution: No precomputed mapping found. Proceeding to compute.\")\n",
        "    return False\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 2: Implement fuzzy matching framework\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_fuzzy_clusters(\n",
        "    df_organisations: pd.DataFrame,\n",
        "    config: EntityResolutionConfig\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Computes fuzzy clusters of organisations using TF-IDF and Cosine Similarity.\n",
        "    Returns a mapping from org_ref to a canonical cluster ID.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_organisations : pd.DataFrame\n",
        "        The cleansed organisations DataFrame. Must contain 'org_name_normalized'.\n",
        "    config : EntityResolutionConfig\n",
        "        Configuration parameters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, str]\n",
        "        A dictionary mapping 'org_ref' -> 'canonical_org_id'.\n",
        "    \"\"\"\n",
        "    # Filter for valid names\n",
        "    df_active = df_organisations.dropna(subset=['org_name_normalized', 'org_ref']).copy()\n",
        "\n",
        "    if len(df_active) == 0:\n",
        "        return {}\n",
        "\n",
        "    names = df_active['org_name_normalized'].tolist()\n",
        "    refs = df_active['org_ref'].tolist()\n",
        "\n",
        "    # Vectorize (Blocking strategy via TF-IDF on n-grams)\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        analyzer='char',\n",
        "        ngram_range=config.ngram_range,\n",
        "        min_df=1  # Keep rare n-grams\n",
        "    )\n",
        "    tfidf_matrix = vectorizer.fit_transform(names)\n",
        "\n",
        "    # Compute Similarity (Candidate generation)\n",
        "    # For N=2500, dense cosine similarity is fast enough (2500^2 = 6.25M entries)\n",
        "    # If N were larger (>10k), we would use sparse_dot_topn or Annoy.\n",
        "    sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    # Build Graph of Matches\n",
        "    G = nx.Graph()\n",
        "    # Add all nodes\n",
        "    for ref in refs:\n",
        "        G.add_node(ref)\n",
        "\n",
        "    # Add edges for pairs exceeding threshold\n",
        "    # We iterate upper triangle\n",
        "    rows, cols = np.where(np.triu(sim_matrix, k=1) >= config.similarity_threshold)\n",
        "\n",
        "    for r, c in zip(rows, cols):\n",
        "        G.add_edge(refs[r], refs[c])\n",
        "\n",
        "    # Extract Connected Components (Clusters)\n",
        "    mapping = {}\n",
        "    for component in nx.connected_components(G):\n",
        "        # Deterministic canonical ID selection:\n",
        "        # Sort by ref length (prefer shorter, often cleaner) or lexicographically\n",
        "        # Here we sort lexicographically for stability\n",
        "        members = sorted(list(component))\n",
        "        canonical_id = members[0] # Pick first as representative\n",
        "\n",
        "        for member in members:\n",
        "            mapping[member] = canonical_id\n",
        "\n",
        "    logger.info(f\"Fuzzy Matching: Resolved {len(refs)} refs into {len(list(nx.connected_components(G)))} clusters.\")\n",
        "\n",
        "    return mapping\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 3: Apply canonicalization to all organisation identities\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def apply_canonical_mapping(\n",
        "    df_organisations: pd.DataFrame,\n",
        "    computed_mapping: Dict[str, str],\n",
        "    use_precomputed: bool\n",
        ") -> Tuple[pd.DataFrame, CanonicalMappingArtifact]:\n",
        "    \"\"\"\n",
        "    Applies the canonical mapping to the organisation master table and generates\n",
        "    the final artifact.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_organisations : pd.DataFrame\n",
        "        The cleansed organisations DataFrame.\n",
        "    computed_mapping : Dict[str, str]\n",
        "        The mapping dictionary from Step 2.\n",
        "    use_precomputed : bool\n",
        "        Whether to use the existing column.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, CanonicalMappingArtifact]\n",
        "        1. The updated organisations DataFrame with 'canonical_org_id'.\n",
        "        2. The mapping artifact.\n",
        "    \"\"\"\n",
        "    df_out = df_organisations.copy()\n",
        "\n",
        "    if use_precomputed:\n",
        "        # Ensure column exists and fill gaps if any (self-map)\n",
        "        if 'canonical_org_id' not in df_out.columns:\n",
        "             df_out['canonical_org_id'] = df_out['org_ref'] # Fallback\n",
        "\n",
        "        df_out['canonical_org_id'] = df_out['canonical_org_id'].fillna(df_out['org_ref'])\n",
        "        method = \"Precomputed\"\n",
        "    else:\n",
        "        # Apply computed mapping\n",
        "        # Map org_ref -> canonical. If not in mapping (e.g. no name), map to self.\n",
        "        df_out['canonical_org_id'] = df_out['org_ref'].map(computed_mapping).fillna(df_out['org_ref'])\n",
        "        method = \"TF-IDF/Cosine Clustering\"\n",
        "\n",
        "    # Create artifact table\n",
        "    mapping_table = df_out[['org_ref', 'org_name', 'canonical_org_id']].copy()\n",
        "    unique_entities = df_out['canonical_org_id'].nunique()\n",
        "\n",
        "    artifact = CanonicalMappingArtifact(\n",
        "        mapping_table=mapping_table,\n",
        "        resolution_method=method,\n",
        "        cluster_count=unique_entities\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Canonicalization Applied. {unique_entities} unique entities.\")\n",
        "\n",
        "    return df_out, artifact\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_canonical_mapping(\n",
        "    df_organisations: pd.DataFrame,\n",
        "    config_dict: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, CanonicalMappingArtifact]:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 10. Performs entity resolution to create a canonical\n",
        "    organisation mapping.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_organisations : pd.DataFrame\n",
        "        The cleansed organisations DataFrame.\n",
        "    config_dict : Dict[str, Any]\n",
        "        The master config dictionary (for ER parameters).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, CanonicalMappingArtifact]\n",
        "        1. The organisations DataFrame enriched with 'canonical_org_id'.\n",
        "        2. The mapping artifact.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 10: Entity Resolution...\")\n",
        "\n",
        "    # Extract config\n",
        "    er_params = config_dict.get(\"ENTITY_RESOLUTION\", {}).get(\"fuzzy_matching\", {})\n",
        "    # Default to 0.90 if unspecified\n",
        "    threshold = er_params.get(\"threshold\", 0.90)\n",
        "    if threshold == \"UNSPECIFIED_IN_PAPER\":\n",
        "        threshold = 0.90\n",
        "\n",
        "    config = EntityResolutionConfig(similarity_threshold=float(threshold))\n",
        "\n",
        "    # Step 1: Check Precomputed\n",
        "    use_precomputed = check_precomputed_mapping(df_organisations)\n",
        "\n",
        "    # Step 2: Compute Clusters (if needed)\n",
        "    mapping_dict = {}\n",
        "    if not use_precomputed:\n",
        "        mapping_dict = compute_fuzzy_clusters(df_organisations, config)\n",
        "\n",
        "    # Step 3: Apply Mapping\n",
        "    df_final, artifact = apply_canonical_mapping(df_organisations, mapping_dict, use_precomputed)\n",
        "\n",
        "    logger.info(\"Task 10 Completed Successfully.\")\n",
        "\n",
        "    return df_final, artifact\n"
      ],
      "metadata": {
        "id": "b-eAzZMwN7YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11 — Compute geographic transaction density (Fig. 1-type output)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Compute geographic transaction density (Fig. 1-type output)\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class GeoDensityArtifact:\n",
        "    \"\"\"\n",
        "    Container for geographic density results and metadata.\n",
        "\n",
        "    This dataclass encapsulates the output of the geographic density computation,\n",
        "    providing both the processed density table (ready for choropleth visualization)\n",
        "    and the necessary coverage statistics to audit the mapping process. It ensures\n",
        "    that the \"global\" nature of the aid map is quantified by tracking exactly\n",
        "    how many transactions and countries are represented versus excluded.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    density_table : pd.DataFrame\n",
        "        A DataFrame containing transaction counts per country, with columns\n",
        "        ['recipient_country_code', 'transaction_count', 'log_transaction_count'].\n",
        "    total_transactions_mapped : int\n",
        "        The count of unique transactions that were successfully associated with\n",
        "        at least one recipient country and thus contribute to the density map.\n",
        "    unique_countries_count : int\n",
        "        The number of distinct recipient countries identified in the transaction\n",
        "        corpus. This is compared against the manuscript's reported 230 territories.\n",
        "    excluded_transaction_count : int\n",
        "        The number of transactions excluded from the map because they lacked\n",
        "        valid recipient country metadata.\n",
        "    \"\"\"\n",
        "    # The computed density table\n",
        "    density_table: pd.DataFrame\n",
        "\n",
        "    # Count of mapped transactions\n",
        "    total_transactions_mapped: int\n",
        "\n",
        "    # Count of unique countries found\n",
        "    unique_countries_count: int\n",
        "\n",
        "    # Count of excluded transactions\n",
        "    excluded_transaction_count: int\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the geographic density results.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of countries found and transaction coverage.\n",
        "        \"\"\"\n",
        "        # Format summary string\n",
        "        return (f\"Geo Density: {self.unique_countries_count} countries, \"\n",
        "                f\"{self.total_transactions_mapped} tx mapped, \"\n",
        "                f\"{self.excluded_transaction_count} tx excluded.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 1: Aggregate transaction counts by country\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def aggregate_country_counts(\n",
        "    df_tx_countries: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates transaction counts per recipient country.\n",
        "    Implements D(c) = sum(1[c(t) = c]).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_tx_countries : pd.DataFrame\n",
        "        Long-form transaction-country mapping (transaction_id, recipient_country_code).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Table with columns ['recipient_country_code', 'transaction_count'].\n",
        "    \"\"\"\n",
        "    # Group by country and count unique transactions\n",
        "    # Note: If a transaction is associated with the same country multiple times (unlikely if normalized),\n",
        "    # nunique ensures we count the transaction once per country.\n",
        "    # However, df_tx_countries should be unique on (tx_id, country).\n",
        "    density = (\n",
        "        df_tx_countries\n",
        "        .groupby('recipient_country_code')['transaction_id']\n",
        "        .nunique()\n",
        "        .reset_index(name='transaction_count')\n",
        "    )\n",
        "\n",
        "    # Sort descending for readability\n",
        "    density = density.sort_values('transaction_count', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    logger.info(f\"Geo Aggregation: Found {len(density)} unique countries.\")\n",
        "\n",
        "    return density\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 2: Apply log transform for visualization\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def apply_log_transform(\n",
        "    density_table: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies log transformation to transaction counts: D_log = log(1 + D).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    density_table : pd.DataFrame\n",
        "        Table with 'transaction_count'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Table with added 'log_transaction_count'.\n",
        "    \"\"\"\n",
        "    df_out = density_table.copy()\n",
        "\n",
        "    # log1p(x) = log(1 + x)\n",
        "    df_out['log_transaction_count'] = np.log1p(df_out['transaction_count'])\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 3: Validate coverage and document exclusions\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_geo_coverage(\n",
        "    df_transactions: pd.DataFrame,\n",
        "    df_tx_countries: pd.DataFrame,\n",
        "    density_table: pd.DataFrame\n",
        ") -> GeoDensityArtifact:\n",
        "    \"\"\"\n",
        "    Validates the geographic mapping coverage and packages the final artifact.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions : pd.DataFrame\n",
        "        Total cleansed transactions.\n",
        "    df_tx_countries : pd.DataFrame\n",
        "        Transactions with country contexts.\n",
        "    density_table : pd.DataFrame\n",
        "        Computed density table.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    GeoDensityArtifact\n",
        "        Final artifact with metadata.\n",
        "    \"\"\"\n",
        "    total_tx = len(df_transactions)\n",
        "    mapped_tx = df_tx_countries['transaction_id'].nunique()\n",
        "    excluded_tx = total_tx - mapped_tx\n",
        "    unique_countries = len(density_table)\n",
        "\n",
        "    artifact = GeoDensityArtifact(\n",
        "        density_table=density_table,\n",
        "        total_transactions_mapped=mapped_tx,\n",
        "        unique_countries_count=unique_countries,\n",
        "        excluded_transaction_count=excluded_tx\n",
        "    )\n",
        "\n",
        "    logger.info(str(artifact))\n",
        "\n",
        "    return artifact\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_geographic_density(\n",
        "    df_transactions: pd.DataFrame,\n",
        "    df_tx_countries: pd.DataFrame\n",
        ") -> GeoDensityArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 11. Computes geographic transaction density and prepares\n",
        "    data for visualization (Fig. 1).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions : pd.DataFrame\n",
        "        Cleansed transactions.\n",
        "    df_tx_countries : pd.DataFrame\n",
        "        Transaction-country mapping.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    GeoDensityArtifact\n",
        "        The density table and coverage metadata.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 11: Geographic Density Computation...\")\n",
        "\n",
        "    # Step 1: Aggregate\n",
        "    density_raw = aggregate_country_counts(df_tx_countries)\n",
        "\n",
        "    # Step 2: Transform\n",
        "    density_final = apply_log_transform(density_raw)\n",
        "\n",
        "    # Step 3: Validate\n",
        "    artifact = validate_geo_coverage(df_transactions, df_tx_countries, density_final)\n",
        "\n",
        "    logger.info(\"Task 11 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "-R-UtAWAQ0xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12 — Compute instrument evolution time series (Extended Data Fig. S1-type output)\n",
        "\n",
        "# =====================================================================================\n",
        "# Task 12: Compute instrument evolution time series (Extended Data Fig. S1-type output)\n",
        "# =====================================================================================\n",
        "\n",
        "@dataclass\n",
        "class InstrumentEvolutionArtifact:\n",
        "    \"\"\"\n",
        "    Container for instrument evolution time series and metadata.\n",
        "\n",
        "    This dataclass encapsulates the longitudinal analysis of financial instruments\n",
        "    used in global aid. It provides the aggregated time series data required to\n",
        "    visualize the shifting composition of aid (e.g., the rise of equity vs. grants)\n",
        "    over the study period. Additionally, it includes validation reports that quantify\n",
        "    data completeness, ensuring that any trends observed are not artifacts of\n",
        "    missing metadata.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    time_series_table : pd.DataFrame\n",
        "        A wide-form DataFrame indexed by `transaction_year`, with columns corresponding\n",
        "        to each instrument type (Grant, Loan, Equity). Values represent the count\n",
        "        of transactions for that instrument in that year.\n",
        "    validation_report : pd.DataFrame\n",
        "        A DataFrame indexed by `transaction_year` containing coverage statistics,\n",
        "        specifically the total number of transactions, the number mapped to a valid\n",
        "        instrument, and the share of missing data per year.\n",
        "    total_transactions_analyzed : int\n",
        "        The total number of transactions included in the analysis scope.\n",
        "    \"\"\"\n",
        "    # The aggregated time series data\n",
        "    time_series_table: pd.DataFrame\n",
        "\n",
        "    # The validation metadata per year\n",
        "    validation_report: pd.DataFrame\n",
        "\n",
        "    # Total count of transactions processed\n",
        "    total_transactions_analyzed: int\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the instrument evolution results.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the temporal coverage and volume.\n",
        "        \"\"\"\n",
        "        # Format summary string\n",
        "        return (f\"Instrument Evolution: {len(self.time_series_table)} years, \"\n",
        "                f\"{self.total_transactions_analyzed} tx analyzed.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 1: Aggregate transaction counts by year and instrument\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def aggregate_instrument_counts(\n",
        "    df_tx_instruments: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates transaction counts by year and instrument type.\n",
        "    Implements C(y, i) = sum(1[year=y, inst=i]).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_tx_instruments : pd.DataFrame\n",
        "        Transactions enriched with 'transaction_year' and 'instrument_type'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Wide-form DataFrame with index 'transaction_year' and columns for each instrument.\n",
        "    \"\"\"\n",
        "    # Group and count\n",
        "    # We use size() to count rows\n",
        "    counts = (\n",
        "        df_tx_instruments\n",
        "        .groupby(['transaction_year', 'instrument_type'])\n",
        "        .size()\n",
        "        .reset_index(name='count')\n",
        "    )\n",
        "\n",
        "    # Pivot to wide format\n",
        "    # Index: Year, Columns: Instrument Type, Values: Count\n",
        "    time_series = counts.pivot(\n",
        "        index='transaction_year',\n",
        "        columns='instrument_type',\n",
        "        values='count'\n",
        "    ).fillna(0).astype(int)\n",
        "\n",
        "    # Ensure all expected columns exist (Grant, Loan, Equity) even if 0\n",
        "    expected_cols = ['Grant', 'Loan', 'Equity']\n",
        "    for col in expected_cols:\n",
        "        if col not in time_series.columns:\n",
        "            time_series[col] = 0\n",
        "\n",
        "    # Sort columns deterministically\n",
        "    time_series = time_series[sorted(time_series.columns)]\n",
        "\n",
        "    # Sort index\n",
        "    time_series = time_series.sort_index()\n",
        "\n",
        "    logger.info(f\"Instrument Aggregation: {len(time_series)} years of data.\")\n",
        "\n",
        "    return time_series\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 2: Validate totals\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_instrument_totals(\n",
        "    df_tx_instruments: pd.DataFrame,\n",
        "    time_series_table: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validates that the sum of instrument counts matches the total transaction count\n",
        "    per year, accounting for missing instruments.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_tx_instruments : pd.DataFrame\n",
        "        Source transactions.\n",
        "    time_series_table : pd.DataFrame\n",
        "        Aggregated counts.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Validation report with columns ['total_tx', 'mapped_tx', 'missing_tx', 'missing_share'].\n",
        "    \"\"\"\n",
        "    # Calculate total transactions per year from source\n",
        "    total_per_year = df_tx_instruments.groupby('transaction_year').size()\n",
        "\n",
        "    # Calculate mapped transactions per year from time series\n",
        "    mapped_per_year = time_series_table.sum(axis=1)\n",
        "\n",
        "    # Align indices\n",
        "    validation = pd.DataFrame({\n",
        "        'total_tx': total_per_year,\n",
        "        'mapped_tx': mapped_per_year\n",
        "    }).fillna(0).astype(int)\n",
        "\n",
        "    # Calculate missing\n",
        "    validation['missing_tx'] = validation['total_tx'] - validation['mapped_tx']\n",
        "    validation['missing_share'] = validation['missing_tx'] / validation['total_tx']\n",
        "\n",
        "    # Log summary\n",
        "    avg_missing = validation['missing_share'].mean()\n",
        "    logger.info(f\"Instrument Validation: Average missing share per year = {avg_missing:.2%}\")\n",
        "\n",
        "    return validation\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 3: Persist time series artifact\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def package_instrument_artifact(\n",
        "    time_series_table: pd.DataFrame,\n",
        "    validation_report: pd.DataFrame,\n",
        "    total_tx: int\n",
        ") -> InstrumentEvolutionArtifact:\n",
        "    \"\"\"\n",
        "    Packages the instrument evolution results.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    time_series_table : pd.DataFrame\n",
        "        The evolution data.\n",
        "    validation_report : pd.DataFrame\n",
        "        The validation metadata.\n",
        "    total_tx : int\n",
        "        Total transactions processed.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    InstrumentEvolutionArtifact\n",
        "        Final artifact.\n",
        "    \"\"\"\n",
        "    artifact = InstrumentEvolutionArtifact(\n",
        "        time_series_table=time_series_table,\n",
        "        validation_report=validation_report,\n",
        "        total_transactions_analyzed=total_tx\n",
        "    )\n",
        "\n",
        "    logger.info(str(artifact))\n",
        "\n",
        "    return artifact\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_instrument_evolution(\n",
        "    df_tx_instruments: pd.DataFrame\n",
        ") -> InstrumentEvolutionArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 12. Computes the longitudinal evolution of aid instruments.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_tx_instruments : pd.DataFrame\n",
        "        Transactions enriched with instrument types.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    InstrumentEvolutionArtifact\n",
        "        The time series data and validation report.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 12: Instrument Evolution Computation...\")\n",
        "\n",
        "    # Step 1: Aggregate\n",
        "    time_series = aggregate_instrument_counts(df_tx_instruments)\n",
        "\n",
        "    # Step 2: Validate\n",
        "    validation = validate_instrument_totals(df_tx_instruments, time_series)\n",
        "\n",
        "    # Step 3: Package\n",
        "    total_tx = len(df_tx_instruments)\n",
        "    artifact = package_instrument_artifact(time_series, validation, total_tx)\n",
        "\n",
        "    logger.info(\"Task 12 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "xDw7Q1FbS3PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13 — Construct the bipartite Provider–Receiver transaction graph\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Construct the bipartite Provider–Receiver transaction graph\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class BipartiteGraphArtifact:\n",
        "    \"\"\"\n",
        "    Container for the bipartite graph structure and metadata.\n",
        "\n",
        "    This dataclass encapsulates the constructed bipartite network connecting\n",
        "    Provider organisations to Receiver organisations. It holds the sparse\n",
        "    incidence matrix representing the transaction flows, the node metadata\n",
        "    tables for both partitions, and the index mappings required to translate\n",
        "    between matrix indices and canonical node identifiers. This artifact is\n",
        "    the foundational data structure for subsequent centrality calculations\n",
        "    and network projections.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    provider_nodes : pd.DataFrame\n",
        "        A DataFrame containing metadata for all unique provider nodes in the\n",
        "        graph (partition U). Includes canonical IDs and role suffixes.\n",
        "    receiver_nodes : pd.DataFrame\n",
        "        A DataFrame containing metadata for all unique receiver nodes in the\n",
        "        graph (partition V). Includes canonical IDs and role suffixes.\n",
        "    incidence_matrix : sparse.csr_matrix\n",
        "        The sparse bi-adjacency matrix B of shape (|U|, |V|), where B_ij\n",
        "        represents the frequency of transactions between provider i and\n",
        "        receiver j.\n",
        "    provider_index_map : Dict[str, int]\n",
        "        A dictionary mapping provider node IDs (strings) to their row indices\n",
        "        (integers) in the incidence matrix.\n",
        "    receiver_index_map : Dict[str, int]\n",
        "        A dictionary mapping receiver node IDs (strings) to their column indices\n",
        "        (integers) in the incidence matrix.\n",
        "    edge_list : pd.DataFrame\n",
        "        A DataFrame representation of the non-zero entries in the incidence\n",
        "        matrix, useful for auditing and visualization.\n",
        "    \"\"\"\n",
        "    # Metadata for provider nodes (U)\n",
        "    provider_nodes: pd.DataFrame\n",
        "\n",
        "    # Metadata for receiver nodes (V)\n",
        "    receiver_nodes: pd.DataFrame\n",
        "\n",
        "    # Sparse incidence matrix B\n",
        "    incidence_matrix: sparse.csr_matrix\n",
        "\n",
        "    # Map from provider node ID to matrix row index\n",
        "    provider_index_map: Dict[str, int]\n",
        "\n",
        "    # Map from receiver node ID to matrix column index\n",
        "    receiver_index_map: Dict[str, int]\n",
        "\n",
        "    # DataFrame of edges (u, v, weight)\n",
        "    edge_list: pd.DataFrame\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the bipartite graph.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the graph dimensions (nodes and edges).\n",
        "        \"\"\"\n",
        "        # Extract dimensions from matrix shape\n",
        "        n_prov = self.incidence_matrix.shape[0]\n",
        "        n_recv = self.incidence_matrix.shape[1]\n",
        "        n_edges = self.incidence_matrix.nnz\n",
        "\n",
        "        return (f\"Bipartite Graph: {n_prov} Providers, {n_recv} Receivers, \"\n",
        "                f\"{n_edges} Edges.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 1: Define bipartite node sets with role-splitting\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_bipartite_nodes(\n",
        "    df_transactions: pd.DataFrame,\n",
        "    canonical_mapping: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Maps transaction endpoints to canonical IDs and generates role-split node sets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions : pd.DataFrame\n",
        "        Cleansed transactions.\n",
        "    canonical_mapping : pd.DataFrame\n",
        "        Mapping table with 'org_ref' and 'canonical_org_id'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n",
        "        1. df_tx_mapped: Transactions with canonical provider/receiver IDs.\n",
        "        2. provider_nodes: Unique provider nodes with role suffix.\n",
        "        3. receiver_nodes: Unique receiver nodes with role suffix.\n",
        "    \"\"\"\n",
        "    # Create a mapping dictionary for fast lookup\n",
        "    # We map org_ref -> canonical_org_id\n",
        "    # Note: If mapping table has duplicates (one ref to multiple canonicals - shouldn't happen), drop dupes\n",
        "    mapping_dict = (\n",
        "        canonical_mapping\n",
        "        .drop_duplicates('org_ref')\n",
        "        .set_index('org_ref')['canonical_org_id']\n",
        "        .to_dict()\n",
        "    )\n",
        "\n",
        "    df_tx = df_transactions.copy()\n",
        "\n",
        "    # Map Provider\n",
        "    # Prefer Ref, fallback to Name if Ref missing (though Task 5 ensured validity)\n",
        "    # Ideally we mapped names too in Task 10.\n",
        "    # For baseline, we assume Task 10 produced a mapping covering all refs found in transactions.\n",
        "    # If a ref is missing from mapping, we use the ref itself as canonical (fallback).\n",
        "\n",
        "    def get_canonical(ref: Any, name: Any) -> str:\n",
        "        ref_str = str(ref).strip()\n",
        "        if ref_str in mapping_dict:\n",
        "            return mapping_dict[ref_str]\n",
        "        # Fallback: if ref is valid, use it. Else use name.\n",
        "        if ref_str and ref_str.lower() != 'nan':\n",
        "            return ref_str\n",
        "        return str(name).strip()\n",
        "\n",
        "    # Apply mapping (vectorized map is faster if possible, but fallback logic requires apply or coalesce)\n",
        "    # Let's use map on ref first, then fillna with ref, then fillna with name\n",
        "\n",
        "    # Provider\n",
        "    df_tx['provider_canonical_id'] = df_tx['transaction_provider_org_ref'].map(mapping_dict)\n",
        "    df_tx['provider_canonical_id'] = df_tx['provider_canonical_id'].fillna(df_tx['transaction_provider_org_ref'])\n",
        "    df_tx['provider_canonical_id'] = df_tx['provider_canonical_id'].fillna(df_tx['transaction_provider_org_name'])\n",
        "\n",
        "    # Receiver\n",
        "    df_tx['receiver_canonical_id'] = df_tx['transaction_receiver_org_ref'].map(mapping_dict)\n",
        "    df_tx['receiver_canonical_id'] = df_tx['receiver_canonical_id'].fillna(df_tx['transaction_receiver_org_ref'])\n",
        "    df_tx['receiver_canonical_id'] = df_tx['receiver_canonical_id'].fillna(df_tx['transaction_receiver_org_name'])\n",
        "\n",
        "    # Role Splitting\n",
        "    df_tx['provider_node_id'] = df_tx['provider_canonical_id'].astype(str) + \"::PROVIDER\"\n",
        "    df_tx['receiver_node_id'] = df_tx['receiver_canonical_id'].astype(str) + \"::RECEIVER\"\n",
        "\n",
        "    # Extract unique nodes\n",
        "    provider_nodes = pd.DataFrame({\n",
        "        'node_id': df_tx['provider_node_id'].unique(),\n",
        "        'canonical_id': df_tx['provider_canonical_id'].unique(),\n",
        "        'role': 'PROVIDER'\n",
        "    })\n",
        "\n",
        "    receiver_nodes = pd.DataFrame({\n",
        "        'node_id': df_tx['receiver_node_id'].unique(),\n",
        "        'canonical_id': df_tx['receiver_canonical_id'].unique(),\n",
        "        'role': 'RECEIVER'\n",
        "    })\n",
        "\n",
        "    logger.info(f\"Bipartite Nodes: {len(provider_nodes)} Providers, {len(receiver_nodes)} Receivers.\")\n",
        "\n",
        "    return df_tx, provider_nodes, receiver_nodes\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 2: Define bipartite incidence matrix\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_incidence_matrix(\n",
        "    df_tx_mapped: pd.DataFrame,\n",
        "    provider_nodes: pd.DataFrame,\n",
        "    receiver_nodes: pd.DataFrame\n",
        ") -> Tuple[sparse.csr_matrix, Dict[str, int], Dict[str, int], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Constructs the frequency-weighted bipartite incidence matrix B.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_tx_mapped : pd.DataFrame\n",
        "        Transactions with role-split node IDs.\n",
        "    provider_nodes : pd.DataFrame\n",
        "        Unique provider nodes.\n",
        "    receiver_nodes : pd.DataFrame\n",
        "        Unique receiver nodes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[sparse.csr_matrix, Dict[str, int], Dict[str, int], pd.DataFrame]\n",
        "        1. Sparse matrix B (rows=providers, cols=receivers).\n",
        "        2. Provider index map (node_id -> row_idx).\n",
        "        3. Receiver index map (node_id -> col_idx).\n",
        "        4. Edge list DataFrame.\n",
        "    \"\"\"\n",
        "    # Create Index Maps\n",
        "    # Sort for determinism\n",
        "    prov_ids = sorted(provider_nodes['node_id'].tolist())\n",
        "    recv_ids = sorted(receiver_nodes['node_id'].tolist())\n",
        "\n",
        "    prov_map = {nid: i for i, nid in enumerate(prov_ids)}\n",
        "    recv_map = {nid: i for i, nid in enumerate(recv_ids)}\n",
        "\n",
        "    # Aggregate edges (frequency weight)\n",
        "    edges = (\n",
        "        df_tx_mapped\n",
        "        .groupby(['provider_node_id', 'receiver_node_id'])\n",
        "        .size()\n",
        "        .reset_index(name='weight')\n",
        "    )\n",
        "\n",
        "    # Map to indices\n",
        "    row_indices = edges['provider_node_id'].map(prov_map).values\n",
        "    col_indices = edges['receiver_node_id'].map(recv_map).values\n",
        "    data = edges['weight'].values\n",
        "\n",
        "    # Build Matrix\n",
        "    shape = (len(prov_ids), len(recv_ids))\n",
        "    B = sparse.coo_matrix((data, (row_indices, col_indices)), shape=shape).tocsr()\n",
        "\n",
        "    logger.info(f\"Incidence Matrix: Shape {shape}, NNZ {B.nnz}.\")\n",
        "\n",
        "    return B, prov_map, recv_map, edges\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 3: Persist bipartite graph artifacts\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def package_bipartite_artifact(\n",
        "    provider_nodes: pd.DataFrame,\n",
        "    receiver_nodes: pd.DataFrame,\n",
        "    B: sparse.csr_matrix,\n",
        "    prov_map: Dict[str, int],\n",
        "    recv_map: Dict[str, int],\n",
        "    edge_list: pd.DataFrame\n",
        ") -> BipartiteGraphArtifact:\n",
        "    \"\"\"\n",
        "    Packages the bipartite graph components.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    provider_nodes : pd.DataFrame\n",
        "        Provider node table.\n",
        "    receiver_nodes : pd.DataFrame\n",
        "        Receiver node table.\n",
        "    B : sparse.csr_matrix\n",
        "        Incidence matrix.\n",
        "    prov_map : Dict[str, int]\n",
        "        Provider index map.\n",
        "    recv_map : Dict[str, int]\n",
        "        Receiver index map.\n",
        "    edge_list : pd.DataFrame\n",
        "        Edge list.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    BipartiteGraphArtifact\n",
        "        Final artifact.\n",
        "    \"\"\"\n",
        "    artifact = BipartiteGraphArtifact(\n",
        "        provider_nodes=provider_nodes,\n",
        "        receiver_nodes=receiver_nodes,\n",
        "        incidence_matrix=B,\n",
        "        provider_index_map=prov_map,\n",
        "        receiver_index_map=recv_map,\n",
        "        edge_list=edge_list\n",
        "    )\n",
        "\n",
        "    logger.info(str(artifact))\n",
        "\n",
        "    return artifact\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_bipartite_graph(\n",
        "    df_transactions: pd.DataFrame,\n",
        "    canonical_mapping: pd.DataFrame\n",
        ") -> BipartiteGraphArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 13. Constructs the bipartite provider-receiver graph.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions : pd.DataFrame\n",
        "        Cleansed transactions.\n",
        "    canonical_mapping : pd.DataFrame\n",
        "        Canonical organisation mapping.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    BipartiteGraphArtifact\n",
        "        The constructed graph.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 13: Bipartite Graph Construction...\")\n",
        "\n",
        "    # Step 1: Prepare Nodes\n",
        "    df_mapped, prov_nodes, recv_nodes = prepare_bipartite_nodes(df_transactions, canonical_mapping)\n",
        "\n",
        "    # Step 2: Build Matrix\n",
        "    B, prov_map, recv_map, edges = build_incidence_matrix(df_mapped, prov_nodes, recv_nodes)\n",
        "\n",
        "    # Step 3: Package\n",
        "    artifact = package_bipartite_artifact(prov_nodes, recv_nodes, B, prov_map, recv_map, edges)\n",
        "\n",
        "    logger.info(\"Task 13 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "12Bg0tTqUn_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14 — Compute transaction counts per organisation (node size metric)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Compute transaction counts per organisation (node size metric)\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class NodeSizeArtifact:\n",
        "    \"\"\"\n",
        "    Container for organisation transaction counts (node sizes).\n",
        "\n",
        "    This dataclass encapsulates the results of the node sizing computation,\n",
        "    which is based on the total number of transactions (\"deal count\") associated\n",
        "    with each organisation. This metric serves as the primary sizing variable\n",
        "    for the \"Solar System\" visualization, contrasting with traditional volume-based\n",
        "    metrics (USD). The artifact includes both the raw counts per organisation\n",
        "    and aggregated validation statistics to verify alignment with the manuscript's\n",
        "    reported distributions.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    deal_counts : pd.DataFrame\n",
        "        A DataFrame indexed by `canonical_org_id` containing the transaction counts\n",
        "        for each organisation. Columns include `provider_tx_count`, `receiver_tx_count`,\n",
        "        and the aggregate `total_tx_count`.\n",
        "    validation_stats : pd.DataFrame\n",
        "        A summary DataFrame containing descriptive statistics (mean, median, count)\n",
        "        of deal counts, stratified by organisation type (e.g., Academic, Foundation).\n",
        "        Used to validate findings such as \"Universities are 85% smaller by deal count\".\n",
        "    \"\"\"\n",
        "    # The per-organisation deal counts\n",
        "    deal_counts: pd.DataFrame\n",
        "\n",
        "    # The validation statistics by org type\n",
        "    validation_stats: pd.DataFrame\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the node size artifact.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary indicating the number of organisations sized.\n",
        "        \"\"\"\n",
        "        # Format summary string\n",
        "        return f\"Node Sizes: Computed for {len(self.deal_counts)} organisations.\"\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 1: Define deal count metric\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def calculate_deal_counts(\n",
        "    df_tx_mapped: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the total number of transactions each organisation participates in,\n",
        "    either as a provider or a receiver.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_tx_mapped : pd.DataFrame\n",
        "        Transactions with 'provider_canonical_id' and 'receiver_canonical_id'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Table indexed by 'canonical_org_id' with columns:\n",
        "        ['provider_tx_count', 'receiver_tx_count', 'total_tx_count'].\n",
        "    \"\"\"\n",
        "    # Count as provider\n",
        "    prov_counts = df_tx_mapped['provider_canonical_id'].value_counts()\n",
        "\n",
        "    # Count as receiver\n",
        "    recv_counts = df_tx_mapped['receiver_canonical_id'].value_counts()\n",
        "\n",
        "    # Align and sum\n",
        "    # We use a DataFrame to handle the outer join logic of aligning indices\n",
        "    counts = pd.DataFrame({\n",
        "        'provider_tx_count': prov_counts,\n",
        "        'receiver_tx_count': recv_counts\n",
        "    }).fillna(0).astype(int)\n",
        "\n",
        "    # Total deals\n",
        "    counts['total_tx_count'] = counts['provider_tx_count'] + counts['receiver_tx_count']\n",
        "\n",
        "    # Name the index\n",
        "    counts.index.name = 'canonical_org_id'\n",
        "\n",
        "    logger.info(f\"Deal Counts: Computed for {len(counts)} unique entities.\")\n",
        "\n",
        "    return counts\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 2: Validate against manuscript statistics\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_deal_statistics(\n",
        "    deal_counts: pd.DataFrame,\n",
        "    df_organisations: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes summary statistics for deal counts, stratified by organisation type,\n",
        "    to compare against manuscript claims.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    deal_counts : pd.DataFrame\n",
        "        The computed deal counts.\n",
        "    df_organisations : pd.DataFrame\n",
        "        Organisation master table with 'org_type' and 'canonical_org_id'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Summary statistics per org_type.\n",
        "    \"\"\"\n",
        "    # Join type info\n",
        "    # We need to map canonical_org_id to org_type.\n",
        "    # If canonical_org_id is in df_organisations, we use that.\n",
        "    # Note: df_organisations might have multiple rows per canonical ID if multiple raw refs mapped to one.\n",
        "    # We need a unique mapping. We take the first type found (assuming consistency within canonical cluster).\n",
        "    type_map = (\n",
        "        df_organisations\n",
        "        .dropna(subset=['canonical_org_id', 'org_type'])\n",
        "        .drop_duplicates('canonical_org_id')\n",
        "        .set_index('canonical_org_id')['org_type']\n",
        "    )\n",
        "\n",
        "    # Enrich counts\n",
        "    enriched = deal_counts.copy()\n",
        "    enriched['org_type'] = enriched.index.map(type_map).fillna('Unknown')\n",
        "\n",
        "    # Compute stats\n",
        "    stats = (\n",
        "        enriched\n",
        "        .groupby('org_type')['total_tx_count']\n",
        "        .agg(['count', 'mean', 'median', 'sum'])\n",
        "        .sort_values('mean', ascending=False)\n",
        "    )\n",
        "\n",
        "    # Add global row\n",
        "    global_stats = pd.DataFrame({\n",
        "        'count': [len(enriched)],\n",
        "        'mean': [enriched['total_tx_count'].mean()],\n",
        "        'median': [enriched['total_tx_count'].median()],\n",
        "        'sum': [enriched['total_tx_count'].sum()]\n",
        "    }, index=['Global'])\n",
        "\n",
        "    final_stats = pd.concat([stats, global_stats])\n",
        "\n",
        "    logger.info(\"Deal Stats Validation:\\n\" + str(final_stats[['count', 'mean', 'median']]))\n",
        "\n",
        "    return final_stats\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 3: Persist deal count table\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def package_node_size_artifact(\n",
        "    deal_counts: pd.DataFrame,\n",
        "    validation_stats: pd.DataFrame\n",
        ") -> NodeSizeArtifact:\n",
        "    \"\"\"\n",
        "    Packages the node size results.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    deal_counts : pd.DataFrame\n",
        "        The counts.\n",
        "    validation_stats : pd.DataFrame\n",
        "        The stats.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    NodeSizeArtifact\n",
        "        Final artifact.\n",
        "    \"\"\"\n",
        "    artifact = NodeSizeArtifact(\n",
        "        deal_counts=deal_counts,\n",
        "        validation_stats=validation_stats\n",
        "    )\n",
        "\n",
        "    logger.info(str(artifact))\n",
        "\n",
        "    return artifact\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_node_sizes(\n",
        "    df_tx_mapped: pd.DataFrame,\n",
        "    df_organisations: pd.DataFrame\n",
        ") -> NodeSizeArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 14. Computes transaction counts for node sizing.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_tx_mapped : pd.DataFrame\n",
        "        Transactions with canonical IDs.\n",
        "    df_organisations : pd.DataFrame\n",
        "        Organisation master table.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    NodeSizeArtifact\n",
        "        The deal counts and validation stats.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 14: Node Size Computation...\")\n",
        "\n",
        "    # Step 1: Calculate\n",
        "    counts = calculate_deal_counts(df_tx_mapped)\n",
        "\n",
        "    # Step 2: Validate\n",
        "    stats = validate_deal_statistics(counts, df_organisations)\n",
        "\n",
        "    # Step 3: Package\n",
        "    artifact = package_node_size_artifact(counts, stats)\n",
        "\n",
        "    logger.info(\"Task 14 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "j3aTJQOoW3vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15 — Construct Provider–Provider co-occurrence projection\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Construct Provider–Provider co-occurrence projection\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ProjectionArtifact:\n",
        "    \"\"\"\n",
        "    Container for the projected provider-provider graph.\n",
        "\n",
        "    This dataclass encapsulates the one-mode projection of the bipartite network\n",
        "    onto the set of Provider organisations. It contains the symmetric adjacency\n",
        "    matrix P, where P_ij represents the strength of the relationship between\n",
        "    provider i and provider j based on their shared contexts (co-funding or\n",
        "    co-implementation in the same country or sector). This artifact is the\n",
        "    primary input for community detection and topological analysis of the\n",
        "    donor ecosystem.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    adjacency_matrix : sparse.csr_matrix\n",
        "        The symmetric, weighted adjacency matrix of the provider network.\n",
        "        Weights correspond to the frequency of co-occurrence.\n",
        "    provider_index_map : Dict[str, int]\n",
        "        A dictionary mapping canonical provider IDs (strings) to their row/column\n",
        "        indices (integers) in the adjacency matrix.\n",
        "    context_index_map : Dict[str, int]\n",
        "        A dictionary mapping context IDs (strings, e.g., \"COUNTRY:US\") to their\n",
        "        column indices in the intermediate incidence matrix M. Included for\n",
        "        provenance and potential re-projection.\n",
        "    edge_list : pd.DataFrame\n",
        "        A DataFrame representation of the upper triangle of the adjacency matrix,\n",
        "        listing unique edges (source, target, weight). Useful for visualization\n",
        "        and export.\n",
        "    \"\"\"\n",
        "    # The projected adjacency matrix P\n",
        "    adjacency_matrix: sparse.csr_matrix\n",
        "\n",
        "    # Map from provider ID to matrix index\n",
        "    provider_index_map: Dict[str, int]\n",
        "\n",
        "    # Map from context ID to incidence matrix index\n",
        "    context_index_map: Dict[str, int]\n",
        "\n",
        "    # DataFrame of edges\n",
        "    edge_list: pd.DataFrame\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the projection graph.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the network size (nodes and edges).\n",
        "        \"\"\"\n",
        "        # Extract dimensions\n",
        "        n_nodes = self.adjacency_matrix.shape[0]\n",
        "        # Count edges (undirected, so divide non-zeros by 2)\n",
        "        n_edges = self.adjacency_matrix.nnz // 2\n",
        "\n",
        "        return f\"Projection: {n_nodes} Providers, {n_edges} Co-occurrence Edges.\"\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 1: Build provider–context incidence matrix\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_context_incidence(\n",
        "    df_contexts: pd.DataFrame,\n",
        "    df_tx_mapped: pd.DataFrame\n",
        ") -> Tuple[sparse.csr_matrix, Dict[str, int], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Constructs the provider-context incidence matrix M.\n",
        "    M_ix = frequency of provider i appearing in context x.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_contexts : pd.DataFrame\n",
        "        Long-form contexts (transaction_id, context_id).\n",
        "    df_tx_mapped : pd.DataFrame\n",
        "        Transactions with 'provider_canonical_id'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[sparse.csr_matrix, Dict[str, int], Dict[str, int]]\n",
        "        1. Sparse matrix M.\n",
        "        2. Provider index map.\n",
        "        3. Context index map.\n",
        "    \"\"\"\n",
        "    # Join to get provider for each context instance\n",
        "    # We need transaction_id -> provider_canonical_id\n",
        "    tx_provider_map = df_tx_mapped[['transaction_id', 'provider_canonical_id']]\n",
        "\n",
        "    # Inner join: only transactions with both context and valid provider\n",
        "    df_incidence = pd.merge(\n",
        "        df_contexts,\n",
        "        tx_provider_map,\n",
        "        on='transaction_id',\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    # Aggregate frequency\n",
        "    # Count how many times provider P appears in context C\n",
        "    counts = (\n",
        "        df_incidence\n",
        "        .groupby(['provider_canonical_id', 'context_id'])\n",
        "        .size()\n",
        "        .reset_index(name='weight')\n",
        "    )\n",
        "\n",
        "    # Create Index Maps\n",
        "    prov_ids = sorted(counts['provider_canonical_id'].unique())\n",
        "    ctx_ids = sorted(counts['context_id'].unique())\n",
        "\n",
        "    prov_map = {pid: i for i, pid in enumerate(prov_ids)}\n",
        "    ctx_map = {cid: i for i, cid in enumerate(ctx_ids)}\n",
        "\n",
        "    # Map to indices\n",
        "    row_indices = counts['provider_canonical_id'].map(prov_map).values\n",
        "    col_indices = counts['context_id'].map(ctx_map).values\n",
        "    data = counts['weight'].values\n",
        "\n",
        "    # Build Matrix M\n",
        "    shape = (len(prov_ids), len(ctx_ids))\n",
        "    M = sparse.coo_matrix((data, (row_indices, col_indices)), shape=shape).tocsr()\n",
        "\n",
        "    logger.info(f\"Context Incidence: {len(prov_ids)} Providers x {len(ctx_ids)} Contexts.\")\n",
        "\n",
        "    return M, prov_map, ctx_map\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 2: Project to provider–provider adjacency\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def project_to_adjacency(\n",
        "    M: sparse.csr_matrix\n",
        ") -> sparse.csr_matrix:\n",
        "    \"\"\"\n",
        "    Computes the one-mode projection P = M * M.T and removes self-loops.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    M : sparse.csr_matrix\n",
        "        Provider-context incidence matrix.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sparse.csr_matrix\n",
        "        Symmetric provider-provider adjacency matrix P.\n",
        "    \"\"\"\n",
        "    # Matrix multiplication\n",
        "    # P_ij = sum_k M_ik * M_jk (dot product of context vectors)\n",
        "    P = M.dot(M.T)\n",
        "\n",
        "    # Remove self-loops (diagonal)\n",
        "    # setdiag works on lil or csr (efficiently on lil, but csr is okay for batch)\n",
        "    P.setdiag(0)\n",
        "\n",
        "    # Eliminate explicit zeros created by setdiag\n",
        "    P.eliminate_zeros()\n",
        "\n",
        "    logger.info(f\"Projection: Generated adjacency with {P.nnz} non-zero entries.\")\n",
        "\n",
        "    return P\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 3: Persist projected graph\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def package_projection_artifact(\n",
        "    P: sparse.csr_matrix,\n",
        "    prov_map: Dict[str, int],\n",
        "    ctx_map: Dict[str, int]\n",
        ") -> ProjectionArtifact:\n",
        "    \"\"\"\n",
        "    Packages the projection graph.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    P : sparse.csr_matrix\n",
        "        Adjacency matrix.\n",
        "    prov_map : Dict[str, int]\n",
        "        Provider index map.\n",
        "    ctx_map : Dict[str, int]\n",
        "        Context index map.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ProjectionArtifact\n",
        "        Final artifact.\n",
        "    \"\"\"\n",
        "    # Extract edge list for persistence/audit\n",
        "    # Use upper triangle to avoid duplicates in undirected graph\n",
        "    P_triu = sparse.triu(P, format='coo')\n",
        "\n",
        "    # Reverse map for IDs\n",
        "    idx_to_prov = {v: k for k, v in prov_map.items()}\n",
        "\n",
        "    rows = [idx_to_prov[i] for i in P_triu.row]\n",
        "    cols = [idx_to_prov[j] for j in P_triu.col]\n",
        "    weights = P_triu.data\n",
        "\n",
        "    edge_list = pd.DataFrame({\n",
        "        'source_canonical_id': rows,\n",
        "        'target_canonical_id': cols,\n",
        "        'weight': weights\n",
        "    })\n",
        "\n",
        "    artifact = ProjectionArtifact(\n",
        "        adjacency_matrix=P,\n",
        "        provider_index_map=prov_map,\n",
        "        context_index_map=ctx_map,\n",
        "        edge_list=edge_list\n",
        "    )\n",
        "\n",
        "    logger.info(str(artifact))\n",
        "\n",
        "    return artifact\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_co_occurrence_projection(\n",
        "    df_contexts: pd.DataFrame,\n",
        "    df_tx_mapped: pd.DataFrame\n",
        ") -> ProjectionArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 15. Constructs the provider-provider co-occurrence network.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_contexts : pd.DataFrame\n",
        "        Unified contexts.\n",
        "    df_tx_mapped : pd.DataFrame\n",
        "        Transactions with canonical IDs.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ProjectionArtifact\n",
        "        The projected graph.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 15: Co-occurrence Projection...\")\n",
        "\n",
        "    # Step 1: Incidence\n",
        "    M, prov_map, ctx_map = build_context_incidence(df_contexts, df_tx_mapped)\n",
        "\n",
        "    # Step 2: Projection\n",
        "    P = project_to_adjacency(M)\n",
        "\n",
        "    # Step 3: Package\n",
        "    artifact = package_projection_artifact(P, prov_map, ctx_map)\n",
        "\n",
        "    logger.info(\"Task 15 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "Fir4wEF1Y0Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16 — Learn node embeddings with node2vec\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Learn node embeddings with node2vec\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class EmbeddingArtifact:\n",
        "    \"\"\"\n",
        "    Container for node embeddings and training metadata.\n",
        "\n",
        "    This dataclass encapsulates the output of the representation learning phase.\n",
        "    It holds the dense vector representations (embeddings) for all nodes in the\n",
        "    network, indexed by their canonical identifiers. These embeddings capture\n",
        "    the structural equivalence and community membership of organisations, serving\n",
        "    as the input for the UMAP dimensionality reduction. The artifact also\n",
        "    preserves the hyperparameters used during the random walk generation and\n",
        "    Skip-gram training to ensure reproducibility.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    embeddings : pd.DataFrame\n",
        "        A DataFrame where the index corresponds to canonical node IDs (strings)\n",
        "        and columns represent the embedding dimensions (e.g., dim_0, ..., dim_99).\n",
        "    hyperparameters : Dict[str, Any]\n",
        "        A dictionary recording the configuration used for Node2Vec, including\n",
        "        p, q, walk_length, num_walks, window_size, and random_seed.\n",
        "    \"\"\"\n",
        "    embeddings: pd.DataFrame\n",
        "    hyperparameters: Dict[str, Any]\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return (f\"Embeddings: {len(self.embeddings)} nodes, \"\n",
        "                f\"{self.embeddings.shape[1]} dimensions.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 1: Specify graph input and parameters\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_symmetric_adjacency(\n",
        "    bipartite_artifact: Any # BipartiteGraphArtifact\n",
        ") -> Tuple[sparse.csr_matrix, Dict[int, str]]:\n",
        "    \"\"\"\n",
        "    Constructs the symmetric adjacency matrix A for the bipartite graph and a unified index map.\n",
        "\n",
        "    A = [[0, B], [B.T, 0]] where B is the incidence matrix (Providers x Receivers).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bipartite_artifact : BipartiteGraphArtifact\n",
        "        The constructed bipartite graph containing incidence matrix B and separate index maps.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[sparse.csr_matrix, Dict[int, str]]\n",
        "        1. The symmetric adjacency matrix A in CSR format.\n",
        "        2. A unified mapping from integer index (0 to |U|+|V|-1) to canonical node ID.\n",
        "    \"\"\"\n",
        "    B = bipartite_artifact.incidence_matrix\n",
        "    n_u, n_v = B.shape\n",
        "\n",
        "    # Construct symmetric adjacency A\n",
        "    # Row indices: 0..n_u-1 (Providers), n_u..n_u+n_v-1 (Receivers)\n",
        "    # We use bmat to create the block matrix efficiently\n",
        "    A = sparse.bmat([[None, B], [B.T, None]], format='csr')\n",
        "\n",
        "    # Create unified index map\n",
        "    # 0..n_u-1 -> Provider IDs\n",
        "    # n_u..end -> Receiver IDs\n",
        "    idx_to_id = {}\n",
        "\n",
        "    # Reverse provider map (0 to n_u-1)\n",
        "    # provider_index_map maps ID -> Index\n",
        "    for pid, idx in bipartite_artifact.provider_index_map.items():\n",
        "        idx_to_id[idx] = pid\n",
        "\n",
        "    # Reverse receiver map (offset by n_u)\n",
        "    for rid, idx in bipartite_artifact.receiver_index_map.items():\n",
        "        idx_to_id[idx + n_u] = rid\n",
        "\n",
        "    logger.info(f\"Graph Preparation: Constructed symmetric adjacency A with shape {A.shape} and {A.nnz} edges.\")\n",
        "\n",
        "    return A, idx_to_id\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 2: Define biased random walk model and generate walks (Numba Optimized)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "@njit\n",
        "def numba_random_walk(\n",
        "    indptr: np.ndarray,\n",
        "    indices: np.ndarray,\n",
        "    data: np.ndarray,\n",
        "    start_nodes: np.ndarray,\n",
        "    walk_length: int,\n",
        "    p: float,\n",
        "    q: float,\n",
        "    seed: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    JIT-compiled function to generate biased random walks for Node2Vec.\n",
        "\n",
        "    This implementation handles the 2nd-order Markov chain logic efficiently.\n",
        "    For a bipartite graph, the distance d(t, x) from previous node t to next node x\n",
        "    is always 2 (unless returning to t, where d=0).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    indptr : np.ndarray\n",
        "        CSR index pointer array.\n",
        "    indices : np.ndarray\n",
        "        CSR column indices array.\n",
        "    data : np.ndarray\n",
        "        CSR data array (edge weights).\n",
        "    start_nodes : np.ndarray\n",
        "        Array of starting node indices for this batch of walks.\n",
        "    walk_length : int\n",
        "        Length of each walk.\n",
        "    p : float\n",
        "        Return parameter (1/p probability to return).\n",
        "    q : float\n",
        "        In-out parameter (1/q probability to move to distance 2).\n",
        "    seed : int\n",
        "        Random seed for reproducibility.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        2D array of walks (num_walks x walk_length).\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    n_walks = len(start_nodes)\n",
        "    walks = np.empty((n_walks, walk_length), dtype=np.int32)\n",
        "\n",
        "    for i in range(n_walks):\n",
        "        curr = start_nodes[i]\n",
        "        walks[i, 0] = curr\n",
        "        prev = -1\n",
        "\n",
        "        for step in range(1, walk_length):\n",
        "            # Get neighbors of curr\n",
        "            start_idx = indptr[curr]\n",
        "            end_idx = indptr[curr + 1]\n",
        "\n",
        "            if start_idx == end_idx:\n",
        "                # Isolated node or dead end, fill rest with current\n",
        "                walks[i, step:] = curr\n",
        "                break\n",
        "\n",
        "            nbrs = indices[start_idx:end_idx]\n",
        "            weights = data[start_idx:end_idx].astype(np.float64) # Copy to float for manipulation\n",
        "\n",
        "            # Calculate unnormalized transition probabilities\n",
        "            if prev == -1:\n",
        "                # First step: proportional to edge weight only\n",
        "                pass # weights are already edge weights\n",
        "            else:\n",
        "                # 2nd order bias\n",
        "                # In bipartite:\n",
        "                # - Neighbor is 'prev' (dist=0) -> weight * 1/p\n",
        "                # - Neighbor is not 'prev' (dist=2) -> weight * 1/q\n",
        "                # (dist=1 impossible in bipartite)\n",
        "\n",
        "                for j in range(len(nbrs)):\n",
        "                    nbr = nbrs[j]\n",
        "                    if nbr == prev:\n",
        "                        weights[j] *= (1.0 / p)\n",
        "                    else:\n",
        "                        weights[j] *= (1.0 / q)\n",
        "\n",
        "            # Sampling\n",
        "            # Compute CDF for weighted sampling\n",
        "            cdf = np.cumsum(weights)\n",
        "            cdf /= cdf[-1]\n",
        "\n",
        "            r = np.random.random()\n",
        "            # Find index where cdf >= r\n",
        "            # Manual search is fast for small degrees\n",
        "            next_idx = 0\n",
        "            for j in range(len(cdf)):\n",
        "                if r <= cdf[j]:\n",
        "                    next_idx = j\n",
        "                    break\n",
        "\n",
        "            next_node = nbrs[next_idx]\n",
        "            walks[i, step] = next_node\n",
        "\n",
        "            prev = curr\n",
        "            curr = next_node\n",
        "\n",
        "    return walks\n",
        "\n",
        "def generate_walks_production(\n",
        "    A: sparse.csr_matrix,\n",
        "    num_walks: int,\n",
        "    walk_length: int,\n",
        "    p: float,\n",
        "    q: float,\n",
        "    seed: int\n",
        ") -> List[List[int]]:\n",
        "    \"\"\"\n",
        "    Wrapper for Numba-optimized walk generation. Handles batching and type conversion.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    A : sparse.csr_matrix\n",
        "        Symmetric adjacency matrix.\n",
        "    num_walks : int\n",
        "        Walks per node.\n",
        "    walk_length : int\n",
        "        Length of walk.\n",
        "    p : float\n",
        "        Return parameter.\n",
        "    q : float\n",
        "        In-out parameter.\n",
        "    seed : int\n",
        "        Random seed.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[List[int]]\n",
        "        List of walks (as lists of integers).\n",
        "    \"\"\"\n",
        "    # Ensure CSR format and sorted indices for deterministic behavior\n",
        "    A_sorted = A.sorted_indices()\n",
        "    indptr = A_sorted.indptr\n",
        "    indices = A_sorted.indices\n",
        "    data = A_sorted.data\n",
        "\n",
        "    n_nodes = A.shape[0]\n",
        "    all_nodes = np.arange(n_nodes, dtype=np.int32)\n",
        "\n",
        "    # Repeat nodes for num_walks\n",
        "    # We shuffle the order of walk generation to avoid artifacts,\n",
        "    # but we seed the shuffle for reproducibility.\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # We generate walks in one go or batches. For 2500 nodes * 10 walks = 25k walks.\n",
        "    # This fits easily in memory.\n",
        "    start_nodes = np.repeat(all_nodes, num_walks)\n",
        "    rng.shuffle(start_nodes)\n",
        "\n",
        "    # Call Numba function\n",
        "    # Note: Numba requires typed arrays\n",
        "    raw_walks = numba_random_walk(\n",
        "        indptr, indices, data,\n",
        "        start_nodes, walk_length,\n",
        "        p, q, seed\n",
        "    )\n",
        "\n",
        "    # Convert to list of lists\n",
        "    walks_list = raw_walks.tolist()\n",
        "\n",
        "    logger.info(f\"Random Walks: Generated {len(walks_list)} walks using Numba optimization.\")\n",
        "    return walks_list\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 3: Train skip-gram embedding model\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def train_skipgram_model(\n",
        "    walks: List[List[int]],\n",
        "    idx_to_id: Dict[int, str],\n",
        "    dimensions: int,\n",
        "    window_size: int,\n",
        "    seed: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Trains the Skip-gram model (Word2Vec) on the generated walks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    walks : List[List[int]]\n",
        "        The random walks (integer indices).\n",
        "    idx_to_id : Dict[int, str]\n",
        "        Mapping from integer index to canonical node ID.\n",
        "    dimensions : int\n",
        "        Embedding vector size.\n",
        "    window_size : int\n",
        "        Context window size.\n",
        "    seed : int\n",
        "        Random seed.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Embeddings indexed by canonical node ID.\n",
        "    \"\"\"\n",
        "    # Convert integer walks to canonical string IDs\n",
        "    # This ensures the model learns vectors for the actual entity IDs\n",
        "    # and handles the mapping transparently.\n",
        "    str_walks = []\n",
        "    for walk in walks:\n",
        "        str_walk = [idx_to_id[idx] for idx in walk]\n",
        "        str_walks.append(str_walk)\n",
        "\n",
        "    logger.info(\"Word2Vec: Training Skip-gram model...\")\n",
        "\n",
        "    # Train Word2Vec\n",
        "    # sg=1 (Skip-gram), hs=0 (Negative Sampling), workers=1 (Deterministic)\n",
        "    model = Word2Vec(\n",
        "        sentences=str_walks,\n",
        "        vector_size=dimensions,\n",
        "        window=window_size,\n",
        "        min_count=1, # Keep all nodes that appear in walks\n",
        "        sg=1,\n",
        "        workers=1,\n",
        "        seed=seed,\n",
        "        epochs=5,\n",
        "        negative=5 # Default negative sampling\n",
        "    )\n",
        "\n",
        "    # Extract vectors\n",
        "    # The model vocabulary contains the canonical IDs\n",
        "    vocab_keys = list(model.wv.index_to_key)\n",
        "    vectors = model.wv[vocab_keys]\n",
        "\n",
        "    df_embeddings = pd.DataFrame(\n",
        "        vectors,\n",
        "        index=vocab_keys,\n",
        "        columns=[f\"dim_{i}\" for i in range(dimensions)]\n",
        "    )\n",
        "\n",
        "    # Sort index for deterministic output\n",
        "    df_embeddings = df_embeddings.sort_index()\n",
        "\n",
        "    logger.info(f\"Word2Vec: Learned embeddings for {len(df_embeddings)} nodes.\")\n",
        "\n",
        "    return df_embeddings\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def learn_node_embeddings(\n",
        "    bipartite_artifact: Any, # BipartiteGraphArtifact\n",
        "    config: Dict[str, Any]\n",
        ") -> EmbeddingArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 16. Generates node embeddings using Node2Vec with Numba acceleration.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bipartite_artifact : BipartiteGraphArtifact\n",
        "        The constructed bipartite graph.\n",
        "    config : Dict[str, Any]\n",
        "        Configuration containing hyperparameters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    EmbeddingArtifact\n",
        "        The learned embeddings and metadata.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 16: Node2Vec Embedding (Production Grade)...\")\n",
        "\n",
        "    # Extract params with defaults\n",
        "    n2v_config = config.get(\"NODE2VEC\", {})\n",
        "    dim = int(n2v_config.get(\"dimensions\", 100))\n",
        "    seed = int(n2v_config.get(\"random_seed\", 42))\n",
        "\n",
        "    # Assumptions (explicitly recorded)\n",
        "    p = float(n2v_config.get(\"p\", 1.0))\n",
        "    q = float(n2v_config.get(\"q\", 1.0))\n",
        "    num_walks = int(n2v_config.get(\"num_walks\", 10))\n",
        "    walk_length = int(n2v_config.get(\"walk_length\", 80))\n",
        "    window_size = int(n2v_config.get(\"window_size\", 10))\n",
        "\n",
        "    # Step 1: Prepare Graph\n",
        "    A, idx_to_id = prepare_symmetric_adjacency(bipartite_artifact)\n",
        "\n",
        "    # Step 2: Generate Walks (Numba)\n",
        "    walks = generate_walks_production(\n",
        "        A, num_walks, walk_length, p, q, seed\n",
        "    )\n",
        "\n",
        "    # Step 3: Train Model\n",
        "    embeddings = train_skipgram_model(\n",
        "        walks, idx_to_id, dim, window_size, seed\n",
        "    )\n",
        "\n",
        "    # Package\n",
        "    params = {\n",
        "        \"p\": p, \"q\": q, \"num_walks\": num_walks,\n",
        "        \"walk_length\": walk_length, \"window_size\": window_size,\n",
        "        \"seed\": seed, \"dimensions\": dim,\n",
        "        \"implementation\": \"numba_accelerated\"\n",
        "    }\n",
        "\n",
        "    artifact = EmbeddingArtifact(embeddings, params)\n",
        "\n",
        "    logger.info(\"Task 16 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "ctlkvjhxar89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17 — Project embeddings to 2D with UMAP\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Project embeddings to 2D with UMAP\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class UMAPArtifact:\n",
        "    \"\"\"\n",
        "    Container for UMAP projection results and metadata.\n",
        "\n",
        "    This dataclass encapsulates the output of the dimensionality reduction phase,\n",
        "    where the high-dimensional node embeddings are projected onto a 2D manifold.\n",
        "    It holds the resulting coordinate table, which serves as the basis for the\n",
        "    \"Hidden Geometry\" visualization (Figure 2). Additionally, it preserves the\n",
        "    exact hyperparameters used for the UMAP algorithm and the qualitative\n",
        "    interpretation of the resulting axes, ensuring that the visual map is both\n",
        "    reproducible and semantically meaningful.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    coordinates : pd.DataFrame\n",
        "        A DataFrame indexed by `canonical_org_id` containing the 2D coordinates\n",
        "        for each organisation. Columns are `umap_x` and `umap_y`.\n",
        "    parameters : Dict[str, Any]\n",
        "        A dictionary recording the UMAP hyperparameters used, including\n",
        "        `n_neighbors`, `min_dist`, `metric`, and `random_state`.\n",
        "    axis_interpretation : Dict[str, str]\n",
        "        A dictionary mapping axis names (e.g., \"horizontal_axis\") to their\n",
        "        qualitative functional interpretations (e.g., \"Humanitarian vs. Development\"),\n",
        "        derived from the positions of known anchor organisations.\n",
        "    \"\"\"\n",
        "    # The 2D coordinates\n",
        "    coordinates: pd.DataFrame\n",
        "\n",
        "    # The UMAP parameters\n",
        "    parameters: Dict[str, Any]\n",
        "\n",
        "    # The semantic interpretation of axes\n",
        "    axis_interpretation: Dict[str, str]\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the UMAP projection.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the projection size and key parameters.\n",
        "        \"\"\"\n",
        "        # Format summary string\n",
        "        return (f\"UMAP Projection: {len(self.coordinates)} points. \"\n",
        "                f\"Params: {self.parameters}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 1 & 2: Fix parameters and Compute 2D coordinates\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_umap_projection(\n",
        "    embeddings: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Projects high-dimensional embeddings to 2D using UMAP with fixed parameters.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embeddings : pd.DataFrame\n",
        "        Node embeddings (index=canonical_id).\n",
        "    config : Dict[str, Any]\n",
        "        Configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, Dict[str, Any]]\n",
        "        1. DataFrame with columns ['umap_x', 'umap_y'], indexed by canonical_id.\n",
        "        2. Dictionary of parameters used.\n",
        "    \"\"\"\n",
        "    # Extract params\n",
        "    umap_config = config.get(\"UMAP\", {})\n",
        "\n",
        "    # Manuscript-stated defaults\n",
        "    params = {\n",
        "        \"n_neighbors\": int(umap_config.get(\"n_neighbors\", 15)),\n",
        "        \"min_dist\": float(umap_config.get(\"min_dist\", 0.1)),\n",
        "        \"metric\": umap_config.get(\"metric\", \"cosine\"),\n",
        "        \"n_components\": int(umap_config.get(\"n_components\", 2)),\n",
        "        \"random_state\": int(umap_config.get(\"random_state\", 42))\n",
        "    }\n",
        "\n",
        "    logger.info(f\"UMAP: Fitting with params {params}...\")\n",
        "\n",
        "    # Initialize UMAP\n",
        "    reducer = umap.UMAP(\n",
        "        n_neighbors=params[\"n_neighbors\"],\n",
        "        min_dist=params[\"min_dist\"],\n",
        "        metric=params[\"metric\"],\n",
        "        n_components=params[\"n_components\"],\n",
        "        random_state=params[\"random_state\"],\n",
        "        transform_seed=params[\"random_state\"] # Ensure transform is also seeded\n",
        "    )\n",
        "\n",
        "    # Fit Transform\n",
        "    # Ensure deterministic row order by sorting index if not already sorted\n",
        "    # (embeddings should be sorted from previous task, but safety first)\n",
        "    embeddings_sorted = embeddings.sort_index()\n",
        "\n",
        "    # Extract numpy array\n",
        "    X = embeddings_sorted.values\n",
        "\n",
        "    # Run UMAP\n",
        "    embedding_2d = reducer.fit_transform(X)\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_coords = pd.DataFrame(\n",
        "        embedding_2d,\n",
        "        index=embeddings_sorted.index,\n",
        "        columns=['umap_x', 'umap_y']\n",
        "    )\n",
        "\n",
        "    logger.info(f\"UMAP: Projected {len(df_coords)} nodes to 2D.\")\n",
        "\n",
        "    return df_coords, params\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 3: Document axis interpretation (qualitative)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def define_axis_interpretation() -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Defines the qualitative interpretation of the UMAP axes based on the manuscript.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, str]\n",
        "        Dictionary describing the axes.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"horizontal_axis\": \"Humanitarian (Crisis Response) <-> Development (Long-term Resilience)\",\n",
        "        \"vertical_axis\": \"Funders (Capital Supply) <-> Implementers (Operational Delivery)\",\n",
        "        \"anchors\": \"OCHA (Humanitarian Funder), Save the Children (Humanitarian Implementer), \"\n",
        "                   \"UNDP (Development Funder), Chemonics (Development Implementer)\"\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def project_embeddings_umap(\n",
        "    embedding_artifact: Any, # EmbeddingArtifact\n",
        "    config: Dict[str, Any]\n",
        ") -> UMAPArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 17. Projects embeddings to 2D manifold.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embedding_artifact : EmbeddingArtifact\n",
        "        The learned embeddings.\n",
        "    config : Dict[str, Any]\n",
        "        Configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    UMAPArtifact\n",
        "        The projection results.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 17: UMAP Projection...\")\n",
        "\n",
        "    # Step 1 & 2: Compute\n",
        "    coords, params = compute_umap_projection(embedding_artifact.embeddings, config)\n",
        "\n",
        "    # Step 3: Interpret\n",
        "    interpretation = define_axis_interpretation()\n",
        "\n",
        "    # Package\n",
        "    artifact = UMAPArtifact(\n",
        "        coordinates=coords,\n",
        "        parameters=params,\n",
        "        axis_interpretation=interpretation\n",
        "    )\n",
        "\n",
        "    logger.info(\"Task 17 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "dZQ7bRSacmku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18 — Compute Hub Score (HITS) centrality\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Compute Hub Score (HITS) centrality\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class HITSArtifact:\n",
        "    \"\"\"\n",
        "    Container for HITS centrality scores and metadata.\n",
        "\n",
        "    This dataclass encapsulates the results of the Hyperlink-Induced Topic Search (HITS)\n",
        "    algorithm applied to the bipartite transaction network. It provides the computed\n",
        "    Hub scores (for Provider organisations) and Authority scores (for Receiver\n",
        "    organisations), which serve as the primary metrics for ranking systemic influence\n",
        "    in the \"Solar System\" visualization. The artifact also includes convergence\n",
        "    metadata to verify the numerical stability of the power iteration process.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    hub_scores : pd.DataFrame\n",
        "        A DataFrame indexed by `canonical_org_id` containing the Hub scores for\n",
        "        provider nodes. High scores indicate providers that fund many authoritative\n",
        "        receivers.\n",
        "    authority_scores : pd.DataFrame\n",
        "        A DataFrame indexed by `canonical_org_id` containing the Authority scores\n",
        "        for receiver nodes. High scores indicate receivers funded by many\n",
        "        influential hubs.\n",
        "    convergence_meta : Dict[str, Any]\n",
        "        A dictionary recording the convergence status of the algorithm, including\n",
        "        the number of iterations performed, the final residual error, and the\n",
        "        tolerance threshold used.\n",
        "    \"\"\"\n",
        "    # DataFrame of Hub scores (Providers)\n",
        "    hub_scores: pd.DataFrame\n",
        "\n",
        "    # DataFrame of Authority scores (Receivers)\n",
        "    authority_scores: pd.DataFrame\n",
        "\n",
        "    # Metadata regarding algorithm convergence\n",
        "    convergence_meta: Dict[str, Any]\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the HITS results.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the number of scored entities and\n",
        "            convergence status.\n",
        "        \"\"\"\n",
        "        # Format summary string\n",
        "        return (f\"HITS Centrality: {len(self.hub_scores)} Hubs, \"\n",
        "                f\"{len(self.authority_scores)} Authorities. \"\n",
        "                f\"Converged: {self.convergence_meta['converged']}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 1 & 2: Iterate HITS algorithm\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_hits_algorithm(\n",
        "    B: sparse.csr_matrix,\n",
        "    tol: float = 1e-6,\n",
        "    max_iter: int = 100\n",
        ") -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Computes HITS hub and authority scores using power iteration on the bipartite matrix.\n",
        "\n",
        "    h = B * a\n",
        "    a = B.T * h\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    B : sparse.csr_matrix\n",
        "        Incidence matrix (Providers x Receivers).\n",
        "    tol : float\n",
        "        Convergence tolerance (L2 norm difference).\n",
        "    max_iter : int\n",
        "        Maximum iterations.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[np.ndarray, np.ndarray, Dict[str, Any]]\n",
        "        1. Hub scores (size |U|).\n",
        "        2. Authority scores (size |V|).\n",
        "        3. Convergence metadata.\n",
        "    \"\"\"\n",
        "    n_u, n_v = B.shape\n",
        "\n",
        "    # Initialize\n",
        "    # Start with uniform positive values\n",
        "    h = np.ones(n_u) / np.sqrt(n_u)\n",
        "    a = np.ones(n_v) / np.sqrt(n_v)\n",
        "\n",
        "    converged = False\n",
        "    iterations = 0\n",
        "    final_diff = 0.0\n",
        "\n",
        "    # Precompute transpose for efficiency\n",
        "    BT = B.transpose()\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        h_prev = h.copy()\n",
        "        a_prev = a.copy()\n",
        "\n",
        "        # Update Authorities: a = B.T * h\n",
        "        a = BT.dot(h)\n",
        "\n",
        "        # Normalize a\n",
        "        norm_a = np.linalg.norm(a, 2)\n",
        "        if norm_a > 0:\n",
        "            a = a / norm_a\n",
        "\n",
        "        # Update Hubs: h = B * a\n",
        "        h = B.dot(a)\n",
        "\n",
        "        # Normalize h\n",
        "        norm_h = np.linalg.norm(h, 2)\n",
        "        if norm_h > 0:\n",
        "            h = h / norm_h\n",
        "\n",
        "        # Check convergence\n",
        "        diff_h = np.linalg.norm(h - h_prev, 2)\n",
        "        diff_a = np.linalg.norm(a - a_prev, 2)\n",
        "        total_diff = diff_h + diff_a\n",
        "\n",
        "        if total_diff < tol:\n",
        "            converged = True\n",
        "            iterations = i + 1\n",
        "            final_diff = total_diff\n",
        "            break\n",
        "\n",
        "        iterations = i + 1\n",
        "        final_diff = total_diff\n",
        "\n",
        "    meta = {\n",
        "        \"converged\": converged,\n",
        "        \"iterations\": iterations,\n",
        "        \"final_diff\": final_diff,\n",
        "        \"tolerance\": tol\n",
        "    }\n",
        "\n",
        "    logger.info(f\"HITS: Converged={converged} in {iterations} iters (Diff={final_diff:.2e}).\")\n",
        "\n",
        "    return h, a, meta\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 3: Persist hub scores\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def package_hits_artifact(\n",
        "    h: np.ndarray,\n",
        "    a: np.ndarray,\n",
        "    meta: Dict[str, Any],\n",
        "    prov_map: Dict[str, int],\n",
        "    recv_map: Dict[str, int]\n",
        ") -> HITSArtifact:\n",
        "    \"\"\"\n",
        "    Packages HITS scores into DataFrames indexed by canonical IDs.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    h : np.ndarray\n",
        "        Hub vector.\n",
        "    a : np.ndarray\n",
        "        Authority vector.\n",
        "    meta : Dict[str, Any]\n",
        "        Metadata.\n",
        "    prov_map : Dict[str, int]\n",
        "        Provider index map.\n",
        "    recv_map : Dict[str, int]\n",
        "        Receiver index map.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    HITSArtifact\n",
        "        Final artifact.\n",
        "    \"\"\"\n",
        "    # Reverse maps\n",
        "    idx_to_prov = {v: k for k, v in prov_map.items()}\n",
        "    idx_to_recv = {v: k for k, v in recv_map.items()}\n",
        "\n",
        "    # Create DataFrames\n",
        "    # Providers (Hubs)\n",
        "    # Note: prov_map keys are \"ID::PROVIDER\". We strip suffix for canonical ID.\n",
        "\n",
        "    prov_ids = [idx_to_prov[i] for i in range(len(h))]\n",
        "    # Strip suffix \"::PROVIDER\"\n",
        "    canonical_provs = [pid.split(\"::\")[0] for pid in prov_ids]\n",
        "\n",
        "    df_hubs = pd.DataFrame({\n",
        "        'provider_node_id': prov_ids,\n",
        "        'hub_score_hits': h\n",
        "    }, index=canonical_provs)\n",
        "    df_hubs.index.name = 'canonical_org_id'\n",
        "\n",
        "    # Receivers (Authorities)\n",
        "    recv_ids = [idx_to_recv[i] for i in range(len(a))]\n",
        "    canonical_recvs = [rid.split(\"::\")[0] for rid in recv_ids]\n",
        "\n",
        "    df_auths = pd.DataFrame({\n",
        "        'receiver_node_id': recv_ids,\n",
        "        'authority_score_hits': a\n",
        "    }, index=canonical_recvs)\n",
        "    df_auths.index.name = 'canonical_org_id'\n",
        "\n",
        "    artifact = HITSArtifact(\n",
        "        hub_scores=df_hubs,\n",
        "        authority_scores=df_auths,\n",
        "        convergence_meta=meta\n",
        "    )\n",
        "\n",
        "    logger.info(str(artifact))\n",
        "\n",
        "    return artifact\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_hits_centrality(\n",
        "    bipartite_artifact: Any # BipartiteGraphArtifact\n",
        ") -> HITSArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 18. Computes HITS centrality on the bipartite graph.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bipartite_artifact : BipartiteGraphArtifact\n",
        "        The bipartite graph.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    HITSArtifact\n",
        "        The computed scores.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 18: HITS Centrality...\")\n",
        "\n",
        "    # Extract matrix and maps\n",
        "    B = bipartite_artifact.incidence_matrix\n",
        "    prov_map = bipartite_artifact.provider_index_map\n",
        "    recv_map = bipartite_artifact.receiver_index_map\n",
        "\n",
        "    # Step 1 & 2: Run Algorithm\n",
        "    h, a, meta = run_hits_algorithm(B)\n",
        "\n",
        "    # Step 3: Package\n",
        "    artifact = package_hits_artifact(h, a, meta, prov_map, recv_map)\n",
        "\n",
        "    logger.info(\"Task 18 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "MLUoljZzfeTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19 — Compute degree and betweenness centrality\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19: Compute degree and betweenness centrality\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class CentralityArtifact:\n",
        "    \"\"\"\n",
        "    Container for all computed network centrality metrics.\n",
        "\n",
        "    This dataclass serves as the unified repository for the topological influence\n",
        "    metrics calculated across the different graph representations. It merges\n",
        "    metrics derived from the bipartite graph (Hub Scores) with those derived\n",
        "    from the one-mode projection (Degree, Strength, Betweenness). This consolidated\n",
        "    view allows for the multi-dimensional ranking of actors required to construct\n",
        "    the \"Solar System\" visualization and identify key brokers.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    centrality_table : pd.DataFrame\n",
        "        A master DataFrame indexed by `canonical_org_id` containing all computed\n",
        "        centrality measures. Columns include `hub_score_hits`, `degree`,\n",
        "        `strength`, and `betweenness`.\n",
        "    metadata : Dict[str, Any]\n",
        "        A dictionary recording the methodological choices made during centrality\n",
        "        computation, such as the specific algorithm variants (e.g., unweighted\n",
        "        betweenness) and source graphs used.\n",
        "    \"\"\"\n",
        "    # The unified centrality DataFrame\n",
        "    centrality_table: pd.DataFrame\n",
        "\n",
        "    # Methodological metadata\n",
        "    metadata: Dict[str, Any]\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the centrality artifact.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the number of nodes scored.\n",
        "        \"\"\"\n",
        "        # Format summary string\n",
        "        return (f\"Centrality: Computed Degree, Strength, Betweenness, Hubs \"\n",
        "                f\"for {len(self.centrality_table)} nodes.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 1: Compute degree centrality\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_degree_metrics(\n",
        "    adjacency_matrix: sparse.csr_matrix,\n",
        "    index_map: Dict[str, int]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes unweighted degree and weighted strength for each node.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    adjacency_matrix : sparse.csr_matrix\n",
        "        Symmetric provider-provider adjacency P.\n",
        "    index_map : Dict[str, int]\n",
        "        Map from canonical ID to matrix index.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Table with columns ['degree', 'strength'], indexed by canonical_org_id.\n",
        "    \"\"\"\n",
        "    # Reverse map\n",
        "    idx_to_id = {v: k for k, v in index_map.items()}\n",
        "\n",
        "    # Weighted Strength: Sum of row weights\n",
        "    # axis=1 sums rows\n",
        "    strength = np.array(adjacency_matrix.sum(axis=1)).flatten()\n",
        "\n",
        "    # Unweighted Degree: Count of non-zero entries per row\n",
        "    degree = adjacency_matrix.getnnz(axis=1)\n",
        "\n",
        "    # Create DataFrame\n",
        "    # Ensure ordering matches index 0..N-1\n",
        "    ids = [idx_to_id[i] for i in range(len(strength))]\n",
        "\n",
        "    df_degree = pd.DataFrame({\n",
        "        'degree': degree,\n",
        "        'strength': strength\n",
        "    }, index=ids)\n",
        "\n",
        "    df_degree.index.name = 'canonical_org_id'\n",
        "\n",
        "    logger.info(f\"Degree Centrality: Computed for {len(df_degree)} nodes.\")\n",
        "\n",
        "    return df_degree\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 2: Compute betweenness centrality\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_betweenness_metrics(\n",
        "    adjacency_matrix: sparse.csr_matrix,\n",
        "    index_map: Dict[str, int]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes Betweenness Centrality using Brandes' algorithm on the unweighted graph.\n",
        "\n",
        "    Note: We use unweighted betweenness because edge weights in P are similarities,\n",
        "    not distances. Shortest paths on similarity weights would require inverting\n",
        "    weights (1/w), which is an assumption not stated in the manuscript.\n",
        "    Topological brokerage is typically defined on the existence of ties.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    adjacency_matrix : sparse.csr_matrix\n",
        "        Symmetric adjacency P.\n",
        "    index_map : Dict[str, int]\n",
        "        Map from canonical ID to matrix index.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Table with column ['betweenness'], indexed by canonical_org_id.\n",
        "    \"\"\"\n",
        "    # Convert to NetworkX graph\n",
        "    # from_scipy_sparse_array is efficient\n",
        "    G = nx.from_scipy_sparse_array(adjacency_matrix)\n",
        "\n",
        "    # Compute Betweenness\n",
        "    # normalized=True is standard (divides by (N-1)(N-2))\n",
        "    # weight=None implies unweighted BFS for shortest paths\n",
        "    logger.info(\"Betweenness: Running Brandes' algorithm (unweighted)...\")\n",
        "    bc_dict = nx.betweenness_centrality(G, weight=None, normalized=True)\n",
        "\n",
        "    # Map indices back to IDs\n",
        "    idx_to_id = {v: k for k, v in index_map.items()}\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    # bc_dict keys are integers (node indices)\n",
        "    ids = []\n",
        "    scores = []\n",
        "    for idx, score in bc_dict.items():\n",
        "        if idx in idx_to_id:\n",
        "            ids.append(idx_to_id[idx])\n",
        "            scores.append(score)\n",
        "\n",
        "    df_bc = pd.DataFrame({\n",
        "        'betweenness': scores\n",
        "    }, index=ids)\n",
        "    df_bc.index.name = 'canonical_org_id'\n",
        "\n",
        "    logger.info(f\"Betweenness: Computed for {len(df_bc)} nodes.\")\n",
        "\n",
        "    return df_bc\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 3: Persist centrality tables\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def unify_centrality_metrics(\n",
        "    df_degree: pd.DataFrame,\n",
        "    df_betweenness: pd.DataFrame,\n",
        "    hits_artifact: Any # HITSArtifact\n",
        ") -> CentralityArtifact:\n",
        "    \"\"\"\n",
        "    Joins all centrality metrics into a single master table.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_degree : pd.DataFrame\n",
        "        Degree/Strength.\n",
        "    df_betweenness : pd.DataFrame\n",
        "        Betweenness.\n",
        "    hits_artifact : HITSArtifact\n",
        "        Contains Hub Scores.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    CentralityArtifact\n",
        "        Unified results.\n",
        "    \"\"\"\n",
        "    # Get Hub Scores\n",
        "    # Note: Hub scores are for Providers.\n",
        "    df_hubs = hits_artifact.hub_scores[['hub_score_hits']]\n",
        "\n",
        "    # Join\n",
        "    # We start with the projection nodes (Degree/Betweenness are defined on Projection)\n",
        "    # Hub scores should exist for all providers in the projection (since projection comes from bipartite)\n",
        "    # Use outer join to be safe, but expect full overlap on providers\n",
        "    df_master = df_degree.join(df_betweenness, how='outer')\n",
        "    df_master = df_master.join(df_hubs, how='outer')\n",
        "\n",
        "    # Fill NaNs\n",
        "    # If a node is in bipartite but not projection (isolated in projection?), it has 0 degree/betweenness\n",
        "    # If a node is in projection but not bipartite (impossible), it has NaN hub score\n",
        "    df_master = df_master.fillna(0.0)\n",
        "\n",
        "    meta = {\n",
        "        \"betweenness_type\": \"unweighted\",\n",
        "        \"degree_type\": \"unweighted_and_weighted\",\n",
        "        \"hub_score_source\": \"bipartite_hits\"\n",
        "    }\n",
        "\n",
        "    artifact = CentralityArtifact(\n",
        "        centrality_table=df_master,\n",
        "        metadata=meta\n",
        "    )\n",
        "\n",
        "    logger.info(str(artifact))\n",
        "\n",
        "    return artifact\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_network_centrality(\n",
        "    projection_artifact: Any, # ProjectionArtifact\n",
        "    hits_artifact: Any # HITSArtifact\n",
        ") -> CentralityArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 19. Computes and unifies network centrality metrics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    projection_artifact : ProjectionArtifact\n",
        "        The provider-provider graph.\n",
        "    hits_artifact : HITSArtifact\n",
        "        The bipartite hub scores.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    CentralityArtifact\n",
        "        The unified centrality table.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 19: Centrality Computation...\")\n",
        "\n",
        "    P = projection_artifact.adjacency_matrix\n",
        "    idx_map = projection_artifact.provider_index_map\n",
        "\n",
        "    # Step 1: Degree\n",
        "    df_deg = compute_degree_metrics(P, idx_map)\n",
        "\n",
        "    # Step 2: Betweenness\n",
        "    df_bet = compute_betweenness_metrics(P, idx_map)\n",
        "\n",
        "    # Step 3: Unify\n",
        "    artifact = unify_centrality_metrics(df_deg, df_bet, hits_artifact)\n",
        "\n",
        "    logger.info(\"Task 19 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "wjewhftFB_DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20 — Construct solar system ranking and ring assignment\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Construct solar system ranking and ring assignment\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class SolarSystemArtifact:\n",
        "    \"\"\"\n",
        "    Container for the Solar System visualization data.\n",
        "\n",
        "    This dataclass encapsulates the final output required to render the \"Solar System\"\n",
        "    visualization of the global aid network. It combines the centrality-based ranking\n",
        "    of organisations (determining their orbital position) with transaction-based\n",
        "    sizing metrics (determining node radius). The artifact explicitly separates the\n",
        "    full ranked list from the \"Top 100\" display set and includes the definitions\n",
        "    used to assign organisations to specific rings (Inner, Middle, Outer), ensuring\n",
        "    that the visual hierarchy is traceable to quantitative thresholds.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    ranked_nodes : pd.DataFrame\n",
        "        A DataFrame containing all ranked organisations, indexed by `canonical_org_id`.\n",
        "        Columns include `rank`, `hub_score_hits`, `node_size`, and `ring`.\n",
        "    top_100_table : pd.DataFrame\n",
        "        A subset of `ranked_nodes` containing only the top 100 organisations,\n",
        "        optimized for visualization rendering.\n",
        "    ring_definitions : Dict[str, str]\n",
        "        A dictionary mapping ring labels (e.g., \"Inner\") to their rank-based\n",
        "        definitions (e.g., \"Rank 1-25\"), documenting the segmentation logic.\n",
        "    \"\"\"\n",
        "    # The full table of ranked nodes\n",
        "    ranked_nodes: pd.DataFrame\n",
        "\n",
        "    # The subset of top 100 nodes for display\n",
        "    top_100_table: pd.DataFrame\n",
        "\n",
        "    # Metadata defining the ring boundaries\n",
        "    ring_definitions: Dict[str, str]\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the Solar System artifact.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the ranking scope and core size.\n",
        "        \"\"\"\n",
        "        # Format summary string\n",
        "        return (f\"Solar System: {len(self.ranked_nodes)} nodes ranked. \"\n",
        "                f\"Inner Ring: 25 nodes.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 1: Rank organisations by Hub Score\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def rank_organisations(\n",
        "    centrality_artifact: Any # CentralityArtifact\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ranks organisations by Hub Score with deterministic tie-breaking.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    centrality_artifact : CentralityArtifact\n",
        "        Unified centrality table.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Table sorted by rank, with 'rank' column.\n",
        "    \"\"\"\n",
        "    df = centrality_artifact.centrality_table.copy()\n",
        "\n",
        "    # Ensure index is a column for sorting\n",
        "    df = df.reset_index()\n",
        "\n",
        "    # Sort: Hub Score Descending, then ID Ascending (Tie-break)\n",
        "    df = df.sort_values(\n",
        "        by=['hub_score_hits', 'canonical_org_id'],\n",
        "        ascending=[False, True]\n",
        "    )\n",
        "\n",
        "    # Assign Rank (1-based)\n",
        "    df['rank'] = range(1, len(df) + 1)\n",
        "\n",
        "    # Restore index\n",
        "    df = df.set_index('canonical_org_id')\n",
        "\n",
        "    logger.info(f\"Ranking: Ranked {len(df)} organisations.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 2: Assign node sizes\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def assign_node_sizes(\n",
        "    ranked_df: pd.DataFrame,\n",
        "    node_size_artifact: Any # NodeSizeArtifact\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Attaches transaction counts (node sizes) to the ranking table.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ranked_df : pd.DataFrame\n",
        "        Ranked organisations.\n",
        "    node_size_artifact : NodeSizeArtifact\n",
        "        Deal counts.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Ranked table with 'node_size' column.\n",
        "    \"\"\"\n",
        "    # Join deal counts\n",
        "    # We use 'total_tx_count' as the size metric\n",
        "    sizes = node_size_artifact.deal_counts[['total_tx_count']]\n",
        "\n",
        "    # Left join to keep ranked order (though indices should match)\n",
        "    df_sized = ranked_df.join(sizes, how='left')\n",
        "\n",
        "    # Rename for clarity\n",
        "    df_sized = df_sized.rename(columns={'total_tx_count': 'node_size'})\n",
        "\n",
        "    # Fill missing sizes with 0 (should not happen if populations align)\n",
        "    df_sized['node_size'] = df_sized['node_size'].fillna(0).astype(int)\n",
        "\n",
        "    return df_sized\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 3: Derive ring assignments from hub score distribution\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def assign_rings(\n",
        "    df_sized: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Assigns organisations to rings based on rank.\n",
        "\n",
        "    Rule:\n",
        "    - Inner: Rank 1-25\n",
        "    - Middle: Rank 26-100\n",
        "    - Outer: Rank > 100\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_sized : pd.DataFrame\n",
        "        Ranked and sized table.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, Dict[str, str]]\n",
        "        1. Table with 'ring' column.\n",
        "        2. Ring definition metadata.\n",
        "    \"\"\"\n",
        "    df_out = df_sized.copy()\n",
        "\n",
        "    conditions = [\n",
        "        (df_out['rank'] <= 25),\n",
        "        (df_out['rank'] <= 100)\n",
        "    ]\n",
        "    choices = ['Inner', 'Middle']\n",
        "\n",
        "    df_out['ring'] = np.select(conditions, choices, default='Outer')\n",
        "\n",
        "    definitions = {\n",
        "        \"Inner\": \"Rank 1-25 (Core)\",\n",
        "        \"Middle\": \"Rank 26-100\",\n",
        "        \"Outer\": \"Rank > 100\"\n",
        "    }\n",
        "\n",
        "    logger.info(\"Ring Assignment: Applied rank-based rings.\")\n",
        "\n",
        "    return df_out, definitions\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_solar_system(\n",
        "    centrality_artifact: Any, # CentralityArtifact\n",
        "    node_size_artifact: Any # NodeSizeArtifact\n",
        ") -> SolarSystemArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 20. Constructs the Solar System visualization data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    centrality_artifact : CentralityArtifact\n",
        "        Centrality scores.\n",
        "    node_size_artifact : NodeSizeArtifact\n",
        "        Node sizes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    SolarSystemArtifact\n",
        "        The solar system data.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 20: Solar System Construction...\")\n",
        "\n",
        "    # Step 1: Rank\n",
        "    ranked = rank_organisations(centrality_artifact)\n",
        "\n",
        "    # Step 2: Size\n",
        "    sized = assign_node_sizes(ranked, node_size_artifact)\n",
        "\n",
        "    # Step 3: Rings\n",
        "    final_df, defs = assign_rings(sized)\n",
        "\n",
        "    # Extract Top 100 for display\n",
        "    top_100 = final_df[final_df['rank'] <= 100].copy()\n",
        "\n",
        "    artifact = SolarSystemArtifact(\n",
        "        ranked_nodes=final_df,\n",
        "        top_100_table=top_100,\n",
        "        ring_definitions=defs\n",
        "    )\n",
        "\n",
        "    logger.info(\"Task 20 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "MJ_LeQ2tkiER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21 — Reproduce subgroup statistics (universities and foundations)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Reproduce subgroup statistics (universities and foundations)\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class SubgroupStatsArtifact:\n",
        "    \"\"\"\n",
        "    Container for subgroup analysis results.\n",
        "\n",
        "    This dataclass encapsulates the comparative statistics derived for specific\n",
        "    functional subgroups of the aid network, namely Universities (Academic/Research)\n",
        "    and Foundations. It provides the quantitative evidence required to support\n",
        "    manuscript claims regarding the \"paradox of scale vs. influence,\" where\n",
        "    smaller actors (by deal count) occupy disproportionately central positions.\n",
        "    The artifact includes both the aggregated summary table and a lookup table\n",
        "    identifying the specific broker organisations (e.g., J-PAL, Hewlett) used\n",
        "    as case studies.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    summary_table : pd.DataFrame\n",
        "        A DataFrame indexed by group name (\"Global\", \"Universities\", \"Foundations\")\n",
        "        containing summary statistics: count, mean/median deal size, median rank,\n",
        "        and representation in the top 100/1000.\n",
        "    broker_lookup : pd.DataFrame\n",
        "        A DataFrame containing the canonical identities and metadata for the\n",
        "        specific broker organisations highlighted in the text, facilitating\n",
        "        downstream ego-network extraction.\n",
        "    \"\"\"\n",
        "    # The summary statistics table\n",
        "    summary_table: pd.DataFrame\n",
        "\n",
        "    # The broker identification table\n",
        "    broker_lookup: pd.DataFrame\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the subgroup statistics.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary indicating the subgroups analyzed.\n",
        "        \"\"\"\n",
        "        # Format summary string\n",
        "        return \"Subgroup Stats: Computed for Academic/Research and Foundation.\"\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 1: Partition organisations by type\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def partition_subgroups(\n",
        "    df_organisations: pd.DataFrame,\n",
        "    ranked_nodes: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Partitions the ranked organisations into Universities and Foundations based on\n",
        "    the controlled taxonomy.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_organisations : pd.DataFrame\n",
        "        Master organisation table with 'org_type'.\n",
        "    ranked_nodes : pd.DataFrame\n",
        "        Ranked organisations (providers).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, pd.DataFrame]\n",
        "        1. Universities DataFrame (subset of ranked_nodes).\n",
        "        2. Foundations DataFrame (subset of ranked_nodes).\n",
        "    \"\"\"\n",
        "    # Map types to ranked nodes\n",
        "    # We need to join org_type from master to ranked nodes\n",
        "    # ranked_nodes is indexed by canonical_org_id\n",
        "\n",
        "    # Prepare type map\n",
        "    type_map = (\n",
        "        df_organisations\n",
        "        .dropna(subset=['canonical_org_id', 'org_type'])\n",
        "        .drop_duplicates('canonical_org_id')\n",
        "        .set_index('canonical_org_id')['org_type']\n",
        "    )\n",
        "\n",
        "    # Enrich ranked nodes\n",
        "    df_enriched = ranked_nodes.copy()\n",
        "    df_enriched['org_type'] = df_enriched.index.map(type_map)\n",
        "\n",
        "    # Filter\n",
        "    # Taxonomy from Task 8: \"Academic/Research\", \"Foundation\"\n",
        "    universities = df_enriched[df_enriched['org_type'] == \"Academic/Research\"].copy()\n",
        "    foundations = df_enriched[df_enriched['org_type'] == \"Foundation\"].copy()\n",
        "\n",
        "    logger.info(f\"Subgroups: Found {len(universities)} Universities, {len(foundations)} Foundations in ranked set.\")\n",
        "\n",
        "    return universities, foundations\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 2: Compute comparative statistics\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def calculate_comparative_stats(\n",
        "    universities: pd.DataFrame,\n",
        "    foundations: pd.DataFrame,\n",
        "    all_ranked: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes comparative statistics (deal count, rank) for subgroups vs global.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    universities : pd.DataFrame\n",
        "        University subgroup.\n",
        "    foundations : pd.DataFrame\n",
        "        Foundation subgroup.\n",
        "    all_ranked : pd.DataFrame\n",
        "        All ranked nodes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Summary table.\n",
        "    \"\"\"\n",
        "    def get_stats(df: pd.DataFrame, label: str) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"Group\": label,\n",
        "            \"Count\": len(df),\n",
        "            \"Mean_Deals\": df['node_size'].mean(),\n",
        "            \"Median_Deals\": df['node_size'].median(),\n",
        "            \"Median_Rank\": df['rank'].median(),\n",
        "            \"In_Top_100\": (df['rank'] <= 100).sum(),\n",
        "            \"In_Top_1000\": (df['rank'] <= 1000).sum()\n",
        "        }\n",
        "\n",
        "    stats_list = [\n",
        "        get_stats(all_ranked, \"Global\"),\n",
        "        get_stats(universities, \"Universities\"),\n",
        "        get_stats(foundations, \"Foundations\")\n",
        "    ]\n",
        "\n",
        "    summary = pd.DataFrame(stats_list).set_index(\"Group\")\n",
        "\n",
        "    # Log comparison to manuscript claims\n",
        "    # Manuscript: Universities median rank 434 vs global 571\n",
        "    logger.info(\"Subgroup Stats:\\n\" + str(summary))\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 3: Persist subgroup summary table\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def identify_brokers(\n",
        "    df_organisations: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Identifies specific brokers (J-PAL, Hewlett) for downstream analysis.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_organisations : pd.DataFrame\n",
        "        Master table.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Lookup table for brokers.\n",
        "    \"\"\"\n",
        "    # We look for \"Abdul Latif Jameel Poverty Action Lab\" or \"J-PAL\"\n",
        "    # and \"William and Flora Hewlett Foundation\"\n",
        "    # This relies on the canonical names or aliases being present.\n",
        "\n",
        "    # Simple search in normalized names\n",
        "    mask_jpal = df_organisations['org_name_normalized'].str.contains(\"poverty action lab\", na=False)\n",
        "    mask_hewlett = df_organisations['org_name_normalized'].str.contains(\"hewlett foundation\", na=False)\n",
        "\n",
        "    brokers = df_organisations[mask_jpal | mask_hewlett].copy()\n",
        "\n",
        "    # Keep relevant columns\n",
        "    brokers = brokers[['canonical_org_id', 'org_name', 'org_type']].drop_duplicates('canonical_org_id')\n",
        "\n",
        "    logger.info(f\"Broker Identification: Found {len(brokers)} candidates.\")\n",
        "\n",
        "    return brokers\n",
        "\n",
        "def package_subgroup_artifact(\n",
        "    summary: pd.DataFrame,\n",
        "    brokers: pd.DataFrame\n",
        ") -> SubgroupStatsArtifact:\n",
        "    \"\"\"\n",
        "    Packages the subgroup stats.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    summary : pd.DataFrame\n",
        "        Stats table.\n",
        "    brokers : pd.DataFrame\n",
        "        Broker lookup.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    SubgroupStatsArtifact\n",
        "        Final artifact.\n",
        "    \"\"\"\n",
        "    artifact = SubgroupStatsArtifact(\n",
        "        summary_table=summary,\n",
        "        broker_lookup=brokers\n",
        "    )\n",
        "\n",
        "    logger.info(str(artifact))\n",
        "\n",
        "    return artifact\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_subgroup_statistics(\n",
        "    df_organisations: pd.DataFrame,\n",
        "    solar_system_artifact: Any # SolarSystemArtifact\n",
        ") -> SubgroupStatsArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 21. Computes subgroup statistics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_organisations : pd.DataFrame\n",
        "        Master table.\n",
        "    solar_system_artifact : SolarSystemArtifact\n",
        "        Ranked nodes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    SubgroupStatsArtifact\n",
        "        The stats.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 21: Subgroup Statistics...\")\n",
        "\n",
        "    ranked = solar_system_artifact.ranked_nodes\n",
        "\n",
        "    # Step 1: Partition\n",
        "    unis, founds = partition_subgroups(df_organisations, ranked)\n",
        "\n",
        "    # Step 2: Stats\n",
        "    summary = calculate_comparative_stats(unis, founds, ranked)\n",
        "\n",
        "    # Step 3: Brokers\n",
        "    brokers = identify_brokers(df_organisations)\n",
        "\n",
        "    # Package\n",
        "    artifact = package_subgroup_artifact(summary, brokers)\n",
        "\n",
        "    logger.info(\"Task 21 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "QkJqdLHgnMwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22 — Reproduce broker betweenness comparison (Extended Data Fig. S3-type)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 22: Reproduce broker betweenness comparison (Extended Data Fig. S3-type)\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class BrokerComparisonArtifact:\n",
        "    \"\"\"\n",
        "    Container for broker betweenness comparison data.\n",
        "\n",
        "    This dataclass encapsulates the quantitative evidence supporting the manuscript's\n",
        "    central finding: that specific \"knowledge brokers\" (e.g., J-PAL, Hewlett Foundation)\n",
        "    possess disproportionately high betweenness centrality relative to their financial\n",
        "    volume. It provides a direct comparison between the betweenness scores of these\n",
        "    brokers and the median betweenness of the \"Inner Ring\" (Top 25) actors. This\n",
        "    comparison validates the \"hidden geometry\" hypothesis by demonstrating that\n",
        "    structural influence is distinct from, and often uncorrelated with, aggregate\n",
        "    transaction volume.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    comparison_table : pd.DataFrame\n",
        "        A DataFrame indexed by `canonical_org_id` containing the betweenness scores\n",
        "        for the identified brokers, the baseline median of the Top 25, and the\n",
        "        calculated ratio (`broker_score / median_score`).\n",
        "    median_top25_betweenness : float\n",
        "        The scalar median betweenness centrality of the 25 most central organisations\n",
        "        (by Hub Score). This serves as the baseline for \"high influence\" in the network.\n",
        "    \"\"\"\n",
        "    # The comparison table with scores and ratios\n",
        "    comparison_table: pd.DataFrame\n",
        "\n",
        "    # The baseline median value\n",
        "    median_top25_betweenness: float\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the broker comparison.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the baseline median and number of brokers compared.\n",
        "        \"\"\"\n",
        "        # Format summary string\n",
        "        return (f\"Broker Comparison: Median Top 25 = {self.median_top25_betweenness:.4f}. \"\n",
        "                f\"Brokers analyzed: {len(self.comparison_table)}.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 1: Extract betweenness for highlighted brokers\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def extract_broker_scores(\n",
        "    centrality_artifact: Any, # CentralityArtifact\n",
        "    subgroup_artifact: Any # SubgroupStatsArtifact\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Retrieves betweenness centrality scores for the identified broker organisations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    centrality_artifact : CentralityArtifact\n",
        "        Unified centrality table.\n",
        "    subgroup_artifact : SubgroupStatsArtifact\n",
        "        Contains broker lookup table.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Table with columns ['org_name', 'betweenness'], indexed by canonical_org_id.\n",
        "    \"\"\"\n",
        "    # Get broker IDs\n",
        "    brokers = subgroup_artifact.broker_lookup.copy()\n",
        "\n",
        "    # Get centrality\n",
        "    centrality = centrality_artifact.centrality_table[['betweenness']]\n",
        "\n",
        "    # Join\n",
        "    # Inner join to ensure we only get brokers that exist in the network\n",
        "    broker_scores = brokers.join(centrality, on='canonical_org_id', how='inner')\n",
        "\n",
        "    # Set index\n",
        "    broker_scores = broker_scores.set_index('canonical_org_id')\n",
        "\n",
        "    logger.info(f\"Broker Scores: Retrieved for {len(broker_scores)} brokers.\")\n",
        "\n",
        "    return broker_scores\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 2: Compute Top 25 median betweenness\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_top25_median(\n",
        "    centrality_artifact: Any, # CentralityArtifact\n",
        "    solar_system_artifact: Any # SolarSystemArtifact\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the median betweenness centrality of the Top 25 organisations (Inner Ring).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    centrality_artifact : CentralityArtifact\n",
        "        Centrality scores.\n",
        "    solar_system_artifact : SolarSystemArtifact\n",
        "        Ranked nodes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Median betweenness.\n",
        "    \"\"\"\n",
        "    # Identify Top 25 IDs\n",
        "    ranked = solar_system_artifact.ranked_nodes\n",
        "    top_25_ids = ranked[ranked['rank'] <= 25].index\n",
        "\n",
        "    # Get scores\n",
        "    centrality = centrality_artifact.centrality_table\n",
        "    top_25_scores = centrality.loc[centrality.index.intersection(top_25_ids), 'betweenness']\n",
        "\n",
        "    # Compute median\n",
        "    median_val = top_25_scores.median()\n",
        "\n",
        "    # Handle empty case (unlikely)\n",
        "    if pd.isna(median_val):\n",
        "        median_val = 0.0\n",
        "\n",
        "    logger.info(f\"Top 25 Median Betweenness: {median_val:.6f}\")\n",
        "\n",
        "    return float(median_val)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 3: Persist comparison artifact\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def package_broker_comparison(\n",
        "    broker_scores: pd.DataFrame,\n",
        "    median_val: float\n",
        ") -> BrokerComparisonArtifact:\n",
        "    \"\"\"\n",
        "    Packages the comparison results.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    broker_scores : pd.DataFrame\n",
        "        Broker scores.\n",
        "    median_val : float\n",
        "        Baseline median.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    BrokerComparisonArtifact\n",
        "        Final artifact.\n",
        "    \"\"\"\n",
        "    df_out = broker_scores.copy()\n",
        "    df_out['median_top25'] = median_val\n",
        "\n",
        "    # Compute ratio\n",
        "    # Handle division by zero\n",
        "    if median_val > 0:\n",
        "        df_out['ratio_to_median'] = df_out['betweenness'] / median_val\n",
        "    else:\n",
        "        df_out['ratio_to_median'] = np.inf # Or NaN, but Inf signals infinite advantage\n",
        "\n",
        "    artifact = BrokerComparisonArtifact(\n",
        "        comparison_table=df_out,\n",
        "        median_top25_betweenness=median_val\n",
        "    )\n",
        "\n",
        "    logger.info(str(artifact))\n",
        "\n",
        "    return artifact\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compare_broker_betweenness(\n",
        "    centrality_artifact: Any, # CentralityArtifact\n",
        "    subgroup_artifact: Any, # SubgroupStatsArtifact\n",
        "    solar_system_artifact: Any # SolarSystemArtifact\n",
        ") -> BrokerComparisonArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 22. Compares broker betweenness to the core median.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    centrality_artifact : CentralityArtifact\n",
        "        Centrality scores.\n",
        "    subgroup_artifact : SubgroupStatsArtifact\n",
        "        Broker info.\n",
        "    solar_system_artifact : SolarSystemArtifact\n",
        "        Rankings.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    BrokerComparisonArtifact\n",
        "        The comparison data.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 22: Broker Comparison...\")\n",
        "\n",
        "    # Step 1: Extract\n",
        "    scores = extract_broker_scores(centrality_artifact, subgroup_artifact)\n",
        "\n",
        "    # Step 2: Baseline\n",
        "    median = compute_top25_median(centrality_artifact, solar_system_artifact)\n",
        "\n",
        "    # Step 3: Package\n",
        "    artifact = package_broker_comparison(scores, median)\n",
        "\n",
        "    logger.info(\"Task 22 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "Jbw8DSNMxBnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23 — Reproduce Hewlett subnetwork characterization\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 23: Reproduce Hewlett subnetwork characterization\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class HewlettNetworkArtifact:\n",
        "    \"\"\"\n",
        "    Container for the Hewlett Foundation ego network analysis.\n",
        "\n",
        "    This dataclass encapsulates the extracted subnetwork centered on the William\n",
        "    and Flora Hewlett Foundation, a key \"knowledge broker\" identified in the study.\n",
        "    It contains the edge list representing the Foundation's direct funding\n",
        "    relationships (downstream partners) and a set of portfolio metrics that\n",
        "    quantify its structural reach. This artifact supports the visualization of\n",
        "    how a single central actor bridges disparate clusters (e.g., connecting\n",
        "    research institutes to implementation NGOs) and validates the claim that\n",
        "    influence flows through connectivity.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    ego_edges : pd.DataFrame\n",
        "        A DataFrame containing the edge list of the Hewlett Foundation's ego network.\n",
        "        Columns include `receiver_canonical_id` (the partner organisation) and\n",
        "        `tx_count` (the number of transactions).\n",
        "    portfolio_metrics : Dict[str, Any]\n",
        "        A dictionary of scalar metrics describing the portfolio, including the\n",
        "        count of `unique_partners`, the `total_transactions` volume, and the\n",
        "        number of partners that are also members of the global `partners_in_top_100`.\n",
        "    \"\"\"\n",
        "    # The edge list of the ego network\n",
        "    ego_edges: pd.DataFrame\n",
        "\n",
        "    # Summary metrics of the portfolio\n",
        "    portfolio_metrics: Dict[str, Any]\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the Hewlett network.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the network size and transaction volume.\n",
        "        \"\"\"\n",
        "        # Format summary string\n",
        "        return (f\"Hewlett Network: {self.portfolio_metrics['unique_partners']} partners, \"\n",
        "                f\"{self.portfolio_metrics['total_transactions']} transactions.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 1: Define Hewlett ego network extraction rule\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def extract_hewlett_ego(\n",
        "    df_transactions: pd.DataFrame,\n",
        "    subgroup_artifact: Any, # SubgroupStatsArtifact\n",
        "    canonical_mapping: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts the ego network for the William and Flora Hewlett Foundation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions : pd.DataFrame\n",
        "        Cleansed transactions.\n",
        "    subgroup_artifact : SubgroupStatsArtifact\n",
        "        Contains broker lookup to find Hewlett's ID.\n",
        "    canonical_mapping : pd.DataFrame\n",
        "        Mapping to resolve transaction endpoints if not already mapped in df_transactions.\n",
        "        (Note: Task 13 mapped them, but we might need to re-map if df_transactions is raw cleansed).\n",
        "        Actually, we should use the mapped transaction table from Task 13 if available,\n",
        "        or re-map here. Let's assume we pass the mapped transaction table from Task 13\n",
        "        or re-map using the artifact.\n",
        "\n",
        "        Better: Use the mapped transaction table from Task 13 (df_tx_mapped) if possible.\n",
        "        However, the orchestrator flow might not persist that large table in memory.\n",
        "        We will re-map efficiently here using the canonical mapping artifact.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Edge list of Hewlett's downstream partners (receiver, tx_count).\n",
        "    \"\"\"\n",
        "    # Find Hewlett ID\n",
        "    brokers = subgroup_artifact.broker_lookup\n",
        "\n",
        "    # Look for \"Hewlett\" in name if ID not known, but we found it in Task 21.\n",
        "    # We assume the broker lookup contains the correct row.\n",
        "    # We filter for the one with \"Hewlett\" in name.\n",
        "    hewlett_row = brokers[brokers['org_name'].str.contains(\"Hewlett\", case=False, na=False)]\n",
        "\n",
        "    if len(hewlett_row) == 0:\n",
        "        logger.warning(\"Hewlett Foundation not found in broker lookup.\")\n",
        "        return pd.DataFrame(columns=['receiver_canonical_id', 'tx_count'])\n",
        "\n",
        "    hewlett_id = hewlett_row.iloc[0]['canonical_org_id']\n",
        "    logger.info(f\"Identified Hewlett ID: {hewlett_id}\")\n",
        "\n",
        "    # Map transactions to canonical if needed\n",
        "    # We need provider_canonical_id and receiver_canonical_id\n",
        "    # We can use the mapping table\n",
        "\n",
        "    # Create map\n",
        "    mapping_dict = canonical_mapping.set_index('org_ref')['canonical_org_id'].to_dict()\n",
        "\n",
        "    # Filter for Hewlett as provider (using Ref or Name)\n",
        "    # Optimization: Find refs that map to Hewlett ID\n",
        "    hewlett_refs = [k for k, v in mapping_dict.items() if v == hewlett_id]\n",
        "\n",
        "    # Filter transactions where provider ref is in hewlett_refs\n",
        "    # OR provider name matches (if we want to be loose, but strict is better)\n",
        "    mask_provider = df_transactions['transaction_provider_org_ref'].isin(hewlett_refs)\n",
        "\n",
        "    df_ego = df_transactions[mask_provider].copy()\n",
        "\n",
        "    # Map receivers\n",
        "    df_ego['receiver_canonical_id'] = df_ego['transaction_receiver_org_ref'].map(mapping_dict)\n",
        "    # Fallback to name if needed (as in Task 13)\n",
        "    df_ego['receiver_canonical_id'] = df_ego['receiver_canonical_id'].fillna(df_ego['transaction_receiver_org_name'])\n",
        "\n",
        "    # Aggregate\n",
        "    edges = (\n",
        "        df_ego\n",
        "        .groupby('receiver_canonical_id')\n",
        "        .size()\n",
        "        .reset_index(name='tx_count')\n",
        "        .sort_values('tx_count', ascending=False)\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Hewlett Ego: Found {len(edges)} downstream partners.\")\n",
        "\n",
        "    return edges\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 2: Compute Hewlett portfolio metrics\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_portfolio_metrics(\n",
        "    ego_edges: pd.DataFrame,\n",
        "    solar_system_artifact: Any # SolarSystemArtifact\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Computes metrics for Hewlett's portfolio.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ego_edges : pd.DataFrame\n",
        "        Hewlett's edges.\n",
        "    solar_system_artifact : SolarSystemArtifact\n",
        "        Rankings.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Metrics dictionary.\n",
        "    \"\"\"\n",
        "    # Basic stats\n",
        "    unique_partners = len(ego_edges)\n",
        "    total_tx = ego_edges['tx_count'].sum()\n",
        "\n",
        "    # Top 100 overlap\n",
        "    top_100_ids = set(solar_system_artifact.top_100_table.index)\n",
        "    partners = set(ego_edges['receiver_canonical_id'])\n",
        "\n",
        "    overlap = partners.intersection(top_100_ids)\n",
        "    count_top_100 = len(overlap)\n",
        "\n",
        "    metrics = {\n",
        "        \"unique_partners\": unique_partners,\n",
        "        \"total_transactions\": int(total_tx),\n",
        "        \"partners_in_top_100\": count_top_100,\n",
        "        \"top_100_partner_ids\": list(overlap)\n",
        "    }\n",
        "\n",
        "    logger.info(f\"Hewlett Metrics: {unique_partners} partners, {total_tx} txs, {count_top_100} in Top 100.\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 3: Persist Hewlett subnetwork artifact\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def package_hewlett_artifact(\n",
        "    ego_edges: pd.DataFrame,\n",
        "    metrics: Dict[str, Any]\n",
        ") -> HewlettNetworkArtifact:\n",
        "    \"\"\"\n",
        "    Packages the Hewlett network analysis.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ego_edges : pd.DataFrame\n",
        "        Edges.\n",
        "    metrics : Dict[str, Any]\n",
        "        Metrics.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    HewlettNetworkArtifact\n",
        "        Final artifact.\n",
        "    \"\"\"\n",
        "    artifact = HewlettNetworkArtifact(\n",
        "        ego_edges=ego_edges,\n",
        "        portfolio_metrics=metrics\n",
        "    )\n",
        "\n",
        "    logger.info(str(artifact))\n",
        "\n",
        "    return artifact\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def characterize_hewlett_network(\n",
        "    df_transactions: pd.DataFrame,\n",
        "    subgroup_artifact: Any, # SubgroupStatsArtifact\n",
        "    canonical_mapping_artifact: Any, # CanonicalMappingArtifact\n",
        "    solar_system_artifact: Any # SolarSystemArtifact\n",
        ") -> HewlettNetworkArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 23. Characterizes the Hewlett Foundation's network.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions : pd.DataFrame\n",
        "        Cleansed transactions.\n",
        "    subgroup_artifact : SubgroupStatsArtifact\n",
        "        Broker info.\n",
        "    canonical_mapping_artifact : CanonicalMappingArtifact\n",
        "        Mapping table.\n",
        "    solar_system_artifact : SolarSystemArtifact\n",
        "        Rankings.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    HewlettNetworkArtifact\n",
        "        The analysis results.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 23: Hewlett Network Characterization...\")\n",
        "\n",
        "    mapping = canonical_mapping_artifact.mapping_table\n",
        "\n",
        "    # Step 1: Extract\n",
        "    edges = extract_hewlett_ego(df_transactions, subgroup_artifact, mapping)\n",
        "\n",
        "    # Step 2: Metrics\n",
        "    metrics = compute_portfolio_metrics(edges, solar_system_artifact)\n",
        "\n",
        "    # Step 3: Package\n",
        "    artifact = package_hewlett_artifact(edges, metrics)\n",
        "\n",
        "    logger.info(\"Task 23 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "um62PMJi0r2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 24 — Construct website hyperlink graph for external validation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 24: Construct website hyperlink graph for external validation\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class WebGraphArtifact:\n",
        "    \"\"\"\n",
        "    Container for the web hyperlink graph.\n",
        "\n",
        "    This dataclass encapsulates the constructed external validation network,\n",
        "    derived from website hyperlink data. It holds the sparse adjacency matrix\n",
        "    representing the directed web graph, the mapping between domain names and\n",
        "    matrix indices, and coverage statistics quantifying the overlap between\n",
        "    the aid organisation population and the web crawl. This artifact is the\n",
        "    input for the PageRank computation used to validate the \"offline\" centrality\n",
        "    metrics.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    adjacency_matrix : sparse.csr_matrix\n",
        "        The sparse adjacency matrix of the web graph, where rows represent source\n",
        "        domains and columns represent target domains. Entries are binary (1 for link).\n",
        "    domain_index_map : Dict[str, int]\n",
        "        A dictionary mapping normalized domain strings (e.g., \"example.org\") to\n",
        "        their integer indices in the adjacency matrix.\n",
        "    node_list : List[str]\n",
        "        A list of all unique domains in the graph, ordered by their index.\n",
        "    coverage_stats : Dict[str, Any]\n",
        "        A dictionary containing metrics on the intersection between the organisation\n",
        "        master list and the web graph nodes (e.g., match rate).\n",
        "    \"\"\"\n",
        "    # The sparse adjacency matrix\n",
        "    adjacency_matrix: sparse.csr_matrix\n",
        "\n",
        "    # Map from domain string to matrix index\n",
        "    domain_index_map: Dict[str, int]\n",
        "\n",
        "    # List of unique domains\n",
        "    node_list: List[str]\n",
        "\n",
        "    # Coverage statistics\n",
        "    coverage_stats: Dict[str, Any]\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the web graph.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the graph size and organisation coverage.\n",
        "        \"\"\"\n",
        "        # Format summary string\n",
        "        return (f\"Web Graph: {len(self.node_list)} nodes, {self.adjacency_matrix.nnz} edges. \"\n",
        "                f\"Org Coverage: {self.coverage_stats['match_rate']:.2%}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 1: Build domain-level directed graph\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_web_adjacency(\n",
        "    df_web_links: pd.DataFrame\n",
        ") -> Tuple[sparse.csr_matrix, Dict[str, int], List[str]]:\n",
        "    \"\"\"\n",
        "    Constructs the directed adjacency matrix of the web graph.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_web_links : pd.DataFrame\n",
        "        Raw web links with 'source_domain', 'target_domain'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[sparse.csr_matrix, Dict[str, int], List[str]]\n",
        "        1. Sparse adjacency matrix (rows=source, cols=target).\n",
        "        2. Domain index map.\n",
        "        3. List of unique domains (nodes).\n",
        "    \"\"\"\n",
        "    # Normalize domains using the same logic as Task 8\n",
        "    # We assume the helper normalize_domains is available or we re-implement\n",
        "    # Re-implementing for self-containment within this task block logic\n",
        "    def norm(s_series):\n",
        "        return (\n",
        "            s_series.astype(str)\n",
        "            .str.lower().str.strip()\n",
        "            .str.replace(r'^https?://', '', regex=True)\n",
        "            .str.replace(r'^www\\.', '', regex=True)\n",
        "            .str.split('/').str[0]\n",
        "        )\n",
        "\n",
        "    df_links = df_web_links.copy()\n",
        "    df_links['source'] = norm(df_links['source_domain'])\n",
        "    df_links['target'] = norm(df_links['target_domain'])\n",
        "\n",
        "    # Filter self-loops and empty\n",
        "    df_links = df_links[df_links['source'] != df_links['target']]\n",
        "    df_links = df_links[df_links['source'] != 'nan']\n",
        "    df_links = df_links[df_links['target'] != 'nan']\n",
        "\n",
        "    # Get unique nodes\n",
        "    unique_domains = sorted(list(set(df_links['source']) | set(df_links['target'])))\n",
        "    domain_map = {d: i for i, d in enumerate(unique_domains)}\n",
        "\n",
        "    # Map to indices\n",
        "    src_idx = df_links['source'].map(domain_map).values\n",
        "    tgt_idx = df_links['target'].map(domain_map).values\n",
        "\n",
        "    # Build Adjacency (Unweighted)\n",
        "    # Use ones for existence\n",
        "    data = np.ones(len(src_idx))\n",
        "\n",
        "    shape = (len(unique_domains), len(unique_domains))\n",
        "    adj = sparse.coo_matrix((data, (src_idx, tgt_idx)), shape=shape).tocsr()\n",
        "\n",
        "    # Binarize (remove duplicate links)\n",
        "    adj.data = np.ones_like(adj.data)\n",
        "\n",
        "    logger.info(f\"Web Graph: Built with {len(unique_domains)} nodes and {adj.nnz} edges.\")\n",
        "\n",
        "    return adj, domain_map, unique_domains\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 2: Validate domain coverage\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_web_coverage(\n",
        "    unique_domains: List[str],\n",
        "    df_organisations: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Computes the overlap between organisation websites and the web graph.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    unique_domains : List[str]\n",
        "        Nodes in web graph.\n",
        "    df_organisations : pd.DataFrame\n",
        "        Master organisation table with 'website_domain_normalized'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Coverage stats.\n",
        "    \"\"\"\n",
        "    web_nodes = set(unique_domains)\n",
        "\n",
        "    # Get org domains\n",
        "    # Filter for non-null\n",
        "    org_domains = df_organisations['website_domain_normalized'].dropna().unique()\n",
        "\n",
        "    # Compute intersection\n",
        "    matched = [d for d in org_domains if d in web_nodes]\n",
        "\n",
        "    match_count = len(matched)\n",
        "    total_orgs_with_web = len(org_domains)\n",
        "\n",
        "    rate = match_count / total_orgs_with_web if total_orgs_with_web > 0 else 0.0\n",
        "\n",
        "    stats = {\n",
        "        \"total_org_domains\": total_orgs_with_web,\n",
        "        \"matched_domains\": match_count,\n",
        "        \"match_rate\": rate\n",
        "    }\n",
        "\n",
        "    logger.info(f\"Web Coverage: {match_count}/{total_orgs_with_web} ({rate:.2%}) matched.\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 3: Persist web graph artifact\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def package_web_artifact(\n",
        "    adj: sparse.csr_matrix,\n",
        "    domain_map: Dict[str, int],\n",
        "    nodes: List[str],\n",
        "    stats: Dict[str, Any]\n",
        ") -> WebGraphArtifact:\n",
        "    \"\"\"\n",
        "    Packages the web graph.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    adj : sparse.csr_matrix\n",
        "        Adjacency.\n",
        "    domain_map : Dict[str, int]\n",
        "        Index map.\n",
        "    nodes : List[str]\n",
        "        Node list.\n",
        "    stats : Dict[str, Any]\n",
        "        Stats.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    WebGraphArtifact\n",
        "        Final artifact.\n",
        "    \"\"\"\n",
        "    artifact = WebGraphArtifact(\n",
        "        adjacency_matrix=adj,\n",
        "        domain_index_map=domain_map,\n",
        "        node_list=nodes,\n",
        "        coverage_stats=stats\n",
        "    )\n",
        "\n",
        "    logger.info(str(artifact))\n",
        "\n",
        "    return artifact\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_web_graph(\n",
        "    df_web_links_raw: pd.DataFrame,\n",
        "    df_organisations: pd.DataFrame\n",
        ") -> WebGraphArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 24. Constructs the web graph for validation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_web_links_raw : pd.DataFrame\n",
        "        Raw links.\n",
        "    df_organisations : pd.DataFrame\n",
        "        Master orgs.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    WebGraphArtifact\n",
        "        The web graph.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 24: Web Graph Construction...\")\n",
        "\n",
        "    # Step 1: Build Graph\n",
        "    adj, dmap, nodes = build_web_adjacency(df_web_links_raw)\n",
        "\n",
        "    # Step 2: Validate\n",
        "    stats = validate_web_coverage(nodes, df_organisations)\n",
        "\n",
        "    # Step 3: Package\n",
        "    artifact = package_web_artifact(adj, dmap, nodes, stats)\n",
        "\n",
        "    logger.info(\"Task 24 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "lfX2sq4H2rgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 25 — Compute website PageRank\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 25: Compute website PageRank\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class PageRankArtifact:\n",
        "    \"\"\"\n",
        "    Container for PageRank scores and metadata.\n",
        "\n",
        "    This dataclass encapsulates the results of the PageRank computation on the\n",
        "    external website hyperlink graph. It provides the \"online\" authority scores\n",
        "    for each domain, which serve as the independent variable for validating the\n",
        "    \"offline\" centrality metrics derived from the transaction network. The artifact\n",
        "    also includes convergence metadata to ensure the numerical stability of the\n",
        "    power iteration process.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    pagerank_scores : pd.DataFrame\n",
        "        A DataFrame indexed by `website_domain_normalized` containing the computed\n",
        "        PageRank scores. These scores represent the stationary distribution of a\n",
        "        random walker on the web graph.\n",
        "    convergence_meta : Dict[str, Any]\n",
        "        A dictionary recording the convergence status of the algorithm, including\n",
        "        the number of iterations performed, the final L1 residual error, and the\n",
        "        damping factor (`alpha`) used.\n",
        "    \"\"\"\n",
        "    # The computed PageRank scores\n",
        "    pagerank_scores: pd.DataFrame\n",
        "\n",
        "    # Metadata regarding algorithm convergence\n",
        "    convergence_meta: Dict[str, Any]\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the PageRank results.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the number of scored domains and\n",
        "            convergence status.\n",
        "        \"\"\"\n",
        "        # Format summary string\n",
        "        return (f\"PageRank: Computed for {len(self.pagerank_scores)} domains. \"\n",
        "                f\"Converged: {self.convergence_meta['converged']}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Step 1: Define PageRank iteration\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_pagerank_matrix(\n",
        "    adjacency: sparse.csr_matrix\n",
        ") -> Tuple[sparse.csr_matrix, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Prepares the column-stochastic transition matrix M and identifies dangling nodes.\n",
        "\n",
        "    M_ij = 1 / outdeg(j) if j->i exists, else 0.\n",
        "    (Note: We construct M such that x_new = M * x_old, so M is column-stochastic-ish.\n",
        "     Actually, standard adjacency A has A_ij=1 if i->j.\n",
        "     Transition matrix P has P_ij = 1/outdeg(i) if i->j.\n",
        "     Power iteration: x_new = P.T * x_old.\n",
        "     So we need P.T).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    adjacency : sparse.csr_matrix\n",
        "        Adjacency matrix A where A_ij = 1 if i -> j.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[sparse.csr_matrix, np.ndarray]\n",
        "        1. Transposed transition matrix M = P.T.\n",
        "        2. Boolean mask of dangling nodes (outdegree == 0).\n",
        "    \"\"\"\n",
        "    n = adjacency.shape[0]\n",
        "\n",
        "    # Calculate outdegrees (row sums)\n",
        "    outdegrees = np.array(adjacency.sum(axis=1)).flatten()\n",
        "\n",
        "    # Identify dangling nodes\n",
        "    dangling_mask = (outdegrees == 0)\n",
        "\n",
        "    # Normalize rows to create P\n",
        "    # Avoid division by zero for dangling nodes\n",
        "    # We multiply A by diag(1/outdeg)\n",
        "\n",
        "    # Inverse outdegrees\n",
        "    inv_outdegrees = np.zeros_like(outdegrees, dtype=float)\n",
        "    inv_outdegrees[~dangling_mask] = 1.0 / outdegrees[~dangling_mask]\n",
        "\n",
        "    # Create diagonal matrix D_inv\n",
        "    D_inv = sparse.diags(inv_outdegrees)\n",
        "\n",
        "    # P = D_inv * A\n",
        "    P = D_inv.dot(adjacency)\n",
        "\n",
        "    # We need M = P.T for x_new = M * x\n",
        "    M = P.transpose()\n",
        "\n",
        "    logger.info(f\"PageRank Prep: {dangling_mask.sum()} dangling nodes identified.\")\n",
        "\n",
        "    return M, dangling_mask\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Step 2: Iterate until convergence\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_pagerank_power_iteration(\n",
        "    M: sparse.csr_matrix,\n",
        "    dangling_mask: np.ndarray,\n",
        "    alpha: float = 0.85,\n",
        "    tol: float = 1e-6,\n",
        "    max_iter: int = 100\n",
        ") -> Tuple[np.ndarray, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Executes the PageRank power iteration.\n",
        "\n",
        "    x_{k+1} = alpha * M * x_k + (alpha * (sum(x_k[dangling]) / N) + (1-alpha) / N) * 1\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    M : sparse.csr_matrix\n",
        "        Transposed transition matrix.\n",
        "    dangling_mask : np.ndarray\n",
        "        Mask of dangling nodes.\n",
        "    alpha : float\n",
        "        Damping factor.\n",
        "    tol : float\n",
        "        Convergence tolerance (L1 norm).\n",
        "    max_iter : int\n",
        "        Max iterations.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[np.ndarray, Dict[str, Any]]\n",
        "        1. PageRank vector.\n",
        "        2. Metadata.\n",
        "    \"\"\"\n",
        "    n = M.shape[0]\n",
        "\n",
        "    # Initialize uniform\n",
        "    x = np.ones(n) / n\n",
        "\n",
        "    converged = False\n",
        "    iterations = 0\n",
        "    final_diff = 0.0\n",
        "\n",
        "    # Constant teleportation term (1-alpha)/N\n",
        "    teleport = (1.0 - alpha) / n\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        x_prev = x.copy()\n",
        "\n",
        "        # Calculate dangling mass sum\n",
        "        dangling_sum = x_prev[dangling_mask].sum()\n",
        "\n",
        "        # Calculate redistribution term\n",
        "        # This term is added to every node\n",
        "        redist = (alpha * dangling_sum / n) + teleport\n",
        "\n",
        "        # Update\n",
        "        # x_new = alpha * M * x_prev + redist\n",
        "        x = alpha * M.dot(x_prev) + redist\n",
        "\n",
        "        # Check convergence (L1 norm)\n",
        "        diff = np.linalg.norm(x - x_prev, 1)\n",
        "\n",
        "        if diff < tol:\n",
        "            converged = True\n",
        "            iterations = i + 1\n",
        "            final_diff = diff\n",
        "            break\n",
        "\n",
        "        iterations = i + 1\n",
        "        final_diff = diff\n",
        "\n",
        "    meta = {\n",
        "        \"converged\": converged,\n",
        "        \"iterations\": iterations,\n",
        "        \"final_diff\": final_diff,\n",
        "        \"alpha\": alpha\n",
        "    }\n",
        "\n",
        "    logger.info(f\"PageRank: Converged={converged} in {iterations} iters (Diff={final_diff:.2e}).\")\n",
        "\n",
        "    return x, meta\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Step 3: Persist PageRank vector\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def package_pagerank_artifact(\n",
        "    scores: np.ndarray,\n",
        "    domain_map: Dict[str, int],\n",
        "    meta: Dict[str, Any]\n",
        ") -> PageRankArtifact:\n",
        "    \"\"\"\n",
        "    Packages the PageRank results.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    scores : np.ndarray\n",
        "        Score vector.\n",
        "    domain_map : Dict[str, int]\n",
        "        Index map.\n",
        "    meta : Dict[str, Any]\n",
        "        Metadata.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    PageRankArtifact\n",
        "        Final artifact.\n",
        "    \"\"\"\n",
        "    # Reverse map\n",
        "    idx_to_domain = {v: k for k, v in domain_map.items()}\n",
        "\n",
        "    # Create DataFrame\n",
        "    domains = [idx_to_domain[i] for i in range(len(scores))]\n",
        "\n",
        "    df_scores = pd.DataFrame({\n",
        "        'pagerank': scores\n",
        "    }, index=domains)\n",
        "    df_scores.index.name = 'website_domain_normalized'\n",
        "\n",
        "    artifact = PageRankArtifact(\n",
        "        pagerank_scores=df_scores,\n",
        "        convergence_meta=meta\n",
        "    )\n",
        "\n",
        "    logger.info(str(artifact))\n",
        "\n",
        "    return artifact\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_website_pagerank(\n",
        "    web_graph_artifact: Any, # WebGraphArtifact\n",
        "    config: Dict[str, Any]\n",
        ") -> PageRankArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 25. Computes PageRank on the web graph.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    web_graph_artifact : WebGraphArtifact\n",
        "        The web graph.\n",
        "    config : Dict[str, Any]\n",
        "        Configuration (for alpha, though usually fixed).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    PageRankArtifact\n",
        "        The scores.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 25: PageRank Computation...\")\n",
        "\n",
        "    # Extract\n",
        "    adj = web_graph_artifact.adjacency_matrix\n",
        "    dmap = web_graph_artifact.domain_index_map\n",
        "\n",
        "    # Assumption: alpha=0.85\n",
        "    alpha = 0.85\n",
        "\n",
        "    # Step 1: Prepare\n",
        "    M, dangling = prepare_pagerank_matrix(adj)\n",
        "\n",
        "    # Step 2: Iterate\n",
        "    scores, meta = run_pagerank_power_iteration(M, dangling, alpha=alpha)\n",
        "\n",
        "    # Step 3: Package\n",
        "    artifact = package_pagerank_artifact(scores, dmap, meta)\n",
        "\n",
        "    logger.info(\"Task 25 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "kOQipu-I5B5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 26 — Compute offline-online Pearson correlation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 26: Compute offline-online Pearson correlation\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class CorrelationArtifact:\n",
        "    \"\"\"\n",
        "    Container for the offline-online correlation validation.\n",
        "\n",
        "    This dataclass encapsulates the final validation metric of the study: the\n",
        "    Pearson correlation coefficient between an organisation's \"offline\" structural\n",
        "    influence (Hub Score in the transaction network) and its \"online\" visibility\n",
        "    (PageRank in the web hyperlink graph). This metric serves as an external\n",
        "    validity check for the constructed aid network, confirming that the topological\n",
        "    centrality derived from financial flows aligns with independent measures of\n",
        "    institutional authority. The artifact includes the matched dataset used for\n",
        "    calculation, the statistical results, and the discrepancy relative to the\n",
        "    manuscript's reported value (r=0.48).\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    matched_data : pd.DataFrame\n",
        "        A DataFrame indexed by `canonical_org_id` containing the aligned\n",
        "        `hub_score_hits` and `pagerank` values for the intersection of organisations\n",
        "        present in both the transaction network and the web crawl.\n",
        "    pearson_r : float\n",
        "        The computed Pearson correlation coefficient (r).\n",
        "    p_value : float\n",
        "        The two-tailed p-value for the hypothesis that the correlation is zero.\n",
        "    sample_size : int\n",
        "        The number of organisations (n) included in the correlation calculation.\n",
        "    discrepancy : float\n",
        "        The difference between the computed r and the manuscript's reported r=0.48.\n",
        "    \"\"\"\n",
        "    # The aligned dataset used for correlation\n",
        "    matched_data: pd.DataFrame\n",
        "\n",
        "    # The Pearson correlation coefficient\n",
        "    pearson_r: float\n",
        "\n",
        "    # The p-value of the correlation\n",
        "    p_value: float\n",
        "\n",
        "    # The sample size (n)\n",
        "    sample_size: int\n",
        "\n",
        "    # The difference from the reported value (r - 0.48)\n",
        "    discrepancy: float\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the correlation results.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the correlation coefficient, significance,\n",
        "            sample size, and alignment with the manuscript.\n",
        "        \"\"\"\n",
        "        # Format summary string\n",
        "        return (f\"Validation Correlation: r={self.pearson_r:.4f} (p={self.p_value:.4e}, n={self.sample_size}). \"\n",
        "                f\"Discrepancy from 0.48: {self.discrepancy:.4f}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Step 1: Align offline and online vectors\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def align_metrics(\n",
        "    centrality_artifact: Any, # CentralityArtifact\n",
        "    pagerank_artifact: Any, # PageRankArtifact\n",
        "    df_organisations: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aligns offline Hub Scores with online PageRank scores via organisation domains.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    centrality_artifact : CentralityArtifact\n",
        "        Offline metrics.\n",
        "    pagerank_artifact : PageRankArtifact\n",
        "        Online metrics.\n",
        "    df_organisations : pd.DataFrame\n",
        "        Master table linking IDs to domains.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Table with columns ['hub_score', 'pagerank'], indexed by canonical_org_id.\n",
        "    \"\"\"\n",
        "    # 1. Get Hub Scores (Offline)\n",
        "    # Indexed by canonical_org_id\n",
        "    hubs = centrality_artifact.centrality_table[['hub_score_hits']]\n",
        "\n",
        "    # 2. Get PageRank (Online)\n",
        "    # Indexed by website_domain_normalized\n",
        "    pr = pagerank_artifact.pagerank_scores[['pagerank']]\n",
        "\n",
        "    # 3. Get Mapping (ID -> Domain)\n",
        "    # We need canonical_org_id -> website_domain_normalized\n",
        "    # Use the cleaned organisation table\n",
        "    mapping = (\n",
        "        df_organisations\n",
        "        .dropna(subset=['canonical_org_id', 'website_domain_normalized'])\n",
        "        .drop_duplicates('canonical_org_id') # One domain per org\n",
        "        .set_index('canonical_org_id')[['website_domain_normalized']]\n",
        "    )\n",
        "\n",
        "    # 4. Join\n",
        "    # Start with Hubs (our population of interest)\n",
        "    # Join Domain\n",
        "    aligned = hubs.join(mapping, how='inner')\n",
        "\n",
        "    # Join PageRank on Domain\n",
        "    # We reset index to join on column, then restore\n",
        "    aligned = aligned.reset_index().merge(\n",
        "        pr,\n",
        "        left_on='website_domain_normalized',\n",
        "        right_index=True,\n",
        "        how='inner'\n",
        "    ).set_index('canonical_org_id')\n",
        "\n",
        "    logger.info(f\"Metric Alignment: Matched {len(aligned)} organisations with both Hub Score and PageRank.\")\n",
        "\n",
        "    return aligned\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Step 2: Compute Pearson correlation coefficient\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def calculate_pearson_correlation(\n",
        "    aligned_data: pd.DataFrame\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Computes Pearson r between Hub Score and PageRank.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    aligned_data : pd.DataFrame\n",
        "        Table with 'hub_score_hits' and 'pagerank'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[float, float]\n",
        "        Pearson r and p-value.\n",
        "    \"\"\"\n",
        "    x = aligned_data['hub_score_hits'].values\n",
        "    y = aligned_data['pagerank'].values\n",
        "\n",
        "    if len(x) < 2:\n",
        "        logger.warning(\"Correlation: Insufficient data (<2 points).\")\n",
        "        return 0.0, 1.0\n",
        "\n",
        "    # Check variance\n",
        "    if np.std(x) == 0 or np.std(y) == 0:\n",
        "        logger.warning(\"Correlation: Zero variance in one or both vectors.\")\n",
        "        return 0.0, 1.0\n",
        "\n",
        "    r, p = stats.pearsonr(x, y)\n",
        "\n",
        "    logger.info(f\"Correlation: r={r:.4f}, p={p:.4e}\")\n",
        "\n",
        "    return r, p\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Step 3: Validate against manuscript\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def package_correlation_artifact(\n",
        "    aligned_data: pd.DataFrame,\n",
        "    r: float,\n",
        "    p: float\n",
        ") -> CorrelationArtifact:\n",
        "    \"\"\"\n",
        "    Packages the correlation results.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    aligned_data : pd.DataFrame\n",
        "        Data used.\n",
        "    r : float\n",
        "        Correlation.\n",
        "    p : float\n",
        "        P-value.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    CorrelationArtifact\n",
        "        Final artifact.\n",
        "    \"\"\"\n",
        "    reported_r = 0.48\n",
        "    discrepancy = r - reported_r\n",
        "\n",
        "    artifact = CorrelationArtifact(\n",
        "        matched_data=aligned_data,\n",
        "        pearson_r=r,\n",
        "        p_value=p,\n",
        "        sample_size=len(aligned_data),\n",
        "        discrepancy=discrepancy\n",
        "    )\n",
        "\n",
        "    logger.info(str(artifact))\n",
        "\n",
        "    return artifact\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_validation_correlation(\n",
        "    centrality_artifact: Any, # CentralityArtifact\n",
        "    pagerank_artifact: Any, # PageRankArtifact\n",
        "    df_organisations: pd.DataFrame\n",
        ") -> CorrelationArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 26. Validates offline centrality against online PageRank.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    centrality_artifact : CentralityArtifact\n",
        "        Offline scores.\n",
        "    pagerank_artifact : PageRankArtifact\n",
        "        Online scores.\n",
        "    df_organisations : pd.DataFrame\n",
        "        Master table.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    CorrelationArtifact\n",
        "        The validation results.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 26: Correlation Validation...\")\n",
        "\n",
        "    # Step 1: Align\n",
        "    aligned = align_metrics(centrality_artifact, pagerank_artifact, df_organisations)\n",
        "\n",
        "    # Step 2: Compute\n",
        "    r, p = calculate_pearson_correlation(aligned)\n",
        "\n",
        "    # Step 3: Package\n",
        "    artifact = package_correlation_artifact(aligned, r, p)\n",
        "\n",
        "    logger.info(\"Task 26 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "g-9kCqi471I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 27 — Create end-to-end orchestrator function\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 27: Create end-to-end orchestrator function\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class PipelineResults:\n",
        "    \"\"\"\n",
        "    Container for all pipeline artifacts, execution metadata, and persistence paths.\n",
        "\n",
        "    This dataclass serves as the final output of the orchestrator. It holds references\n",
        "    to the in-memory artifacts generated by each task, as well as the file paths\n",
        "    where these artifacts have been persisted. This dual-reference system supports\n",
        "    both immediate downstream usage (in-memory) and long-term auditability (disk).\n",
        "    It encapsulates the entire state of a pipeline run, allowing for post-hoc\n",
        "    inspection of any stage's output.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    run_id : str\n",
        "        A unique identifier for the pipeline execution (e.g., timestamp-based).\n",
        "    start_time : str\n",
        "        The timestamp when the pipeline execution began.\n",
        "    end_time : Optional[str], default=None\n",
        "        The timestamp when the pipeline execution completed (or failed).\n",
        "    success : bool, default=False\n",
        "        A flag indicating whether the pipeline completed all tasks successfully.\n",
        "    error_message : Optional[str], default=None\n",
        "        The exception message and stack trace if the pipeline failed.\n",
        "    artifacts : Dict[str, Any], default=dict\n",
        "        A dictionary mapping artifact names (e.g., \"cleansed_transactions\",\n",
        "        \"bipartite_graph\") to their in-memory Python objects.\n",
        "    artifact_paths : Dict[str, Path], default=dict\n",
        "        A dictionary mapping artifact names to the file paths where they were\n",
        "        persisted on disk.\n",
        "    \"\"\"\n",
        "    # Execution Metadata\n",
        "    run_id: str\n",
        "    start_time: str\n",
        "    end_time: Optional[str] = None\n",
        "    success: bool = False\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "    # Artifact Storage (In-Memory)\n",
        "    artifacts: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    # Persistence Paths (On-Disk)\n",
        "    artifact_paths: Dict[str, Path] = field(default_factory=dict)\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the pipeline results.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the run status and artifact count.\n",
        "        \"\"\"\n",
        "        status = \"Success\" if self.success else f\"Failed: {self.error_message}\"\n",
        "        return f\"Pipeline Results (Run {self.run_id}): {status}. {len(self.artifacts)} artifacts generated.\"\n",
        "\n",
        "class ArtifactManager:\n",
        "    \"\"\"\n",
        "    Manages the persistence of pipeline artifacts to a structured directory.\n",
        "\n",
        "    This class handles the I/O operations required to save DataFrames and generic\n",
        "    Python objects to disk in a reproducible manner. It enforces a directory\n",
        "    structure keyed by the run ID, ensuring that outputs from different executions\n",
        "    do not overwrite each other. It abstracts the serialization logic (Parquet for\n",
        "    tables, Pickle for objects), simplifying the orchestrator's responsibility.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    base_path : Path\n",
        "        The root directory for the current run's artifacts.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_dir: str, run_id: str):\n",
        "        \"\"\"\n",
        "        Initializes the ArtifactManager.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        base_dir : str\n",
        "            The parent directory for all pipeline outputs.\n",
        "        run_id : str\n",
        "            The unique identifier for the current run.\n",
        "        \"\"\"\n",
        "        self.base_path = Path(base_dir) / run_id\n",
        "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def save_dataframe(self, df: pd.DataFrame, name: str, subdir: str = \"\") -> Path:\n",
        "        \"\"\"\n",
        "        Saves a pandas DataFrame to a Parquet file.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        df : pd.DataFrame\n",
        "            The DataFrame to persist.\n",
        "        name : str\n",
        "            The base filename (without extension).\n",
        "        subdir : str, optional\n",
        "            A subdirectory within the run folder (e.g., \"01_cleansing\").\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Path\n",
        "            The full path to the saved file.\n",
        "        \"\"\"\n",
        "        dir_path = self.base_path / subdir\n",
        "        dir_path.mkdir(exist_ok=True)\n",
        "        file_path = dir_path / f\"{name}.parquet\"\n",
        "        df.to_parquet(file_path, index=True)\n",
        "        return file_path\n",
        "\n",
        "    def save_object(self, obj: Any, name: str, subdir: str = \"\") -> Path:\n",
        "        \"\"\"\n",
        "        Saves a generic Python object (dataclass, dict, etc.) to a Pickle file.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        obj : Any\n",
        "            The object to persist.\n",
        "        name : str\n",
        "            The base filename (without extension).\n",
        "        subdir : str, optional\n",
        "            A subdirectory within the run folder.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Path\n",
        "            The full path to the saved file.\n",
        "        \"\"\"\n",
        "        dir_path = self.base_path / subdir\n",
        "        dir_path.mkdir(exist_ok=True)\n",
        "        file_path = dir_path / f\"{name}.pkl\"\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            pickle.dump(obj, f)\n",
        "        return file_path\n",
        "\n",
        "def run_global_aid_pipeline(\n",
        "    df_transactions_raw: pd.DataFrame,\n",
        "    df_activities_raw: pd.DataFrame,\n",
        "    df_organisations_raw: pd.DataFrame,\n",
        "    df_web_links_raw: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"./pipeline_output\"\n",
        ") -> PipelineResults:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end Global Aid Network analysis pipeline.\n",
        "\n",
        "    This orchestrator manages the data lifecycle from raw ingestion through\n",
        "    cleansing, graph construction, topological analysis, and external validation.\n",
        "    It enforces the strict dependency order required by the research methodology\n",
        "    and ensures that all intermediate and final artifacts are persisted for\n",
        "    reproducibility.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions_raw : pd.DataFrame\n",
        "        Raw IATI transaction data.\n",
        "    df_activities_raw : pd.DataFrame\n",
        "        Raw IATI activity metadata.\n",
        "    df_organisations_raw : pd.DataFrame\n",
        "        Raw organisation master list.\n",
        "    df_web_links_raw : pd.DataFrame\n",
        "        Raw web crawl data for validation.\n",
        "    config : Dict[str, Any]\n",
        "        Master configuration dictionary defining scope, parameters, and assumptions.\n",
        "    output_dir : str\n",
        "        Root directory for artifact persistence.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    PipelineResults\n",
        "        A container object holding all computed artifacts and execution status.\n",
        "    \"\"\"\n",
        "    run_id = f\"run_{int(time.time())}\"\n",
        "    results = PipelineResults(run_id=run_id, start_time=time.ctime())\n",
        "    manager = ArtifactManager(output_dir, run_id)\n",
        "\n",
        "    logger.info(f\"=== Starting Global Aid Pipeline (Run ID: {run_id}) ===\")\n",
        "\n",
        "    try:\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Phase 1: Validation & Setup\n",
        "        # ---------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 1: Validation & Setup ---\")\n",
        "        input_data = {\n",
        "            \"df_transactions_raw\": df_transactions_raw,\n",
        "            \"df_activities_raw\": df_activities_raw,\n",
        "            \"df_organisations_raw\": df_organisations_raw,\n",
        "            \"df_web_links_raw\": df_web_links_raw\n",
        "        }\n",
        "\n",
        "        # Task 1: Config Validation\n",
        "        validate_study_configuration(config, input_data)\n",
        "\n",
        "        # Task 2: Data Quality\n",
        "        dq_report = validate_data_quality(df_transactions_raw, df_activities_raw, df_organisations_raw)\n",
        "        results.artifacts[\"dq_report\"] = dq_report\n",
        "        results.artifact_paths[\"dq_report\"] = manager.save_object(dq_report, \"dq_report\", \"01_validation\")\n",
        "\n",
        "        # Task 3: Reproducibility Context\n",
        "        repro_context = establish_reproducibility_context(config, input_data)\n",
        "        results.artifacts[\"reproducibility_context\"] = repro_context\n",
        "        results.artifact_paths[\"reproducibility_context\"] = manager.save_object(repro_context, \"repro_context\", \"01_validation\")\n",
        "\n",
        "        sort_policies = repro_context.sort_policies\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Phase 2: Cleansing\n",
        "        # ---------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 2: Cleansing ---\")\n",
        "\n",
        "        # Task 4: Transaction Temporal/Value\n",
        "        tx_temp, tx_excl_1 = cleanse_transactions_temporal_value(df_transactions_raw, config)\n",
        "        results.artifacts[\"transaction_exclusions_1\"] = tx_excl_1\n",
        "\n",
        "        # Task 5: Transaction Endpoints/Dedup\n",
        "        tx_clean, tx_excl_2 = cleanse_transactions_endpoints_dedup(tx_temp, sort_policies[\"df_transactions_raw\"])\n",
        "        results.artifacts[\"cleansed_transactions\"] = tx_clean\n",
        "        results.artifacts[\"transaction_exclusions_2\"] = tx_excl_2\n",
        "        results.artifact_paths[\"cleansed_transactions\"] = manager.save_dataframe(tx_clean, \"transactions_clean\", \"02_cleansing\")\n",
        "\n",
        "        # Task 6: Activity Normalization\n",
        "        act_backbone, act_ctry, act_sect, act_excl = cleanse_activities_normalization(\n",
        "            df_activities_raw, sort_policies[\"df_activities_raw\"]\n",
        "        )\n",
        "        results.artifacts[\"activity_exclusions\"] = act_excl\n",
        "\n",
        "        # Task 7: Instrument Normalization\n",
        "        act_final, inst_excl, act_cov = cleanse_activities_instruments_coverage(act_backbone, act_ctry, act_sect)\n",
        "        results.artifacts[\"activity_backbone\"] = act_final\n",
        "        results.artifacts[\"activity_countries\"] = act_ctry\n",
        "        results.artifacts[\"activity_sectors\"] = act_sect\n",
        "        results.artifacts[\"activity_coverage\"] = act_cov\n",
        "        results.artifact_paths[\"activity_backbone\"] = manager.save_dataframe(act_final, \"activities_clean\", \"02_cleansing\")\n",
        "\n",
        "        # Task 8: Organisation Cleansing\n",
        "        org_clean, org_excl = cleanse_organisations_substrate(df_organisations_raw)\n",
        "        results.artifacts[\"cleansed_organisations\"] = org_clean\n",
        "        results.artifacts[\"organisation_exclusions\"] = org_excl\n",
        "        results.artifact_paths[\"cleansed_organisations\"] = manager.save_dataframe(org_clean, \"organisations_clean\", \"02_cleansing\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Phase 3: Integration & ER\n",
        "        # ---------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 3: Integration & ER ---\")\n",
        "\n",
        "        # Task 9: Context Joins\n",
        "        tx_inst, contexts, ctx_cov = join_transactions_to_contexts(\n",
        "            tx_clean, act_final, act_ctry, act_sect\n",
        "        )\n",
        "        results.artifacts[\"tx_instruments\"] = tx_inst\n",
        "        results.artifacts[\"unified_contexts\"] = contexts\n",
        "        results.artifacts[\"context_coverage\"] = ctx_cov\n",
        "        results.artifact_paths[\"unified_contexts\"] = manager.save_dataframe(contexts, \"contexts\", \"03_integration\")\n",
        "\n",
        "        # Task 10: Entity Resolution\n",
        "        org_mapped, er_artifact = construct_canonical_mapping(org_clean, config)\n",
        "        results.artifacts[\"canonical_mapping\"] = er_artifact\n",
        "        results.artifacts[\"organisations_mapped\"] = org_mapped\n",
        "        results.artifact_paths[\"canonical_mapping\"] = manager.save_object(er_artifact, \"er_artifact\", \"03_integration\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Phase 4: Descriptive Analysis\n",
        "        # ---------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 4: Descriptive Analysis ---\")\n",
        "\n",
        "        # Task 11: Geo Density\n",
        "        # Filter contexts for countries\n",
        "        tx_countries = contexts[contexts['context_id'].str.startswith('COUNTRY:')].copy()\n",
        "        tx_countries['recipient_country_code'] = tx_countries['context_id'].str.replace('COUNTRY:', '')\n",
        "\n",
        "        geo_density = compute_geographic_density(tx_clean, tx_countries)\n",
        "        results.artifacts[\"geo_density\"] = geo_density\n",
        "        results.artifact_paths[\"geo_density\"] = manager.save_object(geo_density, \"geo_density\", \"04_descriptive\")\n",
        "\n",
        "        # Task 12: Instrument Evolution\n",
        "        inst_evolution = compute_instrument_evolution(tx_inst)\n",
        "        results.artifacts[\"instrument_evolution\"] = inst_evolution\n",
        "        results.artifact_paths[\"instrument_evolution\"] = manager.save_object(inst_evolution, \"instrument_evolution\", \"04_descriptive\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Phase 5: Network Construction\n",
        "        # ---------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 5: Network Construction ---\")\n",
        "\n",
        "        # Task 13: Bipartite Graph\n",
        "        bipartite_graph = construct_bipartite_graph(tx_clean, org_mapped)\n",
        "        results.artifacts[\"bipartite_graph\"] = bipartite_graph\n",
        "        results.artifact_paths[\"bipartite_graph\"] = manager.save_object(bipartite_graph, \"bipartite_graph\", \"05_networks\")\n",
        "\n",
        "        # Task 14: Node Sizes\n",
        "        # Re-map transactions using the logic from Task 13 (exposed via helper or re-run)\n",
        "        # We use the helper function directly\n",
        "        df_tx_mapped, _, _ = prepare_bipartite_nodes(tx_clean, org_mapped)\n",
        "        node_sizes = compute_node_sizes(df_tx_mapped, org_mapped)\n",
        "        results.artifacts[\"node_sizes\"] = node_sizes\n",
        "        results.artifact_paths[\"node_sizes\"] = manager.save_object(node_sizes, \"node_sizes\", \"05_networks\")\n",
        "\n",
        "        # Task 15: Projection\n",
        "        projection_graph = construct_co_occurrence_projection(contexts, df_tx_mapped)\n",
        "        results.artifacts[\"projection_graph\"] = projection_graph\n",
        "        results.artifact_paths[\"projection_graph\"] = manager.save_object(projection_graph, \"projection_graph\", \"05_networks\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Phase 6: Topology & Embeddings\n",
        "        # ---------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 6: Topology & Embeddings ---\")\n",
        "\n",
        "        # Task 16: Node2Vec\n",
        "        embeddings = learn_node_embeddings(bipartite_graph, config)\n",
        "        results.artifacts[\"embeddings\"] = embeddings\n",
        "        results.artifact_paths[\"embeddings\"] = manager.save_object(embeddings, \"embeddings\", \"06_topology\")\n",
        "\n",
        "        # Task 17: UMAP\n",
        "        umap_proj = project_embeddings_umap(embeddings, config)\n",
        "        results.artifacts[\"umap_projection\"] = umap_proj\n",
        "        results.artifact_paths[\"umap_projection\"] = manager.save_object(umap_proj, \"umap_projection\", \"06_topology\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Phase 7: Centrality & Ranking\n",
        "        # ---------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 7: Centrality & Ranking ---\")\n",
        "\n",
        "        # Task 18: HITS\n",
        "        hits_scores = compute_hits_centrality(bipartite_graph)\n",
        "        results.artifacts[\"hits_scores\"] = hits_scores\n",
        "        results.artifact_paths[\"hits_scores\"] = manager.save_object(hits_scores, \"hits_scores\", \"07_centrality\")\n",
        "\n",
        "        # Task 19: Centrality\n",
        "        centrality_metrics = compute_network_centrality(projection_graph, hits_scores)\n",
        "        results.artifacts[\"centrality_metrics\"] = centrality_metrics\n",
        "        results.artifact_paths[\"centrality_metrics\"] = manager.save_object(centrality_metrics, \"centrality_metrics\", \"07_centrality\")\n",
        "\n",
        "        # Task 20: Solar System\n",
        "        solar_system = construct_solar_system(centrality_metrics, node_sizes)\n",
        "        results.artifacts[\"solar_system\"] = solar_system\n",
        "        results.artifact_paths[\"solar_system\"] = manager.save_object(solar_system, \"solar_system\", \"07_centrality\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Phase 8: Analysis & Validation\n",
        "        # ---------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 8: Analysis & Validation ---\")\n",
        "\n",
        "        # Task 21: Subgroups\n",
        "        subgroup_stats = compute_subgroup_statistics(org_mapped, solar_system)\n",
        "        results.artifacts[\"subgroup_stats\"] = subgroup_stats\n",
        "        results.artifact_paths[\"subgroup_stats\"] = manager.save_object(subgroup_stats, \"subgroup_stats\", \"08_analysis\")\n",
        "\n",
        "        # Task 22: Broker Comparison\n",
        "        broker_comparison = compare_broker_betweenness(\n",
        "            centrality_metrics, subgroup_stats, solar_system\n",
        "        )\n",
        "        results.artifacts[\"broker_comparison\"] = broker_comparison\n",
        "        results.artifact_paths[\"broker_comparison\"] = manager.save_object(broker_comparison, \"broker_comparison\", \"08_analysis\")\n",
        "\n",
        "        # Task 23: Hewlett Network\n",
        "        hewlett_network = characterize_hewlett_network(\n",
        "            tx_clean, subgroup_stats, results.artifacts[\"canonical_mapping\"], solar_system\n",
        "        )\n",
        "        results.artifacts[\"hewlett_network\"] = hewlett_network\n",
        "        results.artifact_paths[\"hewlett_network\"] = manager.save_object(hewlett_network, \"hewlett_network\", \"08_analysis\")\n",
        "\n",
        "        # Task 24: Web Graph\n",
        "        web_graph = construct_web_graph(df_web_links_raw, org_mapped)\n",
        "        results.artifacts[\"web_graph\"] = web_graph\n",
        "        results.artifact_paths[\"web_graph\"] = manager.save_object(web_graph, \"web_graph\", \"09_validation\")\n",
        "\n",
        "        # Task 25: PageRank\n",
        "        pagerank = compute_website_pagerank(web_graph, config)\n",
        "        results.artifacts[\"pagerank\"] = pagerank\n",
        "        results.artifact_paths[\"pagerank\"] = manager.save_object(pagerank, \"pagerank\", \"09_validation\")\n",
        "\n",
        "        # Task 26: Correlation\n",
        "        correlation = compute_validation_correlation(\n",
        "            centrality_metrics, pagerank, org_mapped\n",
        "        )\n",
        "        results.artifacts[\"correlation\"] = correlation\n",
        "        results.artifact_paths[\"correlation\"] = manager.save_object(correlation, \"correlation\", \"09_validation\")\n",
        "\n",
        "        results.success = True\n",
        "        results.end_time = time.ctime()\n",
        "        logger.info(\"=== Pipeline Completed Successfully ===\")\n",
        "\n",
        "    except Exception as e:\n",
        "        results.success = False\n",
        "        results.end_time = time.ctime()\n",
        "        results.error_message = str(e)\n",
        "        logger.error(f\"Pipeline Failed: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "        # Attempt to save partial results\n",
        "        try:\n",
        "            manager.save_object(results, \"partial_results_dump\", \"failures\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "QmMn6nfc74E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 28 — Conduct robustness analysis\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 28: Conduct robustness analysis\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class RobustnessArtifact:\n",
        "    \"\"\"\n",
        "    Container for robustness analysis results.\n",
        "\n",
        "    This dataclass encapsulates the results of the sensitivity analysis, which\n",
        "    systematically perturbs key methodological parameters (entity resolution\n",
        "    thresholds, random walk biases, projection contexts) to assess the stability\n",
        "    of the study's core findings. It includes a detailed table of metrics for\n",
        "    each simulation run, as well as aggregated summary statistics quantifying\n",
        "    the variance in rank ordering and external validation correlation.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    sensitivity_table : pd.DataFrame\n",
        "        A DataFrame where each row corresponds to a single simulation run. Columns\n",
        "        include the parameter settings (tau, p, q, context), the resulting\n",
        "        Spearman rank correlation with the baseline, the Pearson validation\n",
        "        correlation, and the Jaccard overlap of the Top 25 core.\n",
        "    rank_stability_summary : Dict[str, float]\n",
        "        A dictionary of summary statistics (mean, min, std) for the Spearman\n",
        "        rank correlation across all runs, indicating the overall robustness of\n",
        "        the centrality rankings.\n",
        "    correlation_stability_summary : Dict[str, float]\n",
        "        A dictionary of summary statistics for the Pearson validation correlation,\n",
        "        indicating the stability of the external validity check.\n",
        "    \"\"\"\n",
        "    # Detailed metrics per run\n",
        "    sensitivity_table: pd.DataFrame\n",
        "\n",
        "    # Summary of rank stability\n",
        "    rank_stability_summary: Dict[str, float]\n",
        "\n",
        "    # Summary of correlation stability\n",
        "    correlation_stability_summary: Dict[str, float]\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the robustness analysis.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the number of scenarios and mean stability.\n",
        "        \"\"\"\n",
        "        # Format summary string\n",
        "        return (f\"Robustness: Analyzed {len(self.sensitivity_table)} scenarios. \"\n",
        "                f\"Mean Rank Correlation: {self.rank_stability_summary['mean_spearman']:.4f}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Step 1: Define robustness perturbation axes\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_perturbation_grid(\n",
        "    baseline_config: Dict[str, Any]\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Generates a list of configuration dictionaries representing the robustness grid.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    baseline_config : Dict[str, Any]\n",
        "        The baseline configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[Dict[str, Any]]\n",
        "        List of perturbed configurations.\n",
        "    \"\"\"\n",
        "    # Define axes\n",
        "    er_thresholds = [0.80, 0.85, 0.90]\n",
        "    p_values = [0.5, 1.0, 2.0]\n",
        "    q_values = [0.5, 1.0, 2.0]\n",
        "    context_modes = [\"both\", \"country_only\", \"sector_only\"]\n",
        "\n",
        "    grid = list(itertools.product(er_thresholds, p_values, q_values, context_modes))\n",
        "\n",
        "    configs = []\n",
        "    for i, (tau, p, q, ctx) in enumerate(grid):\n",
        "        # Deep copy baseline to avoid mutation\n",
        "        # (In production use copy.deepcopy, here we construct dict)\n",
        "        new_config = baseline_config.copy()\n",
        "\n",
        "        # Update ER\n",
        "        if \"ENTITY_RESOLUTION\" not in new_config: new_config[\"ENTITY_RESOLUTION\"] = {}\n",
        "        if \"fuzzy_matching\" not in new_config[\"ENTITY_RESOLUTION\"]: new_config[\"ENTITY_RESOLUTION\"][\"fuzzy_matching\"] = {}\n",
        "        new_config[\"ENTITY_RESOLUTION\"][\"fuzzy_matching\"][\"threshold\"] = tau\n",
        "\n",
        "        # Update Node2Vec\n",
        "        if \"NODE2VEC\" not in new_config: new_config[\"NODE2VEC\"] = {}\n",
        "        new_config[\"NODE2VEC\"][\"p\"] = p\n",
        "        new_config[\"NODE2VEC\"][\"q\"] = q\n",
        "\n",
        "        # Update Projection Contexts\n",
        "        if \"NETWORKS\" not in new_config: new_config[\"NETWORKS\"] = {}\n",
        "        if \"projection\" not in new_config[\"NETWORKS\"]: new_config[\"NETWORKS\"][\"projection\"] = {}\n",
        "        if \"context_definition\" not in new_config[\"NETWORKS\"][\"projection\"]: new_config[\"NETWORKS\"][\"projection\"][\"context_definition\"] = {}\n",
        "\n",
        "        if ctx == \"both\":\n",
        "            new_config[\"NETWORKS\"][\"projection\"][\"context_definition\"][\"use_country\"] = True\n",
        "            new_config[\"NETWORKS\"][\"projection\"][\"context_definition\"][\"use_sector\"] = True\n",
        "        elif ctx == \"country_only\":\n",
        "            new_config[\"NETWORKS\"][\"projection\"][\"context_definition\"][\"use_country\"] = True\n",
        "            new_config[\"NETWORKS\"][\"projection\"][\"context_definition\"][\"use_sector\"] = False\n",
        "        elif ctx == \"sector_only\":\n",
        "            new_config[\"NETWORKS\"][\"projection\"][\"context_definition\"][\"use_country\"] = False\n",
        "            new_config[\"NETWORKS\"][\"projection\"][\"context_definition\"][\"use_sector\"] = True\n",
        "\n",
        "        # Tag run\n",
        "        new_config[\"run_tag\"] = f\"robustness_{i}_tau{tau}_p{p}_q{q}_{ctx}\"\n",
        "        configs.append(new_config)\n",
        "\n",
        "    logger.info(f\"Robustness Grid: Generated {len(configs)} configurations.\")\n",
        "    return configs\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Step 2: Execute orchestrator across perturbation grid\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_robustness_batch(\n",
        "    configs: List[Dict[str, Any]],\n",
        "    pipeline_func: Any, # run_global_aid_pipeline\n",
        "    data_inputs: Dict[str, pd.DataFrame]\n",
        ") -> List[Any]: # List[PipelineResults]\n",
        "    \"\"\"\n",
        "    Executes the pipeline for each configuration in the grid.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    configs : List[Dict[str, Any]]\n",
        "        List of configs.\n",
        "    pipeline_func : Callable\n",
        "        The orchestrator function.\n",
        "    data_inputs : Dict[str, pd.DataFrame]\n",
        "        Raw data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[PipelineResults]\n",
        "        Results for each run.\n",
        "    \"\"\"\n",
        "    results_list = []\n",
        "\n",
        "    for cfg in configs:\n",
        "        tag = cfg.get(\"run_tag\", \"unknown\")\n",
        "        logger.info(f\"Robustness Run: Starting {tag}...\")\n",
        "\n",
        "        try:\n",
        "            # Execute pipeline\n",
        "            # We pass output_dir specific to run to avoid overwrites\n",
        "            res = pipeline_func(\n",
        "                data_inputs[\"df_transactions_raw\"],\n",
        "                data_inputs[\"df_activities_raw\"],\n",
        "                data_inputs[\"df_organisations_raw\"],\n",
        "                data_inputs[\"df_web_links_raw\"],\n",
        "                cfg,\n",
        "                output_dir=f\"./output/{tag}\"\n",
        "            )\n",
        "            results_list.append(res)\n",
        "            logger.info(f\"Robustness Run: {tag} Success={res.success}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Robustness Run: {tag} Failed: {e}\")\n",
        "            # Append None or failed result object\n",
        "            results_list.append(None)\n",
        "\n",
        "    return results_list\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Step 3: Synthesize robustness report\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def synthesize_robustness_results(\n",
        "    baseline_results: Any, # PipelineResults\n",
        "    robustness_results: List[Any] # List[PipelineResults]\n",
        ") -> RobustnessArtifact:\n",
        "    \"\"\"\n",
        "    Synthesizes metrics from robustness runs, comparing them to the baseline.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    baseline_results : PipelineResults\n",
        "        The baseline run.\n",
        "    robustness_results : List[PipelineResults]\n",
        "        The batch runs.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    RobustnessArtifact\n",
        "        Summary analysis.\n",
        "    \"\"\"\n",
        "    # Extract Baseline Metrics\n",
        "    base_ranks = baseline_results.solar_system.ranked_nodes[['rank']].copy()\n",
        "    base_r = baseline_results.correlation.pearson_r\n",
        "\n",
        "    metrics = []\n",
        "\n",
        "    for res in robustness_results:\n",
        "        if not res or not res.success:\n",
        "            continue\n",
        "\n",
        "        # Extract Run Metrics\n",
        "        # 1. Rank Correlation\n",
        "        run_ranks = res.solar_system.ranked_nodes[['rank']]\n",
        "\n",
        "        # Align on index (canonical_id)\n",
        "        # Inner join to compare common nodes\n",
        "        common = base_ranks.join(run_ranks, lsuffix='_base', rsuffix='_run', how='inner')\n",
        "\n",
        "        if len(common) > 10:\n",
        "            spearman_rho, _ = stats.spearmanr(common['rank_base'], common['rank_run'])\n",
        "        else:\n",
        "            spearman_rho = np.nan\n",
        "\n",
        "        # 2. Validation Correlation\n",
        "        run_r = res.correlation.pearson_r\n",
        "\n",
        "        # 3. Top 25 Overlap (Jaccard)\n",
        "        base_top25 = set(base_ranks[base_ranks['rank_base'] <= 25].index)\n",
        "        run_top25 = set(run_ranks[run_ranks['rank'] <= 25].index)\n",
        "        overlap = len(base_top25.intersection(run_top25))\n",
        "        jaccard = overlap / len(base_top25.union(run_top25))\n",
        "\n",
        "        metrics.append({\n",
        "            \"run_id\": res.run_id,\n",
        "            \"spearman_rank_corr\": spearman_rho,\n",
        "            \"pearson_validation_r\": run_r,\n",
        "            \"top25_jaccard\": jaccard\n",
        "        })\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics)\n",
        "\n",
        "    # Summarize\n",
        "    rank_stats = {\n",
        "        \"mean_spearman\": df_metrics['spearman_rank_corr'].mean(),\n",
        "        \"min_spearman\": df_metrics['spearman_rank_corr'].min(),\n",
        "        \"std_spearman\": df_metrics['spearman_rank_corr'].std()\n",
        "    }\n",
        "\n",
        "    corr_stats = {\n",
        "        \"mean_pearson\": df_metrics['pearson_validation_r'].mean(),\n",
        "        \"min_pearson\": df_metrics['pearson_validation_r'].min(),\n",
        "        \"std_pearson\": df_metrics['pearson_validation_r'].std()\n",
        "    }\n",
        "\n",
        "    artifact = RobustnessArtifact(\n",
        "        sensitivity_table=df_metrics,\n",
        "        rank_stability_summary=rank_stats,\n",
        "        correlation_stability_summary=corr_stats\n",
        "    )\n",
        "\n",
        "    logger.info(str(artifact))\n",
        "\n",
        "    return artifact\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def conduct_robustness_analysis(\n",
        "    baseline_results: Any, # PipelineResults\n",
        "    baseline_config: Dict[str, Any],\n",
        "    pipeline_func: Any,\n",
        "    data_inputs: Dict[str, pd.DataFrame]\n",
        ") -> RobustnessArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 28. Runs the robustness analysis.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    baseline_results : PipelineResults\n",
        "        Baseline output.\n",
        "    baseline_config : Dict[str, Any]\n",
        "        Baseline config.\n",
        "    pipeline_func : Callable\n",
        "        Orchestrator.\n",
        "    data_inputs : Dict[str, pd.DataFrame]\n",
        "        Raw data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    RobustnessArtifact\n",
        "        Analysis.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 28: Robustness Analysis...\")\n",
        "\n",
        "    # Step 1: Grid\n",
        "    configs = generate_perturbation_grid(baseline_config)\n",
        "\n",
        "    # Step 2: Execute\n",
        "    run_results = execute_robustness_batch(configs, pipeline_func, data_inputs)\n",
        "\n",
        "    # Step 3: Synthesize\n",
        "    artifact = synthesize_robustness_results(baseline_results, run_results)\n",
        "\n",
        "    logger.info(\"Task 28 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "62LBLaNHDFgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 29 — Package reproducible outputs and provenance\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 29: Package reproducible outputs and provenance\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ProvenanceArtifact:\n",
        "    \"\"\"\n",
        "    Container for the full reproducibility manifest.\n",
        "\n",
        "    This dataclass serves as the master record for a single execution of the\n",
        "    Global Aid Network analysis pipeline. It aggregates all necessary information\n",
        "    required to audit, validate, and reproduce the study's findings. By linking\n",
        "    the input configuration, the cryptographic hashes of all generated artifacts,\n",
        "    and the high-level summary metrics, this object provides a complete lineage\n",
        "    trace from raw data to final manuscript figures.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    run_id : str\n",
        "        The unique identifier for this pipeline execution (e.g., \"run_167889234\"),\n",
        "        used to namespace the output directory and log files.\n",
        "    timestamp : str\n",
        "        The ISO 8601 formatted timestamp indicating when the packaging process\n",
        "        was completed.\n",
        "    config : Dict[str, Any]\n",
        "        The complete configuration dictionary used to drive the pipeline, including\n",
        "        all parameters for entity resolution, graph construction, and dimensionality\n",
        "        reduction. This ensures that the exact parameter set is preserved.\n",
        "    artifact_manifest : Dict[str, str]\n",
        "        A dictionary mapping relative file paths of persisted artifacts to their\n",
        "        SHA-256 checksums. This allows for the verification of file integrity\n",
        "        and ensures that the outputs have not been tampered with post-generation.\n",
        "    metadata_summary : Dict[str, Any]\n",
        "        A consolidated dictionary of key metrics extracted from all pipeline stages\n",
        "        (e.g., data quality rates, HITS convergence stats, validation correlations).\n",
        "        This provides a high-level overview of the run's analytical performance.\n",
        "    \"\"\"\n",
        "    # Unique execution identifier\n",
        "    run_id: str\n",
        "\n",
        "    # Execution timestamp (ISO 8601)\n",
        "    timestamp: str\n",
        "\n",
        "    # Input configuration parameters\n",
        "    config: Dict[str, Any]\n",
        "\n",
        "    # Map of artifact paths to SHA-256 hashes\n",
        "    artifact_manifest: Dict[str, str]\n",
        "\n",
        "    # Aggregated metrics from all pipeline stages\n",
        "    metadata_summary: Dict[str, Any]\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a formatted string summary of the provenance artifact.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary including the run ID and the count of\n",
        "            tracked files in the manifest.\n",
        "        \"\"\"\n",
        "        # Format summary string for logging\n",
        "        return f\"Provenance: Run {self.run_id}, {len(self.artifact_manifest)} files tracked.\"\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 29, Step 1: Persist all intermediate and final artifacts\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_file_hash(file_path: Path) -> str:\n",
        "    \"\"\"\n",
        "    Computes the SHA-256 cryptographic hash of a file in a memory-efficient manner.\n",
        "\n",
        "    This utility function is essential for generating the artifact manifest,\n",
        "    ensuring the integrity and reproducibility of the pipeline's outputs. By\n",
        "    producing a unique checksum for every persisted file, we allow downstream\n",
        "    consumers to verify that the data has not been corrupted or altered since\n",
        "    generation. The implementation uses buffered reading to handle potentially\n",
        "    large artifacts (e.g., the 10-million-row transaction table) without\n",
        "    exhausting system memory.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_path : Path\n",
        "        The filesystem path to the file to be hashed.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The hexadecimal representation of the SHA-256 hash.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    FileNotFoundError\n",
        "        If the specified `file_path` does not exist.\n",
        "    IsADirectoryError\n",
        "        If `file_path` points to a directory instead of a file.\n",
        "    PermissionError\n",
        "        If the process lacks read permissions for the file.\n",
        "    \"\"\"\n",
        "    # Input Validation: Ensure the path exists and is a file\n",
        "    if not file_path.exists():\n",
        "        raise FileNotFoundError(f\"Cannot hash file: Path '{file_path}' does not exist.\")\n",
        "    if not file_path.is_file():\n",
        "        raise IsADirectoryError(f\"Cannot hash file: Path '{file_path}' is a directory.\")\n",
        "\n",
        "    # Initialize the SHA-256 hash object from the hashlib library\n",
        "    sha256_hash = hashlib.sha256()\n",
        "\n",
        "    try:\n",
        "        # Open the file in binary read mode ('rb') to handle all file types (text, parquet, pickle)\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            # Read the file in fixed-size chunks (4096 bytes) to maintain low memory footprint\n",
        "            # iter() with a sentinel (b\"\") creates an iterator that stops at EOF\n",
        "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
        "                # Update the hash object with the current chunk of data\n",
        "                sha256_hash.update(byte_block)\n",
        "\n",
        "    except OSError as e:\n",
        "        # Handle potential I/O errors during the read process\n",
        "        raise OSError(f\"Error reading file '{file_path}' for hashing: {e}\") from e\n",
        "\n",
        "    # Return the final digest as a hexadecimal string\n",
        "    return sha256_hash.hexdigest()\n",
        "\n",
        "def create_artifact_manifest(\n",
        "    pipeline_results: Any, # PipelineResults\n",
        "    output_dir: Path\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Generates a manifest of all persisted artifacts with their hashes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pipeline_results : PipelineResults\n",
        "        The results object containing paths.\n",
        "    output_dir : Path\n",
        "        Root output directory.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, str]\n",
        "        Dictionary mapping relative file paths to SHA-256 hashes.\n",
        "    \"\"\"\n",
        "    manifest = {}\n",
        "\n",
        "    # Iterate over paths stored in results\n",
        "    for name, path in pipeline_results.artifact_paths.items():\n",
        "        if path.exists():\n",
        "            # Compute hash\n",
        "            file_hash = generate_file_hash(path)\n",
        "            # Store relative path\n",
        "            rel_path = str(path.relative_to(output_dir))\n",
        "            manifest[rel_path] = file_hash\n",
        "\n",
        "    logger.info(f\"Manifest: Tracked {len(manifest)} artifacts.\")\n",
        "    return manifest\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 29, Step 2: Persist complete method metadata\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def consolidate_metadata(\n",
        "    pipeline_results: Any, # PipelineResults\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Aggregates metadata from all pipeline stages into a single dictionary.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pipeline_results : PipelineResults\n",
        "        The results.\n",
        "    config : Dict[str, Any]\n",
        "        The input config.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Consolidated metadata.\n",
        "    \"\"\"\n",
        "    meta = {\n",
        "        \"run_id\": pipeline_results.run_id,\n",
        "        \"start_time\": pipeline_results.start_time,\n",
        "        \"end_time\": pipeline_results.end_time,\n",
        "        \"status\": \"Success\" if pipeline_results.success else \"Failed\",\n",
        "        \"config\": config,\n",
        "        \"metrics\": {}\n",
        "    }\n",
        "\n",
        "    # Extract specific metrics from artifacts if available\n",
        "    artifacts = pipeline_results.artifacts\n",
        "\n",
        "    if \"dq_report\" in artifacts:\n",
        "        meta[\"metrics\"][\"data_quality\"] = artifacts[\"dq_report\"]\n",
        "\n",
        "    if \"reproducibility_context\" in artifacts:\n",
        "        # Extract seeds\n",
        "        meta[\"seeds\"] = artifacts[\"reproducibility_context\"].seeds\n",
        "\n",
        "    if \"activity_coverage\" in artifacts:\n",
        "        cov = artifacts[\"activity_coverage\"]\n",
        "        meta[\"metrics\"][\"activity_coverage\"] = {\n",
        "            \"total\": cov.total_activities,\n",
        "            \"country_rate\": cov.country_coverage_rate\n",
        "        }\n",
        "\n",
        "    if \"context_coverage\" in artifacts:\n",
        "        cov = artifacts[\"context_coverage\"]\n",
        "        meta[\"metrics\"][\"transaction_coverage\"] = {\n",
        "            \"total\": cov.total_transactions,\n",
        "            \"country_rate\": cov.transactions_with_country / cov.total_transactions if cov.total_transactions else 0\n",
        "        }\n",
        "\n",
        "    if \"canonical_mapping\" in artifacts:\n",
        "        meta[\"metrics\"][\"entity_resolution\"] = {\n",
        "            \"clusters\": artifacts[\"canonical_mapping\"].cluster_count,\n",
        "            \"method\": artifacts[\"canonical_mapping\"].resolution_method\n",
        "        }\n",
        "\n",
        "    if \"hits_scores\" in artifacts:\n",
        "        meta[\"metrics\"][\"hits_convergence\"] = artifacts[\"hits_scores\"].convergence_meta\n",
        "\n",
        "    if \"pagerank\" in artifacts:\n",
        "        meta[\"metrics\"][\"pagerank_convergence\"] = artifacts[\"pagerank\"].convergence_meta\n",
        "\n",
        "    if \"correlation\" in artifacts:\n",
        "        corr = artifacts[\"correlation\"]\n",
        "        meta[\"metrics\"][\"validation\"] = {\n",
        "            \"r\": corr.pearson_r,\n",
        "            \"p\": corr.p_value,\n",
        "            \"n\": corr.sample_size\n",
        "        }\n",
        "\n",
        "    return meta\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 29, Step 3: Generate manuscript-facing outputs deterministically\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_manuscript_tables(\n",
        "    pipeline_results: Any, # PipelineResults\n",
        "    output_dir: Path\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Extracts and saves the specific tables required for manuscript figures.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pipeline_results : PipelineResults\n",
        "        The results.\n",
        "    output_dir : Path\n",
        "        Output directory.\n",
        "    \"\"\"\n",
        "    tables_dir = output_dir / \"manuscript_tables\"\n",
        "    tables_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    artifacts = pipeline_results.artifacts\n",
        "\n",
        "    # Fig 1: Geo Density\n",
        "    if \"geo_density\" in artifacts:\n",
        "        df = artifacts[\"geo_density\"].density_table\n",
        "        df.to_csv(tables_dir / \"fig1_geo_density.csv\", index=False)\n",
        "\n",
        "    # Fig S1: Instrument Evolution\n",
        "    if \"instrument_evolution\" in artifacts:\n",
        "        df = artifacts[\"instrument_evolution\"].time_series_table\n",
        "        df.to_csv(tables_dir / \"figS1_instrument_evolution.csv\", index=True)\n",
        "\n",
        "    # Fig 2: UMAP\n",
        "    if \"umap_projection\" in artifacts:\n",
        "        df = artifacts[\"umap_projection\"].coordinates\n",
        "        # Join with org type for coloring\n",
        "        if \"solar_system\" in artifacts:\n",
        "            ranks = artifacts[\"solar_system\"].ranked_nodes[['org_type', 'node_size']]\n",
        "            df = df.join(ranks, how='left')\n",
        "        df.to_csv(tables_dir / \"fig2_umap_coords.csv\", index=True)\n",
        "\n",
        "    # Fig 3: Solar System\n",
        "    if \"solar_system\" in artifacts:\n",
        "        df = artifacts[\"solar_system\"].top_100_table\n",
        "        df.to_csv(tables_dir / \"fig3_solar_system_top100.csv\", index=True)\n",
        "\n",
        "    # Fig S3: Broker Comparison\n",
        "    if \"broker_comparison\" in artifacts:\n",
        "        df = artifacts[\"broker_comparison\"].comparison_table\n",
        "        df.to_csv(tables_dir / \"figS3_broker_comparison.csv\", index=True)\n",
        "\n",
        "    logger.info(f\"Manuscript Tables: Saved to {tables_dir}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 29, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def package_reproducible_outputs(\n",
        "    pipeline_results: Any, # PipelineResults\n",
        "    config: Dict[str, Any],\n",
        "    output_root: str\n",
        ") -> ProvenanceArtifact:\n",
        "    \"\"\"\n",
        "    Orchestrator for Task 29. Packages all outputs and generates provenance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pipeline_results : PipelineResults\n",
        "        Pipeline output.\n",
        "    config : Dict[str, Any]\n",
        "        Config.\n",
        "    output_root : str\n",
        "        Root directory used for the run.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ProvenanceArtifact\n",
        "        The final provenance object.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 29: Packaging Outputs...\")\n",
        "\n",
        "    run_dir = Path(output_root) / pipeline_results.run_id\n",
        "\n",
        "    # Step 1: Manifest\n",
        "    manifest = create_artifact_manifest(pipeline_results, Path(output_root))\n",
        "\n",
        "    # Step 2: Metadata\n",
        "    meta = consolidate_metadata(pipeline_results, config)\n",
        "\n",
        "    # Save metadata json\n",
        "    with open(run_dir / \"provenance.json\", \"w\") as f:\n",
        "        json.dump(meta, f, indent=2, default=str)\n",
        "\n",
        "    # Step 3: Manuscript Tables\n",
        "    generate_manuscript_tables(pipeline_results, run_dir)\n",
        "\n",
        "    artifact = ProvenanceArtifact(\n",
        "        run_id=pipeline_results.run_id,\n",
        "        timestamp=datetime.datetime.now().isoformat(),\n",
        "        config=config,\n",
        "        artifact_manifest=manifest,\n",
        "        metadata_summary=meta\n",
        "    )\n",
        "\n",
        "    logger.info(\"Task 29 Completed Successfully.\")\n",
        "\n",
        "    return artifact\n"
      ],
      "metadata": {
        "id": "AzdNmCXbLnrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator Callable\n",
        "\n",
        "# ==============================================================================\n",
        "# Master Orchestrator: Global Aid Network Analysis\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class MasterWorkflowResults:\n",
        "    \"\"\"\n",
        "    Container for the outputs of the entire research workflow.\n",
        "\n",
        "    This dataclass aggregates the results from the three primary phases of the\n",
        "    study: the baseline analysis pipeline, the robustness/sensitivity analysis,\n",
        "    and the final provenance packaging. It serves as the single return object\n",
        "    for the master orchestrator, providing access to every artifact generated\n",
        "    during the execution.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    baseline_results : PipelineResults\n",
        "        The complete set of artifacts and metadata from the primary (baseline)\n",
        "        execution of the analysis pipeline (Task 27).\n",
        "    robustness_results : RobustnessArtifact\n",
        "        The summary statistics and detailed metrics from the sensitivity analysis,\n",
        "        quantifying the stability of the findings under parameter perturbation (Task 28).\n",
        "    provenance_artifact : ProvenanceArtifact\n",
        "        The final reproducibility manifest, including cryptographic hashes of all\n",
        "        persisted files and a consolidated metadata summary (Task 29).\n",
        "    execution_time : float\n",
        "        The total wall-clock time (in seconds) taken to execute the master workflow.\n",
        "    success : bool\n",
        "        Global success flag indicating if all phases completed without critical error.\n",
        "    \"\"\"\n",
        "    baseline_results: Any # PipelineResults\n",
        "    robustness_results: Any # RobustnessArtifact\n",
        "    provenance_artifact: Any # ProvenanceArtifact\n",
        "    execution_time: float\n",
        "    success: bool = False\n",
        "\n",
        "def execute_master_workflow(\n",
        "    df_transactions_raw: pd.DataFrame,\n",
        "    df_activities_raw: pd.DataFrame,\n",
        "    df_organisations_raw: pd.DataFrame,\n",
        "    df_web_links_raw: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    output_root: str = \"./study_output\"\n",
        ") -> MasterWorkflowResults:\n",
        "    \"\"\"\n",
        "    Executes the complete Global Aid Network research workflow.\n",
        "\n",
        "    This master orchestrator coordinates the three high-level components of the\n",
        "    study:\n",
        "    1.  **Baseline Analysis (Task 27)**: Runs the end-to-end pipeline using the\n",
        "        primary configuration to generate the core findings (Solar System,\n",
        "        Brokerage, etc.).\n",
        "    2.  **Robustness Analysis (Task 28)**: Performs a sensitivity analysis by\n",
        "        perturbing key parameters (ER threshold, Node2Vec bias) and re-running\n",
        "        the pipeline to assess the stability of rankings and correlations.\n",
        "    3.  **Provenance Packaging (Task 29)**: Consolidates all outputs, generates\n",
        "        cryptographic manifests, and produces the final manuscript-ready tables.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_transactions_raw : pd.DataFrame\n",
        "        Raw IATI transaction data.\n",
        "    df_activities_raw : pd.DataFrame\n",
        "        Raw IATI activity metadata.\n",
        "    df_organisations_raw : pd.DataFrame\n",
        "        Raw organisation master list.\n",
        "    df_web_links_raw : pd.DataFrame\n",
        "        Raw web crawl data for validation.\n",
        "    config : Dict[str, Any]\n",
        "        Master configuration dictionary.\n",
        "    output_root : str\n",
        "        Root directory for all study outputs.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    MasterWorkflowResults\n",
        "        A container holding the baseline results, robustness analysis, and\n",
        "        provenance metadata.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    logger.info(\"=== Initiating Master Workflow ===\")\n",
        "\n",
        "    # Initialize container with None\n",
        "    results = MasterWorkflowResults(\n",
        "        baseline_results=None,\n",
        "        robustness_results=None,\n",
        "        provenance_artifact=None,\n",
        "        execution_time=0.0,\n",
        "        success=False\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Phase 1: Baseline Pipeline Execution (Task 27)\n",
        "        # ---------------------------------------------------------------------\n",
        "        logger.info(\">>> Phase 1: Executing Baseline Pipeline...\")\n",
        "\n",
        "        # Define baseline output directory\n",
        "        baseline_dir = f\"{output_root}/baseline\"\n",
        "\n",
        "        baseline_res = run_global_aid_pipeline(\n",
        "            df_transactions_raw,\n",
        "            df_activities_raw,\n",
        "            df_organisations_raw,\n",
        "            df_web_links_raw,\n",
        "            config,\n",
        "            output_dir=baseline_dir\n",
        "        )\n",
        "\n",
        "        results.baseline_results = baseline_res\n",
        "\n",
        "        if not baseline_res.success:\n",
        "            raise RuntimeError(f\"Baseline pipeline failed: {baseline_res.error_message}\")\n",
        "\n",
        "        logger.info(\">>> Phase 1 Complete.\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Phase 2: Robustness Analysis (Task 28)\n",
        "        # ---------------------------------------------------------------------\n",
        "        logger.info(\">>> Phase 2: Conducting Robustness Analysis...\")\n",
        "\n",
        "        # Prepare data inputs dictionary for the robustness orchestrator\n",
        "        data_inputs = {\n",
        "            \"df_transactions_raw\": df_transactions_raw,\n",
        "            \"df_activities_raw\": df_activities_raw,\n",
        "            \"df_organisations_raw\": df_organisations_raw,\n",
        "            \"df_web_links_raw\": df_web_links_raw\n",
        "        }\n",
        "\n",
        "        # Execute robustness analysis\n",
        "        # Note: We pass the run_global_aid_pipeline function itself to allow\n",
        "        # the robustness orchestrator to invoke it for each grid point.\n",
        "        robustness_res = conduct_robustness_analysis(\n",
        "            baseline_res,\n",
        "            config,\n",
        "            run_global_aid_pipeline,\n",
        "            data_inputs\n",
        "        )\n",
        "\n",
        "        results.robustness_results = robustness_res\n",
        "        logger.info(\">>> Phase 2 Complete.\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Phase 3: Provenance Packaging (Task 29)\n",
        "        # ---------------------------------------------------------------------\n",
        "        logger.info(\">>> Phase 3: Packaging Provenance...\")\n",
        "\n",
        "        # Package outputs based on the baseline run\n",
        "        # (Robustness results are typically summarized in the manuscript text/tables\n",
        "        # rather than requiring full artifact tracking for every sub-run in the main manifest,\n",
        "        # though the robustness artifact itself is tracked).\n",
        "\n",
        "        provenance_res = package_reproducible_outputs(\n",
        "            baseline_res,\n",
        "            config,\n",
        "            baseline_dir # Point to where baseline artifacts were saved\n",
        "        )\n",
        "\n",
        "        results.provenance_artifact = provenance_res\n",
        "        logger.info(\">>> Phase 3 Complete.\")\n",
        "\n",
        "        # Finalize\n",
        "        results.success = True\n",
        "        logger.info(\"=== Master Workflow Completed Successfully ===\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Master Workflow Failed: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        results.success = False\n",
        "\n",
        "    finally:\n",
        "        results.execution_time = time.time() - start_time\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "SlOIm3mmcyKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "zLyoRZsB6qMy"
      }
    }
  ]
}