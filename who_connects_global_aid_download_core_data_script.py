# -*- coding: utf-8 -*-
"""who_connects_global_aid_download_core_data_script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J7F7urkoF0vnJLHpK5WNoxGFpb_GTnxH
"""



"""# Paper

Title: "*Who Connects Global Aid? The Hidden Geometry of 10 Million Transactions*"

Authors: Paul X. McCarthy, Xian Gong, Marian-Andrei Rizoiu, Paolo Boldi

E-Journal Submission Date: 19 December 2025

Link: https://arxiv.org/abs/2512.17243

Abstract:

The global aid system functions as a complex and evolving ecosystem; yet widespread understanding of its structure remains largely limited to aggregate volume flows. Here we map the network topology of global aid using a dataset of unprecedented scale: over 10 million transaction records connecting 2,456 publishing organisations across 230 countries between 1967 and 2025. We apply bipartite projection and dimensionality reduction to reveal the geometry of the system and unveil hidden patterns. This exposes distinct functional clusters that are otherwise sparsely connected. We find that while governments and multilateral agencies provide the primary resources, a small set of knowledge brokers provide the critical connectivity. Universities and research foundations specifically act as essential bridges between disparate islands of implementers and funders. We identify a core solar system of 25 central actors who drive this connectivity including unanticipated brokers like J-PAL and the Hewlett Foundation. These findings demonstrate that influence in the aid ecosystem flows through structural connectivity as much as financial volume. Our results provide a new framework for donors to identify strategic partners that accelerate coordination and evidence diffusion across the global network.
"""

"""
IATI Datastore API Client Module
================================

This module provides a robust, production-grade client for bulk downloading
transaction data from the International Aid Transparency Initiative (IATI)
Datastore API v3. It is designed to support large-scale network analysis
of global aid flows as described in:

    McCarthy, P.X., Gong, X., Rizoiu, M.-A., & Boldi, P. (2025).
    "Who Connects Global Aid? The Hidden Geometry of 10 Million Transactions."
    arXiv:2512.17243

The module implements:
    - Configurable HTTP client with exponential backoff retry logic
    - SOLR-style cursor mark pagination for memory-efficient streaming
    - Fallback start/rows pagination for API compatibility
    - Parquet-based sharded storage for downstream processing
    - Provenance metadata for reproducibility

Architecture Notes:
    The IATI Datastore v3 exposes a SOLR-based search interface. For bulk
    extraction of N > 10^7 documents, cursor-based pagination is preferred
    over offset pagination to avoid deep-paging performance degradation
    (O(N) vs O(N log N) for sorted result sets).

Minimum Computational Requirements:
    The following hardware specifications are required for full dataset
    processing. Insufficient resources may result in out-of-memory errors
    or degraded performance during bulk download and serialization.

    +----------------+-------------------+
    | Component      | Minimum Capacity  |
    +----------------+-------------------+
    | GPU HBM        | 128+ GB           |
    | System RAM     | 50+ GB            |
    | Disk Storage   | ~150+ GB          |
    +----------------+-------------------+

    Note: For smaller subsets or filtered queries, requirements scale
    proportionally with the number of documents retrieved.

Dependencies:
    - requests: HTTP client with session management
    - pandas: DataFrame construction and Parquet I/O
    - urllib3: Retry strategy implementation

Author: CS Chirinda
License: MIT
Version: 1.0.0
"""

# ==============================================================================
# IMPORTS
# ==============================================================================

# Enable postponed evaluation of annotations for forward references (PEP 563).
from __future__ import annotations

# Standard library imports for data structures and configuration.
import dataclasses
import json
import logging
import math
import os
import time

# Dataclass decorator for immutable configuration objects.
from dataclasses import dataclass

# Datetime utilities for UTC timestamp generation in metadata.
from datetime import datetime, timezone

# Path abstraction for cross-platform filesystem operations.
from pathlib import Path

# Type hints for static analysis and documentation.
from typing import Any, Dict, Iterable, Iterator, List, Optional, Sequence, Tuple

# Pandas for DataFrame operations and Parquet serialization.
import pandas as pd

# HTTP client library with session management and connection pooling.
import requests

# HTTP adapter for mounting custom retry strategies to sessions.
from requests.adapters import HTTPAdapter

# Retry strategy with exponential backoff for transient failure handling.
from urllib3.util.retry import Retry

# ==============================================================================
# MODULE-LEVEL LOGGER CONFIGURATION
# ==============================================================================

# Initialize module-level logger following Python logging best practices.
LOGGER: logging.Logger = logging.getLogger(__name__)

# ==============================================================================
# API Key
# ==============================================================================

# Set API Key International Aid Transparency Initiative "Full Access" API Key. Subscribe here: https://developer.iatistandard.org/
os.environ["IATI_SUBSCRIPTION_KEY"] = "paste_api_key_here"

# ==============================================================================
# Data Download Utilities
# ==============================================================================

@dataclass(frozen=True)
class DatastoreConfig:
    """
    Immutable configuration container for IATI Datastore API v3 access.

    This frozen dataclass encapsulates all parameters required to establish
    and maintain a reliable connection to the IATI Datastore API. The frozen
    attribute ensures thread-safety and prevents accidental mutation during
    long-running bulk download operations.

    The IATI Datastore v3 API is hosted on Azure API Management and requires
    an Ocp-Apim-Subscription-Key header for authentication. Rate limiting
    and retry behavior are configurable to accommodate varying network
    conditions and API quotas.

    Attributes
    ----------
    subscription_key : str
        Azure API Management subscription key for IATI API Gateway
        authentication. This key must be obtained from the IATI technical
        team or via the Azure portal. Required for all API requests.

    base_url : str, default="https://api.iatistandard.org/datastore"
        Root URL of the IATI Datastore API v3. The default points to the
        production endpoint. For testing, this may be overridden to point
        to a staging environment.

    timeout_s : float, default=60.0
        HTTP request timeout in seconds. This value should be tuned based
        on expected payload sizes and network latency. For bulk downloads
        with rows_per_page=1000, 60 seconds is typically sufficient.

    rows_per_page : int, default=1000
        Number of documents to retrieve per API call. This parameter
        directly affects memory consumption and API response latency.
        Empirically, values between 500-2000 provide optimal throughput
        for the IATI API. Higher values may trigger timeouts.

    sleep_s_between_calls : float, default=0.0
        Optional delay (in seconds) between consecutive API calls.
        Set to a positive value to implement client-side rate limiting
        when the API enforces strict request quotas.

    max_retries : int, default=8
        Maximum number of retry attempts for transient failures.
        With backoff_factor=1.0, this provides coverage for outages
        lasting up to 2^8 = 256 seconds (approximately 4 minutes).

    backoff_factor : float, default=1.0
        Exponential backoff multiplier for retry delays. The delay
        before retry attempt i is: backoff_factor * (2 ** (i - 1)).
        With backoff_factor=1.0: delays are 0.5s, 1s, 2s, 4s, 8s, ...

    Notes
    -----
    The retry strategy targets HTTP status codes that indicate transient
    failures: 429 (Too Many Requests), 500, 502, 503, 504 (Server Errors).
    Client errors (4xx except 429) are not retried as they indicate
    malformed requests.

    Examples
    --------
    >>> config = DatastoreConfig(
    ...     subscription_key="your-api-key-here",
    ...     rows_per_page=500,
    ...     max_retries=5
    ... )
    >>> config.base_url
    'https://api.iatistandard.org/datastore'

    See Also
    --------
    IATIDatastoreClient : HTTP client that consumes this configuration.
    urllib3.util.retry.Retry : Underlying retry strategy implementation.
    """

    # -------------------------------------------------------------------------
    # REQUIRED AUTHENTICATION PARAMETER
    # -------------------------------------------------------------------------

    # Azure API Management subscription key for IATI API Gateway access.
    # This is the only required parameter with no default value.
    subscription_key: str

    # -------------------------------------------------------------------------
    # API ENDPOINT CONFIGURATION
    # -------------------------------------------------------------------------

    # Base URL for the IATI Datastore API v3 production endpoint.
    base_url: str = "https://api.iatistandard.org/datastore"

    # HTTP request timeout in seconds for each API call.
    timeout_s: float = 60.0

    # -------------------------------------------------------------------------
    # THROUGHPUT CONTROL PARAMETERS
    # -------------------------------------------------------------------------

    # Number of documents to retrieve per paginated API request.
    # Tune based on API behavior and acceptable payload size.
    rows_per_page: int = 1000

    # Optional inter-request delay for client-side rate limiting.
    # Set to 0.0 for maximum throughput when API permits.
    sleep_s_between_calls: float = 0.0

    # -------------------------------------------------------------------------
    # RETRY STRATEGY PARAMETERS
    # -------------------------------------------------------------------------

    # Maximum number of retry attempts for transient HTTP failures.
    max_retries: int = 8

    # Exponential backoff multiplier: delay = backoff_factor * 2^(retry-1).
    backoff_factor: float = 1.0


class IATIDatastoreClient:
    """
    Robust HTTP client for the IATI Datastore v3 SOLR-based API.

    This client provides a thin abstraction over the IATI Datastore API,
    handling authentication, retry logic, and response parsing. It is
    designed for reliability in bulk data extraction scenarios where
    network interruptions and transient API failures are expected.

    The client implements the following reliability features:
        - Automatic retry with exponential backoff for transient failures
        - Connection pooling via requests.Session for TCP reuse
        - Configurable timeouts to prevent indefinite blocking
        - Optional inter-request throttling for rate limit compliance

    Architecture
    ------------
    The IATI Datastore v3 API exposes SOLR collections (transaction,
    activity, budget) with a /select endpoint supporting standard SOLR
    query parameters. This client constructs properly formatted requests
    and handles the JSON response envelope.

    Attributes
    ----------
    _cfg : DatastoreConfig
        Immutable configuration object containing connection parameters.

    _session : requests.Session
        Persistent HTTP session with mounted retry adapter.

    Parameters
    ----------
    cfg : DatastoreConfig
        Configuration object specifying API endpoint, authentication,
        timeout, and retry parameters.

    Examples
    --------
    >>> config = DatastoreConfig(subscription_key="your-key")
    >>> client = IATIDatastoreClient(config)
    >>> result = client.select("transaction", q="*:*", rows=10)
    >>> len(result["response"]["docs"])
    10

    Notes
    -----
    The client mounts the retry adapter to both HTTPS and HTTP schemes,
    though the production API exclusively uses HTTPS. HTTP mounting is
    included for testing scenarios with local mock servers.

    See Also
    --------
    DatastoreConfig : Configuration container for client parameters.
    iter_docs_cursor_mark : Streaming iteration using this client.
    """

    def __init__(self, cfg: DatastoreConfig) -> None:
        """
        Initialize the IATI Datastore client with the provided configuration.

        This constructor performs the following initialization steps:
            1. Store the configuration object for parameter access
            2. Create a persistent HTTP session for connection reuse
            3. Configure authentication headers required by the API
            4. Construct and mount a retry adapter with exponential backoff

        Parameters
        ----------
        cfg : DatastoreConfig
            Immutable configuration object containing:
                - subscription_key: API authentication credential
                - base_url: API root endpoint
                - timeout_s: Request timeout in seconds
                - max_retries: Maximum retry attempts
                - backoff_factor: Exponential backoff multiplier

        Raises
        ------
        TypeError
            If cfg is not an instance of DatastoreConfig.

        Notes
        -----
        The retry strategy is configured to respect the Retry-After header
        returned by the API when rate limiting is applied (HTTP 429). This
        ensures compliance with API rate limits without manual intervention.
        """
        # ---------------------------------------------------------------------
        # INPUT VALIDATION
        # ---------------------------------------------------------------------

        # Validate that the configuration object is of the expected type.
        if not isinstance(cfg, DatastoreConfig):
            raise TypeError(
                f"cfg must be an instance of DatastoreConfig, "
                f"received {type(cfg).__name__}"
            )

        # Validate that the subscription key is a non-empty string.
        if not cfg.subscription_key or not isinstance(cfg.subscription_key, str):
            raise ValueError(
                "subscription_key must be a non-empty string for API authentication"
            )

        # ---------------------------------------------------------------------
        # CONFIGURATION STORAGE
        # ---------------------------------------------------------------------

        # Store the immutable configuration object as a private attribute.
        self._cfg: DatastoreConfig = cfg

        # ---------------------------------------------------------------------
        # SESSION INITIALIZATION
        # ---------------------------------------------------------------------

        # Create a persistent HTTP session for connection pooling and reuse.
        # Session objects maintain TCP connections across multiple requests,
        # reducing latency from repeated TLS handshakes.
        self._session: requests.Session = requests.Session()

        # Configure required HTTP headers for all requests in this session.
        # The Ocp-Apim-Subscription-Key header authenticates with Azure APIM.
        self._session.headers.update(
            {
                # Required by IATI API Gateway.
                "Ocp-Apim-Subscription-Key": cfg.subscription_key,
                # Request JSON response format from the SOLR endpoint.
                "Accept": "application/json",
                # Identify the client for API analytics and debugging.
                "User-Agent": "iati-bulk-downloader/1.0",
            }
        )

        # ---------------------------------------------------------------------
        # RETRY STRATEGY CONFIGURATION
        # ---------------------------------------------------------------------

        # Construct the urllib3 Retry object with exponential backoff.
        # This strategy handles transient network and server failures.
        retry: Retry = Retry(
            # Total number of retry attempts for any failure type.
            total=cfg.max_retries,
            # Retry on connection establishment failures.
            connect=cfg.max_retries,
            # Retry on read timeout failures.
            read=cfg.max_retries,
            # Retry on specific HTTP status codes.
            status=cfg.max_retries,
            # Exponential backoff: delay = backoff_factor * 2^(retry-1).
            backoff_factor=cfg.backoff_factor,
            # HTTP status codes that trigger automatic retry.
            # 429: Rate limited, 5xx: Server errors (transient).
            status_forcelist=(429, 500, 502, 503, 504),
            # Only retry idempotent GET requests (safe for bulk downloads).
            allowed_methods=frozenset(["GET"]),
            # Do not raise exception immediately; allow response inspection.
            raise_on_status=False,
            # Honor the Retry-After header from 429 responses.
            respect_retry_after_header=True,
        )

        # ---------------------------------------------------------------------
        # ADAPTER MOUNTING
        # ---------------------------------------------------------------------

        # Create an HTTP adapter with the configured retry strategy.
        adapter: HTTPAdapter = HTTPAdapter(max_retries=retry)

        # Mount the adapter to the HTTPS scheme (production API).
        self._session.mount("https://", adapter)

        # Mount the adapter to the HTTP scheme (testing/mock servers).
        self._session.mount("http://", adapter)

    def select(
        self,
        collection: str,
        *,
        q: str,
        fl: Optional[Sequence[str]] = None,
        sort: Optional[str] = None,
        rows: Optional[int] = None,
        start: Optional[int] = None,
        cursor_mark: Optional[str] = None,
        extra_params: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Execute a SOLR /select query against the specified IATI collection.

        This method constructs and sends a parameterized GET request to the
        SOLR /select endpoint, handling pagination, field selection, and
        sorting. It supports both offset-based (start/rows) and cursor-based
        (cursorMark) pagination strategies.

        Parameters
        ----------
        collection : str
            Target SOLR collection name. Valid values are:
                - "transaction": Individual financial transactions
                - "activity": Aid activities/projects
                - "budget": Budget allocations

        q : str
            SOLR query string in Lucene syntax. Use "*:*" for all documents.
            Example: "transaction_value:[1000 TO *]" for transactions >= 1000.

        fl : Optional[Sequence[str]], default=None
            Field list specifying which document fields to return.
            If None, the API returns its default field set.
            Example: ["iati_identifier", "transaction_value"]

        sort : Optional[str], default=None
            Sort specification in SOLR format: "field_name asc|desc".
            Required for cursor mark pagination.
            Example: "id asc"

        rows : Optional[int], default=None
            Number of documents to return. If None, uses the configured
            rows_per_page from DatastoreConfig.

        start : Optional[int], default=None
            Offset for pagination. Used with rows for offset-based paging.
            Ignored when cursor_mark is provided.

        cursor_mark : Optional[str], default=None
            Opaque cursor for deep pagination. Use "*" for initial request.
            The API returns nextCursorMark for subsequent requests.

        extra_params : Optional[Dict[str, Any]], default=None
            Additional SOLR parameters to include in the request.
            Example: {"fq": "recipient_country_code:KE"}

        Returns
        -------
        Dict[str, Any]
            Parsed JSON response from the SOLR API containing:
                - response.numFound: Total matching documents
                - response.docs: List of document dictionaries
                - nextCursorMark: Cursor for next page (if applicable)

        Raises
        ------
        RuntimeError
            If the API returns an HTTP status code >= 400, indicating
            a client or server error that was not resolved by retries.

        ValueError
            If collection is not a non-empty string.

        Notes
        -----
        The method implements client-side throttling if sleep_s_between_calls
        is configured. This occurs after the response is received to maintain
        consistent request spacing.

        For datasets with N > 10^5 documents, cursor mark pagination is
        strongly preferred over offset pagination to avoid O(N) complexity
        in SOLR's skip operations.

        Examples
        --------
        >>> result = client.select(
        ...     "transaction",
        ...     q="transaction_value:[* TO *]",
        ...     fl=["iati_identifier", "transaction_value"],
        ...     rows=100
        ... )
        >>> len(result["response"]["docs"])
        100
        """
        # ---------------------------------------------------------------------
        # INPUT VALIDATION
        # ---------------------------------------------------------------------

        # Validate that collection is a non-empty string.
        if not collection or not isinstance(collection, str):
            raise ValueError(
                f"collection must be a non-empty string, received: {collection!r}"
            )

        # Validate that the query string is provided.
        if not isinstance(q, str):
            raise ValueError(
                f"q (query) must be a string, received: {type(q).__name__}"
            )

        # ---------------------------------------------------------------------
        # URL CONSTRUCTION
        # ---------------------------------------------------------------------

        # Construct the full URL for the SOLR /select endpoint.
        # Format: {base_url}/{collection}/select
        url: str = f"{self._cfg.base_url}/{collection}/select"

        # ---------------------------------------------------------------------
        # PARAMETER DICTIONARY CONSTRUCTION
        # ---------------------------------------------------------------------

        # Initialize the request parameters with required values.
        params: Dict[str, Any] = {
            # The SOLR query string (Lucene syntax).
            "q": q,
            # Request JSON response format.
            "wt": "json",
            # Number of rows to return, defaulting to configured value.
            "rows": rows if rows is not None else self._cfg.rows_per_page,
        }

        # Add optional field list if specified (comma-separated string).
        if fl:
            params["fl"] = ",".join(fl)

        # Add sort specification if provided (required for cursor pagination).
        if sort:
            params["sort"] = sort

        # Add start offset for offset-based pagination.
        if start is not None:
            params["start"] = start

        # Add cursor mark for cursor-based deep pagination.
        if cursor_mark is not None:
            params["cursorMark"] = cursor_mark

        # Merge any additional parameters provided by the caller.
        if extra_params:
            params.update(extra_params)

        # ---------------------------------------------------------------------
        # HTTP REQUEST EXECUTION
        # ---------------------------------------------------------------------

        # Execute the GET request with configured timeout.
        # The session's retry adapter handles transient failures automatically.
        resp: requests.Response = self._session.get(
            url, params=params, timeout=self._cfg.timeout_s
        )

        # ---------------------------------------------------------------------
        # OPTIONAL THROTTLING
        # ---------------------------------------------------------------------

        # Apply inter-request delay if configured for rate limit compliance.
        if self._cfg.sleep_s_between_calls > 0:
            time.sleep(self._cfg.sleep_s_between_calls)

        # ---------------------------------------------------------------------
        # ERROR HANDLING
        # ---------------------------------------------------------------------

        # Check for HTTP error status codes (4xx, 5xx).
        # Fail fast with readable diagnostics for debugging.
        if resp.status_code >= 400:
            raise RuntimeError(
                f"Datastore API error {resp.status_code}: {resp.text[:1000]}"
            )

        # ---------------------------------------------------------------------
        # RESPONSE PARSING
        # ---------------------------------------------------------------------

        # Parse and return the JSON response body.
        return resp.json()

    def sample_document(self, collection: str, *, q: str = "*:*") -> Dict[str, Any]:
        """
        Retrieve exactly one document from the specified collection.

        This utility method is useful for schema discovery, allowing
        inspection of available field names and data types before
        constructing bulk download queries.

        Parameters
        ----------
        collection : str
            Target SOLR collection name ("transaction", "activity", "budget").

        q : str, default="*:*"
            SOLR query string. Defaults to matching all documents.

        Returns
        -------
        Dict[str, Any]
            A single document dictionary containing all default fields,
            or an empty dictionary if no documents match the query.

        Examples
        --------
        >>> doc = client.sample_document("transaction")
        >>> list(doc.keys())[:5]
        ['iati_identifier', 'transaction_type', 'transaction_value', ...]

        Notes
        -----
        This method is intended for development and debugging. For
        production field discovery, consult the IATI Datastore schema
        documentation directly.
        """
        # ---------------------------------------------------------------------
        # FETCH SINGLE DOCUMENT
        # ---------------------------------------------------------------------

        # Execute select query with rows=1 to retrieve a single sample.
        payload: Dict[str, Any] = self.select(collection, q=q, rows=1)

        # ---------------------------------------------------------------------
        # EXTRACT DOCUMENT FROM RESPONSE
        # ---------------------------------------------------------------------

        # Navigate the SOLR response structure to extract the docs array.
        docs: List[Dict[str, Any]] = payload.get("response", {}).get("docs", [])

        # Return the first document if available, otherwise empty dict.
        return docs[0] if docs else {}


def iter_docs_cursor_mark(
    client: IATIDatastoreClient,
    *,
    collection: str,
    q: str,
    fl: Optional[Sequence[str]],
    sort: str,
    rows: int,
) -> Iterator[Dict[str, Any]]:
    """
    Stream documents from IATI Datastore using SOLR cursor mark pagination.

    This generator function implements memory-efficient iteration over
    arbitrarily large result sets using SOLR's cursor mark feature. Cursor
    mark pagination maintains O(1) server-side state per request, avoiding
    the O(N) complexity of deep offset pagination.

    The cursor mark algorithm:
        1. Initial request uses cursorMark="*"
        2. Response includes nextCursorMark for subsequent page
        3. Iteration continues until nextCursorMark equals previous cursor
           (indicating exhaustion)

    Parameters
    ----------
    client : IATIDatastoreClient
        Configured HTTP client for API communication.

    collection : str
        Target SOLR collection ("transaction", "activity", "budget").

    q : str
        SOLR query string in Lucene syntax.

    fl : Optional[Sequence[str]]
        Field list to retrieve. None retrieves all default fields.

    sort : str
        Sort specification REQUIRED for cursor pagination.
        Must include a unique field (typically "id asc") to ensure
        deterministic ordering across pages.

    rows : int
        Number of documents per page. Affects memory usage per iteration.

    Yields
    ------
    Dict[str, Any]
        Individual document dictionaries, yielded one at a time to
        minimize memory footprint during bulk iteration.

    Raises
    ------
    RuntimeError
        Propagated from client.select() if API returns error status.

    ValueError
        If required parameters (client, collection, q, sort) are invalid.

    Notes
    -----
    Cursor mark pagination requires a sort specification that includes
    at least one uniquely-valued field. The conventional choice is "id asc"
    which leverages SOLR's internal document ID.

    For the IATI transaction collection with N ≈ 10^7 documents, cursor
    pagination reduces total query time from O(N²) to O(N) compared to
    offset pagination.

    Examples
    --------
    >>> for doc in iter_docs_cursor_mark(
    ...     client,
    ...     collection="transaction",
    ...     q="*:*",
    ...     fl=["iati_identifier"],
    ...     sort="id asc",
    ...     rows=1000
    ... ):
    ...     process(doc)

    See Also
    --------
    iter_docs_start_rows : Fallback pagination using start/rows offsets.
    IATIDatastoreClient.select : Underlying API request method.

    References
    ----------
    .. [1] Apache SOLR Documentation: Using Cursor Mark
           https://solr.apache.org/guide/pagination-of-results.html
    """
    # -------------------------------------------------------------------------
    # INPUT VALIDATION
    # -------------------------------------------------------------------------

    # Validate that client is an instance of IATIDatastoreClient.
    if not isinstance(client, IATIDatastoreClient):
        raise ValueError(
            f"client must be an IATIDatastoreClient instance, "
            f"received {type(client).__name__}"
        )

    # Validate that collection is a non-empty string.
    if not collection or not isinstance(collection, str):
        raise ValueError(
            f"collection must be a non-empty string, received: {collection!r}"
        )

    # Validate that sort is provided (required for cursor pagination).
    if not sort or not isinstance(sort, str):
        raise ValueError(
            "sort parameter is required for cursor mark pagination"
        )

    # Validate that rows is a positive integer.
    if not isinstance(rows, int) or rows <= 0:
        raise ValueError(
            f"rows must be a positive integer, received: {rows}"
        )

    # -------------------------------------------------------------------------
    # CURSOR INITIALIZATION
    # -------------------------------------------------------------------------

    # Initialize the cursor mark with the SOLR start marker "*".
    # This special value indicates the beginning of the result set.
    cursor: str = "*"

    # -------------------------------------------------------------------------
    # PAGINATION LOOP
    # -------------------------------------------------------------------------

    # Iterate until cursor exhaustion is detected.
    while True:
        # ---------------------------------------------------------------------
        # API REQUEST
        # ---------------------------------------------------------------------

        # Execute the paginated query with current cursor position.
        payload: Dict[str, Any] = client.select(
            collection,
            q=q,
            fl=fl,
            sort=sort,
            rows=rows,
            cursor_mark=cursor,
        )

        # ---------------------------------------------------------------------
        # RESPONSE PARSING
        # ---------------------------------------------------------------------

        # Extract the documents array from the SOLR response envelope.
        docs: List[Dict[str, Any]] = payload.get("response", {}).get("docs", [])

        # Extract the next cursor mark for pagination continuation.
        next_cursor: Optional[str] = payload.get("nextCursorMark")

        # ---------------------------------------------------------------------
        # DOCUMENT YIELDING
        # ---------------------------------------------------------------------

        # Yield each document individually for memory-efficient processing.
        for d in docs:
            yield d

        # ---------------------------------------------------------------------
        # TERMINATION CHECK
        # ---------------------------------------------------------------------

        # Check for cursor exhaustion: nextCursorMark equals current cursor
        # when no more documents remain, or nextCursorMark is absent.
        if not next_cursor or next_cursor == cursor:
            break

        # ---------------------------------------------------------------------
        # CURSOR ADVANCEMENT
        # ---------------------------------------------------------------------

        # Update cursor for next iteration.
        cursor = next_cursor


  def iter_docs_start_rows(
    client: IATIDatastoreClient,
    *,
    collection: str,
    q: str,
    fl: Optional[Sequence[str]],
    sort: Optional[str],
    rows: int,
) -> Iterator[Dict[str, Any]]:
    """
    Stream documents from IATI Datastore using start/rows offset pagination.

    This generator function provides a fallback pagination strategy for
    SOLR deployments that do not support cursor mark pagination. It
    implements traditional offset-based pagination using the start and
    rows parameters.

    Warning: Offset pagination has O(N) complexity per page for deep
    result sets, resulting in O(N²) total complexity for full extraction.
    Use cursor mark pagination when available.

    Parameters
    ----------
    client : IATIDatastoreClient
        Configured HTTP client for API communication.

    collection : str
        Target SOLR collection ("transaction", "activity", "budget").

    q : str
        SOLR query string in Lucene syntax.

    fl : Optional[Sequence[str]]
        Field list to retrieve. None retrieves all default fields.

    sort : Optional[str]
        Sort specification. Unlike cursor pagination, this is optional
        for offset pagination, though recommended for determinism.

    rows : int
        Number of documents per page (must be positive).

    Yields
    ------
    Dict[str, Any]
        Individual document dictionaries, yielded one at a time.

    Raises
    ------
    RuntimeError
        Propagated from client.select() if API returns error status.

    ValueError
        If required parameters are invalid.

    Notes
    -----
    The total number of documents (numFound) is extracted from the first
    API response and used to calculate the total number of pages:

        total_pages = ceil(numFound / rows)

    For very large datasets (N > 10^6), this method may exhibit
    degraded performance compared to cursor mark pagination due to
    SOLR's internal skip operations.

    Examples
    --------
    >>> for doc in iter_docs_start_rows(
    ...     client,
    ...     collection="transaction",
    ...     q="*:*",
    ...     fl=["iati_identifier"],
    ...     sort="id asc",
    ...     rows=1000
    ... ):
    ...     process(doc)

    See Also
    --------
    iter_docs_cursor_mark : Preferred pagination for large datasets.
    """
    # -------------------------------------------------------------------------
    # INPUT VALIDATION
    # -------------------------------------------------------------------------

    # Validate that client is an instance of IATIDatastoreClient.
    if not isinstance(client, IATIDatastoreClient):
        raise ValueError(
            f"client must be an IATIDatastoreClient instance, "
            f"received {type(client).__name__}"
        )

    # Validate that collection is a non-empty string.
    if not collection or not isinstance(collection, str):
        raise ValueError(
            f"collection must be a non-empty string, received: {collection!r}"
        )

    # Validate that rows is a positive integer.
    if not isinstance(rows, int) or rows <= 0:
        raise ValueError(
            f"rows must be a positive integer, received: {rows}"
        )

    # -------------------------------------------------------------------------
    # OFFSET INITIALIZATION
    # -------------------------------------------------------------------------

    # Initialize the start offset to zero (beginning of result set).
    start: int = 0

    # -------------------------------------------------------------------------
    # FIRST PAGE REQUEST (for numFound extraction)
    # -------------------------------------------------------------------------

    # Execute the initial query to determine total result count.
    first: Dict[str, Any] = client.select(
        collection, q=q, fl=fl, sort=sort, rows=rows, start=start
    )

    # Extract the total number of matching documents from the response.
    # This value is used to calculate the number of pagination iterations.
    num_found: int = int(first.get("response", {}).get("numFound", 0))

    # Extract documents from the first page.
    docs: List[Dict[str, Any]] = first.get("response", {}).get("docs", [])

    # -------------------------------------------------------------------------
    # YIELD FIRST PAGE DOCUMENTS
    # -------------------------------------------------------------------------

    # Yield each document from the first page.
    for d in docs:
        yield d

    # -------------------------------------------------------------------------
    # CALCULATE TOTAL PAGES
    # -------------------------------------------------------------------------

    # Compute the total number of pages using ceiling division.
    # Formula: total_pages = ⌈numFound / rows⌉
    total_pages: int = math.ceil(num_found / rows) if rows > 0 else 0

    # -------------------------------------------------------------------------
    # PAGINATION LOOP (remaining pages)
    # -------------------------------------------------------------------------

    # Iterate through remaining pages (starting from page index 1).
    for page_idx in range(1, total_pages):
        # ---------------------------------------------------------------------
        # CALCULATE OFFSET
        # ---------------------------------------------------------------------

        # Compute the start offset for the current page.
        # Formula: start = page_idx * rows
        start = page_idx * rows

        # ---------------------------------------------------------------------
        # API REQUEST
        # ---------------------------------------------------------------------

        # Execute the paginated query with calculated offset.
        payload: Dict[str, Any] = client.select(
            collection, q=q, fl=fl, sort=sort, rows=rows, start=start
        )

        # Extract documents from the current page.
        docs = payload.get("response", {}).get("docs", [])

        # ---------------------------------------------------------------------
        # DOCUMENT YIELDING
        # ---------------------------------------------------------------------

        # Yield each document individually for memory-efficient processing.
        for d in docs:
            yield d


def docs_to_parquet_shards(
    docs: Iterable[Dict[str, Any]],
    *,
    out_dir: Path,
    shard_size: int = 100_000,
    parquet_compression: str = "zstd",
) -> List[Path]:
    """
    Serialize an iterable of document dictionaries to Parquet shard files.

    This function implements a streaming write pattern that converts
    an arbitrarily large iterable of documents into a directory of
    fixed-size Parquet files. This approach enables:
        - Bounded memory usage during ingestion
        - Parallel downstream processing of individual shards
        - Efficient columnar storage with compression

    The sharding strategy writes documents in batches of shard_size,
    creating files named part-000000.parquet, part-000001.parquet, etc.

    Parameters
    ----------
    docs : Iterable[Dict[str, Any]]
        Input iterable of document dictionaries. This can be a generator
        (e.g., from iter_docs_cursor_mark) for memory-efficient streaming.

    out_dir : Path
        Output directory for Parquet shard files. Created if it does not
        exist. Must be writable.

    shard_size : int, default=100_000
        Maximum number of documents per shard file. Smaller values reduce
        memory usage during writes; larger values reduce file count and
        improve read performance for sequential scans.

    parquet_compression : str, default="zstd"
        Compression codec for Parquet files. Options include:
            - "zstd": Zstandard (excellent compression, fast)
            - "snappy": Google Snappy (fast, moderate compression)
            - "gzip": Gzip (high compression, slower)
            - None: No compression

    Returns
    -------
    List[Path]
        Ordered list of created Parquet shard file paths.

    Raises
    ------
    ValueError
        If shard_size is not a positive integer.
        If parquet_compression is not a valid codec.

    OSError
        If the output directory cannot be created or is not writable.

    Notes
    -----
    Memory Complexity:
        Peak memory usage is O(shard_size * avg_doc_size) plus Pandas
        DataFrame overhead. For shard_size=100,000 and average document
        size of 1KB, expect ~100MB peak memory per shard write.

    File Naming Convention:
        Shard files use zero-padded six-digit indices (part-000000.parquet)
        to support lexicographic sorting up to 1 million shards.

    Examples
    --------
    >>> docs_iter = iter_docs_cursor_mark(client, ...)
    >>> shards = docs_to_parquet_shards(
    ...     docs_iter,
    ...     out_dir=Path("./data/transactions"),
    ...     shard_size=50_000
    ... )
    >>> len(shards)
    200

    See Also
    --------
    load_parquet_dataset_as_pandas : Load shards as unified DataFrame.
    pandas.DataFrame.to_parquet : Underlying serialization method.
    """
    # -------------------------------------------------------------------------
    # INPUT VALIDATION
    # -------------------------------------------------------------------------

    # Validate that out_dir is a Path object.
    if not isinstance(out_dir, Path):
        raise TypeError(
            f"out_dir must be a Path object, received {type(out_dir).__name__}"
        )

    # Validate that shard_size is a positive integer.
    if not isinstance(shard_size, int) or shard_size <= 0:
        raise ValueError(
            f"shard_size must be a positive integer, received: {shard_size}"
        )

    # Validate compression codec.
    valid_codecs = {"zstd", "snappy", "gzip", "lz4", "brotli", None}
    if parquet_compression not in valid_codecs:
        raise ValueError(
            f"parquet_compression must be one of {valid_codecs}, "
            f"received: {parquet_compression!r}"
        )

    # -------------------------------------------------------------------------
    # DIRECTORY CREATION
    # -------------------------------------------------------------------------

    # Create the output directory if it does not exist.
    # parents=True creates intermediate directories as needed.
    out_dir.mkdir(parents=True, exist_ok=True)

    # -------------------------------------------------------------------------
    # STATE INITIALIZATION
    # -------------------------------------------------------------------------

    # Initialize list to track created shard file paths.
    shard_paths: List[Path] = []

    # Initialize batch accumulator for current shard.
    batch: List[Dict[str, Any]] = []

    # Initialize shard index counter for file naming.
    shard_idx: int = 0

    # -------------------------------------------------------------------------
    # DOCUMENT ITERATION AND BATCHING
    # -------------------------------------------------------------------------

    # Iterate through input documents, batching for shard writes.
    for doc in docs:
        # Append current document to the accumulator batch.
        batch.append(doc)

        # Check if batch has reached shard_size threshold.
        if len(batch) >= shard_size:
            # -----------------------------------------------------------------
            # SHARD WRITE
            # -----------------------------------------------------------------

            # Construct shard file path with zero-padded index.
            shard_path: Path = out_dir / f"part-{shard_idx:06d}.parquet"

            # Convert batch to Pandas DataFrame for Parquet serialization.
            df: pd.DataFrame = pd.DataFrame.from_records(batch)

            # Write DataFrame to Parquet with specified compression.
            df.to_parquet(shard_path, index=False, compression=parquet_compression)

            # Record the created shard path.
            shard_paths.append(shard_path)

            # Reset batch accumulator for next shard.
            batch = []

            # Increment shard index counter.
            shard_idx += 1

    # -------------------------------------------------------------------------
    # FINAL SHARD WRITE (remaining documents)
    # -------------------------------------------------------------------------

    # Write any remaining documents in the final partial batch.
    if batch:
        # Construct shard file path for final batch.
        shard_path = out_dir / f"part-{shard_idx:06d}.parquet"

        # Convert remaining batch to DataFrame.
        df = pd.DataFrame.from_records(batch)

        # Write final shard to Parquet.
        df.to_parquet(shard_path, index=False, compression=parquet_compression)

        # Record the final shard path.
        shard_paths.append(shard_path)

    # -------------------------------------------------------------------------
    # RETURN SHARD PATHS
    # -------------------------------------------------------------------------

    # Return ordered list of all created shard file paths.
    return shard_paths


def write_snapshot_metadata(
    *,
    out_dir: Path,
    collection: str,
    q: str,
    fl: Optional[Sequence[str]],
) -> Path:
    """
    Write provenance metadata for a dataset snapshot to support reproducibility.

    This function creates a JSON metadata file capturing the query parameters
    and timestamp of a dataset download. This metadata enables:
        - Reproducible data extraction with identical parameters
        - Audit trail for data lineage tracking
        - Documentation of dataset scope and field selection

    The metadata file is written to _snapshot_metadata.json in the
    specified output directory.

    Parameters
    ----------
    out_dir : Path
        Output directory for the metadata file. Created if it does not exist.

    collection : str
        SOLR collection name that was queried ("transaction", "activity", etc.).

    q : str
        SOLR query string used for data extraction.

    fl : Optional[Sequence[str]]
        Field list that was requested, or None if all default fields were used.

    Returns
    -------
    Path
        Path to the created metadata JSON file.

    Raises
    ------
    TypeError
        If out_dir is not a Path object.

    ValueError
        If collection or q are not valid strings.

    OSError
        If the output directory cannot be created or file cannot be written.

    Notes
    -----
    The metadata file contains:
        - collection: The queried SOLR collection
        - q: The SOLR query string
        - fl: The field list (null if all defaults)
        - downloaded_at_utc: ISO 8601 timestamp in UTC

    The underscore prefix in _snapshot_metadata.json signals that this is
    a metadata file, not a data shard, following common conventions for
    partitioned datasets.

    Examples
    --------
    >>> meta_path = write_snapshot_metadata(
    ...     out_dir=Path("./data/transactions"),
    ...     collection="transaction",
    ...     q="*:*",
    ...     fl=["iati_identifier", "transaction_value"]
    ... )
    >>> meta_path
    PosixPath('./data/transactions/_snapshot_metadata.json')

    See Also
    --------
    download_iati_transactions_1967_2025 : Main download function that uses this.
    """
    # -------------------------------------------------------------------------
    # INPUT VALIDATION
    # -------------------------------------------------------------------------

    # Validate that out_dir is a Path object.
    if not isinstance(out_dir, Path):
        raise TypeError(
            f"out_dir must be a Path object, received {type(out_dir).__name__}"
        )

    # Validate that collection is a non-empty string.
    if not collection or not isinstance(collection, str):
        raise ValueError(
            f"collection must be a non-empty string, received: {collection!r}"
        )

    # Validate that q is a string.
    if not isinstance(q, str):
        raise ValueError(
            f"q (query) must be a string, received: {type(q).__name__}"
        )

    # -------------------------------------------------------------------------
    # DIRECTORY CREATION
    # -------------------------------------------------------------------------

    # Ensure the output directory exists (create if necessary).
    out_dir.mkdir(parents=True, exist_ok=True)

    # -------------------------------------------------------------------------
    # METADATA DICTIONARY CONSTRUCTION
    # -------------------------------------------------------------------------

    # Construct the metadata dictionary with provenance information.
    meta: Dict[str, Any] = {
        # The SOLR collection that was queried.
        "collection": collection,
        # The SOLR query string used for filtering.
        "q": q,
        # The field list (converted from Sequence to List for JSON serialization).
        "fl": list(fl) if fl else None,
        # UTC timestamp of the download in ISO 8601 format.
        "downloaded_at_utc": datetime.now(timezone.utc).isoformat(),
    }

    # -------------------------------------------------------------------------
    # FILE PATH CONSTRUCTION
    # -------------------------------------------------------------------------

    # Construct the metadata file path with underscore prefix convention.
    meta_path: Path = out_dir / "_snapshot_metadata.json"

    # -------------------------------------------------------------------------
    # FILE WRITE
    # -------------------------------------------------------------------------

    # Serialize metadata dictionary to JSON and write to file.
    # indent=2 provides human-readable formatting.
    # encoding="utf-8" ensures proper Unicode handling.
    meta_path.write_text(json.dumps(meta, indent=2), encoding="utf-8")

    # -------------------------------------------------------------------------
    # RETURN METADATA PATH
    # -------------------------------------------------------------------------

    # Return the path to the created metadata file.
    return meta_path


def load_parquet_dataset_as_pandas(dataset_dir: Path) -> pd.DataFrame:
    """
    Materialize a sharded Parquet dataset as a unified Pandas DataFrame.

    This function reads all Parquet files in the specified directory and
    concatenates them into a single in-memory DataFrame. It leverages
    PyArrow's efficient Parquet reader for optimized I/O.

    WARNING: This function loads the entire dataset into memory. For
    datasets with N > 10^6 rows, memory usage can exceed available RAM.
    Consider filtered reads or chunked processing for large datasets.

    Parameters
    ----------
    dataset_dir : Path
        Directory containing Parquet shard files (part-*.parquet).
        This should be the output directory from docs_to_parquet_shards().

    Returns
    -------
    pd.DataFrame
        Unified DataFrame containing all records from all shards,
        with schema inferred from Parquet metadata.

    Raises
    ------
    TypeError
        If dataset_dir is not a Path object.

    FileNotFoundError
        If the directory does not exist or contains no Parquet files.

    MemoryError
        If the dataset exceeds available system memory.

    Notes
    -----
    Memory Estimation:
        For the IATI transaction dataset (N ≈ 10^7 records), expect
        memory usage of 5-20 GB depending on field selection. Use
        pandas.read_parquet with columns parameter for partial loads.

    Implementation:
        Uses pandas.read_parquet with directory path, which internally
        uses PyArrow to discover and read all Parquet files in the
        directory.

    Examples
    --------
    >>> df = load_parquet_dataset_as_pandas(Path("./data/transactions"))
    >>> df.shape
    (10234567, 45)
    >>> df.memory_usage(deep=True).sum() / 1e9  # GB
    8.5

    See Also
    --------
    docs_to_parquet_shards : Function that creates the Parquet shards.
    pandas.read_parquet : Underlying implementation.
    """
    # -------------------------------------------------------------------------
    # INPUT VALIDATION
    # -------------------------------------------------------------------------

    # Validate that dataset_dir is a Path object.
    if not isinstance(dataset_dir, Path):
        raise TypeError(
            f"dataset_dir must be a Path object, received {type(dataset_dir).__name__}"
        )

    # Validate that the directory exists.
    if not dataset_dir.exists():
        raise FileNotFoundError(
            f"Dataset directory does not exist: {dataset_dir}"
        )

    # Validate that the path is a directory.
    if not dataset_dir.is_dir():
        raise ValueError(
            f"dataset_dir must be a directory, received file: {dataset_dir}"
        )

    # -------------------------------------------------------------------------
    # PARQUET DATASET LOADING
    # -------------------------------------------------------------------------

    # Read all Parquet files in the directory and concatenate into DataFrame.
    # PyArrow automatically discovers part-*.parquet files and merges schemas.
    return pd.read_parquet(dataset_dir)


def download_iati_transactions_1967_2025(
    *,
    subscription_key: str,
    out_dir: Path,
    fields: Optional[Sequence[str]] = None,
    rows_per_page: int = 1000,
    shard_size: int = 100_000,
    prefer_cursor_mark: bool = True,
) -> Tuple[List[Path], Path]:
    """
    Execute end-to-end bulk download of IATI transactions from 1967 to 2025.

    This function orchestrates the complete data extraction pipeline for
    IATI transaction records, implementing the data acquisition phase
    described in McCarthy et al. (2025) "Who Connects Global Aid? The
    Hidden Geometry of 10 Million Transactions."

    The function performs the following steps:
        1. Initialize HTTP client with retry configuration
        2. Construct temporal filter query (1967-01-01 to 2025-12-31)
        3. Write provenance metadata for reproducibility
        4. Stream documents using cursor or offset pagination
        5. Serialize to sharded Parquet files for downstream processing

    Parameters
    ----------
    subscription_key : str
        Azure API Management subscription key for IATI API authentication.
        This is required and must be obtained from the IATI technical team.

    out_dir : Path
        Output directory for Parquet shards and metadata.
        Created if it does not exist. Must be writable.

    fields : Optional[Sequence[str]], default=None
        List of SOLR field names to retrieve. If None, the API returns
        its default field set (typically wide). Specify a subset for
        reduced storage and faster downloads.
        Example: ["iati_identifier", "transaction_value", "recipient_country_code"]

    rows_per_page : int, default=1000
        Number of documents per API request. Values between 500-2000 are
        recommended. Higher values may trigger API timeouts.

    shard_size : int, default=100_000
        Number of documents per Parquet shard file. Affects memory usage
        during writes and parallelism during downstream reads.

    prefer_cursor_mark : bool, default=True
        If True, attempt cursor mark pagination first (recommended).
        If cursor mark fails, automatically fall back to offset pagination.

    Returns
    -------
    Tuple[List[Path], Path]
        A tuple containing:
            - List[Path]: Ordered list of created Parquet shard file paths
            - Path: Path to the metadata JSON file

    Raises
    ------
    ValueError
        If subscription_key is empty or invalid.
        If rows_per_page or shard_size are not positive integers.

    TypeError
        If out_dir is not a Path object.

    RuntimeError
        If both cursor mark and offset pagination fail due to API errors.

    OSError
        If output directory cannot be created or files cannot be written.

    Notes
    -----
    Temporal Coverage:
        The query filter `transaction_transaction_date_iso_date:[* TO 2025-12-31]`
        captures all transactions with dates up to and including December 31, 2025.
        This provides a reproducible snapshot cutoff for the study.

    Performance Characteristics:
        - Expected runtime: 2-8 hours for full dataset (network dependent)
        - Disk usage: ~2-5 GB compressed Parquet
        - Peak memory: ~500 MB (dominated by shard_size batching)

    Fallback Behavior:
        If cursor mark pagination fails (some SOLR deployments do not support
        the required sort field), the function logs a warning and falls back
        to start/rows pagination automatically.

    Examples
    --------
    >>> shards, meta = download_iati_transactions_1967_2025(
    ...     subscription_key="your-api-key",
    ...     out_dir=Path("./data/iati_transactions"),
    ...     fields=["iati_identifier", "transaction_value"],
    ...     rows_per_page=500,
    ...     shard_size=50_000
    ... )
    >>> len(shards)
    200
    >>> meta
    PosixPath('./data/iati_transactions/_snapshot_metadata.json')

    See Also
    --------
    IATIDatastoreClient : HTTP client used for API communication.
    iter_docs_cursor_mark : Streaming pagination implementation.
    docs_to_parquet_shards : Parquet serialization implementation.

    References
    ----------
    .. [1] McCarthy, P.X., Gong, X., Rizoiu, M.-A., & Boldi, P. (2025).
           "Who Connects Global Aid? The Hidden Geometry of 10 Million
           Transactions." arXiv:2512.17243
    """
    # -------------------------------------------------------------------------
    # INPUT VALIDATION
    # -------------------------------------------------------------------------

    # Validate that subscription_key is a non-empty string.
    if not subscription_key or not isinstance(subscription_key, str):
        raise ValueError(
            "subscription_key must be a non-empty string for API authentication"
        )

    # Validate that out_dir is a Path object.
    if not isinstance(out_dir, Path):
        raise TypeError(
            f"out_dir must be a Path object, received {type(out_dir).__name__}"
        )

    # Validate that rows_per_page is a positive integer.
    if not isinstance(rows_per_page, int) or rows_per_page <= 0:
        raise ValueError(
            f"rows_per_page must be a positive integer, received: {rows_per_page}"
        )

    # Validate that shard_size is a positive integer.
    if not isinstance(shard_size, int) or shard_size <= 0:
        raise ValueError(
            f"shard_size must be a positive integer, received: {shard_size}"
        )

    # -------------------------------------------------------------------------
    # CLIENT CONFIGURATION
    # -------------------------------------------------------------------------

    # Create the datastore configuration with provided parameters.
    cfg: DatastoreConfig = DatastoreConfig(
        subscription_key=subscription_key,
        rows_per_page=rows_per_page,
    )

    # Initialize the HTTP client with retry configuration.
    client: IATIDatastoreClient = IATIDatastoreClient(cfg)

    # -------------------------------------------------------------------------
    # QUERY CONSTRUCTION
    # -------------------------------------------------------------------------

    # Construct SOLR query with temporal filter for reproducible cutoff.
    # This captures all transactions with dates from the beginning of records
    # through December 31, 2025 (inclusive), as specified in the study.
    q: str = "transaction_transaction_date_iso_date:[* TO 2025-12-31T23:59:59Z]"

    # -------------------------------------------------------------------------
    # FIELD LIST PREPARATION
    # -------------------------------------------------------------------------

    # Convert fields Sequence to List for API compatibility, or None for defaults.
    # If fields is None, the API returns its complete default field set.
    fl: Optional[List[str]] = list(fields) if fields else None

    # -------------------------------------------------------------------------
    # METADATA WRITING
    # -------------------------------------------------------------------------

    # Write provenance metadata before data extraction for reproducibility.
    # This ensures metadata exists even if extraction is interrupted.
    write_snapshot_metadata(out_dir=out_dir, collection="transaction", q=q, fl=fl)

    # -------------------------------------------------------------------------
    # PAGINATION STRATEGY SELECTION
    # -------------------------------------------------------------------------

    # Prefer cursor mark pagination if available (better performance for deep paging).
    # Provide robust fallback to start/rows pagination if cursor mark fails.
    if prefer_cursor_mark:
        # ---------------------------------------------------------------------
        # CURSOR MARK PAGINATION ATTEMPT
        # ---------------------------------------------------------------------

        # Define sort specification required for cursor mark pagination.
        # "id asc" uses SOLR's internal document ID for stable ordering.
        # Many SOLR deployments accept this; some may require a different field.
        sort: str = "id asc"

        try:
            # Create streaming document iterator using cursor mark pagination.
            docs_iter: Iterator[Dict[str, Any]] = iter_docs_cursor_mark(
                client,
                collection="transaction",
                q=q,
                fl=fl,
                sort=sort,
                rows=rows_per_page,
            )

            # Stream documents to Parquet shards.
            shards: List[Path] = docs_to_parquet_shards(
                docs_iter,
                out_dir=out_dir,
                shard_size=shard_size,
            )

            # Return shard paths and metadata path on success.
            return shards, out_dir / "_snapshot_metadata.json"

        except RuntimeError as exc:
            # -----------------------------------------------------------------
            # FALLBACK TRIGGER
            # -----------------------------------------------------------------

            # Log warning and fall through to start/rows pagination.
            # This handles SOLR deployments that reject cursor mark queries.
            LOGGER.warning(
                "cursorMark mode failed, falling back to start/rows: %s", exc
            )

    # -------------------------------------------------------------------------
    # START/ROWS PAGINATION (fallback or explicit)
    # -------------------------------------------------------------------------

    # Create streaming document iterator using offset pagination.
    docs_iter = iter_docs_start_rows(
        client,
        collection="transaction",
        q=q,
        fl=fl,
        sort=None,
        rows=rows_per_page,
    )

    # Stream documents to Parquet shards.
    shards = docs_to_parquet_shards(
        docs_iter,
        out_dir=out_dir,
        shard_size=shard_size,
    )

    # -------------------------------------------------------------------------
    # RETURN RESULTS
    # -------------------------------------------------------------------------

    # Return tuple of shard paths and metadata path.
    return shards, out_dir / "_snapshot_metadata.json"

# ==============================================================================
# Name Data Fields
# ==============================================================================

FIELDS = [
    "iati_identifier",
    "reporting_org_ref",
    "transaction_transaction_date_iso_date",
    "transaction_transaction_type_code",
    "transaction_value",
    "transaction_value_currency",
    "transaction_provider_org_ref",
    "transaction_receiver_org_ref",
]

# ==============================================================================
# Download Data
# ==============================================================================

shards, meta_path = download_iati_transactions_1967_2025(
    subscription_key=os.environ["IATI_SUBSCRIPTION_KEY"],
    out_dir=Path("./iati_transactions_1967_2025_parquet"),
    fields=FIELDS,
    rows_per_page=1000,
    shard_size=100_000,
    prefer_cursor_mark=True,
)

# ==============================================================================
# IMPORTS
# ==============================================================================

df = pd.read_parquet(Path("./iati_transactions_1967_2025_parquet"))
print(df.shape)
print(df.head())